{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic NLP\n",
    "\n",
    "## Natural Language Processing - NLP\n",
    "\n",
    "NLP is processing natual language - free text and speech. We can use free text in predictive modelling, in fact it is a quickly developing field.\n",
    "\n",
    "Technologies such as speech recognition, automatic translation, and computer speech are all based on the concpets that we'll cover here. \n",
    "\n",
    "The premise of NLP is that we take a piece of text and process it to transform into a format that we can process. In our case here we'll take free text and convert it into a set of features - we can then use those features to make predictions for our target, just like always!\n",
    "\n",
    "### Spam Filtering\n",
    "\n",
    "For an example we'll build a spam filter. The dataset here has two columns - one is a text message, the other is a human assigned label of spam or ham. We want to be able to detect the spam messages and filter them out. The only feature we have to be able to do so is the message itself..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "df = pd.read_csv(\"data/spam.csv\", encoding=\"ISO-8859-1\")\n",
    "df.drop(columns={\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"}, inplace=True)\n",
    "df.rename(columns={\"v1\":\"target\", \"v2\":\"text\"}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Feeding in random text messages to a predictive algorithm is unlikely to be effective. We need to transform our random text into a more consistent data structure, that we can feed into an alogorithm. This process is called Tokenizing. \n",
    "\n",
    "Tokenizing takes a sentance and transforms it into a list of tolkens - the words in the sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Very Simple Tokenizer\n",
    "\n",
    " We can visualize the process of tokenization pretty easily by looking at an example of a dead simple tokenizer. The function below will tokenize a sentence in a basic way - it will chop apart the sentence into words, and add them to a list. This example uses regex to do basic filtering to only extract words that are 2+ letters. \n",
    "\n",
    "<b>Note:</b> This example of a tokenizer (and this stuff in general) is a very basic version, and the field of NLP is developing quickly. More advanced text processing is better able to capture more of the structure of the language, and more of the meaning. We are stripping lots of \"hidden\" meaning out to make it manageable, more advanced NLP tries to understand as much of that meaning as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Really simple tokenizer\n",
    "def tokenize(sentence):\n",
    "    tokens = []\n",
    "    for token in re.findall(r\"\\b\\w\\w+\\b\", sentence):\n",
    "        tokens.append(token.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize a Thing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tolk = tokenize(df[\"text\"][5])\n",
    "print(df[\"text\"][5])\n",
    "tolk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize Results\n",
    "\n",
    "Tokenizing transforms our random text into something more orderly and able to be processed - in this case a list of words. This tokenization process is the basis of all other processing. \n",
    "\n",
    "We can take this set of tokens and do some further processing. For this we'll use something called a Vectorizer. The vectorizer will do the simple act of tokenizing, and build the actual data structure that we need as a feature set. \n",
    "\n",
    "#### Vocabulary\n",
    "\n",
    "The set of all our tokens, or all words used in our dataset is called the vocabulary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizers\n",
    "\n",
    "In sklearn we have some libraries called vectorizors - they can do much of the text processing for us. There are two that we'll touch on - CountVectorizer and Tf-idf Vectorizer. \n",
    "\n",
    "Each of these does the bulk of the prep for us:\n",
    "<ul>\n",
    "<li> Tokenize the strings. \n",
    "<li> Count the occurances of each. \n",
    "<li> Weight the relative importance of different words. In different ways...\n",
    "</ul> \n",
    "\n",
    "<b> Each takes in a dataset of text strings and outputs a set of features that we can use for our predictions. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorization\n",
    "\n",
    "Count vectorization is the most simple process we can use to make our text into a set of features. The count vectorization will split apart our data into tokens, count them up, and produce an array where:\n",
    "<ul>\n",
    "<li> Each column is a word. \n",
    "<li> Each row is an input piece of text (e.g. an email)\n",
    "<li> Each cell is a count of the number of times that word appears. \n",
    "</ul>\n",
    "\n",
    "This is our Bag of Words - now instead of having a sentence as an input, we have something like a one-hot matrix of words used. We can picture this by printing it out (Note: there's a little reconstruction below to put it into a nice dataframe format)\n",
    "\n",
    "#### Count Vectorizer Benefits and Drawbacks\n",
    "\n",
    "The main benefit of the count vectorizer is the simplicity and speed - all it needs to do is count. It has the downside of being quite simple in the analysis of the language - we don't extract which words are more or less important, we just get a count. For things that are written similarly this can be effective - I have used this for a simple tool to detect cheaters on tests - people copying from each other or a common source like Chegg tend to have the same words repeated in their answer. \n",
    "\n",
    "#### Sparse Features\n",
    "\n",
    "This process generally produces a sparse matrix - most words are not in most sentences, so most scores in the final matrix are 0. For this we'll keep it simple and use algorithms that deal with sparse matrices (e.g. SVC). Later on we'll look at ways to reduce dimensionality.\n",
    "\n",
    "Some algorithms may throw an error if you feed them a sparse matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_cv = CountVectorizer(max_features=150)\n",
    "tmp = vec_cv.fit_transform(df[\"text\"])\n",
    "tok_cols = vec_cv.get_feature_names()\n",
    "tok_df = pd.DataFrame(tmp.toarray(), columns=tok_cols)\n",
    "print(\"original:\", df[\"text\"].shape)\n",
    "print(\"vectorized:\", tmp.shape)\n",
    "tok_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That has the number of features limited to 150, if we imposed no limits we'd get something way messier..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_cv2 = CountVectorizer()\n",
    "tmp2 = vec_cv2.fit_transform(df[\"text\"])\n",
    "tok_cols2 = vec_cv2.get_feature_names()\n",
    "tok_df2 = pd.DataFrame(tmp2.toarray(), columns=tok_cols2)\n",
    "print(\"original:\", df[\"text\"].shape)\n",
    "print(\"vectorized:\", tmp2.shape)\n",
    "tok_df2.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorization\n",
    "\n",
    "TF-IDF vectorization is similar to the count vectorizor, but it does some calculations to determine the importance of the word. The calculations are based on the name:\n",
    "<ul>\n",
    "<li> Term Frequency - the number of times a word appears in a document divided by the total number of words in the document.\n",
    "<li> Inverse Document Frequency - the log of the total number of documents divided by the number of documents that contain the word.\n",
    "<li> Note: each input phrase (row in dataset) is a document. \n",
    "</ul>\n",
    "\n",
    "![TF-IDF](images/tfidf.png \"TF-IDF\" )\n",
    "\n",
    "The final result is the two multiplied by each other, hence TF-IDF. \n",
    "\n",
    "#### TF-IDF Importance\n",
    "\n",
    "TF-IDF weights the importance of each word to give lower scores to words that are:\n",
    "<ul>\n",
    "<li> Too frequent - words that repeat constantly are likely to not be helpful in differentiating sentences. \n",
    "<li> Too rare - words that almost never occur don't exist often enough to establish a pattern. \n",
    "</ul>\n",
    "\n",
    "TF-IDF tends to give us a better ability to evaluate work importance, but it is still not able to extract relationships between words nor generate more sophisticated meaning of the words. For that we need to use more sophisticated processing libraries, such as word2vec that we'll look at later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF\n",
    "vec_tf = TfidfVectorizer(max_features=150)\n",
    "tmp = vec_tf.fit_transform(df[\"text\"])\n",
    "tok_cols = vec_tf.get_feature_names()\n",
    "tok_df = pd.DataFrame(tmp.toarray(), columns=tok_cols)\n",
    "print(\"original:\", df[\"text\"].shape)\n",
    "print(\"vectorized:\", tmp.shape)\n",
    "tok_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorization Parameters\n",
    "\n",
    "There are several parameters that can be pretty important when doing vectorization:\n",
    "<ul>\n",
    "<li> Max Features - as seen above. Limits how many feature columns are produced. This will cap it to the N most frequent words instead of every word seen. \n",
    "<li> strip_accents - remove random characters such as accents. \n",
    "<li> lowercase - covert all to lower case. This is helpful as case matters in code, but doesn't matter for us. \n",
    "<li> stop_words - filter out stop words. More on this later. \n",
    "<li> tokenizer - we can specify our own tokenizer function, where we can layer in more processing. More on this later. \n",
    "<li> ngram_range - how \"big\" can tokens be? I.e. can you have a 2 word token - e.g. \"downhill skiing\". \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive Model with Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "model_svc = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_tf = TfidfVectorizer(max_features=150)\n",
    "vec_cv = CountVectorizer(max_features=150)\n",
    "\n",
    "y = df[\"target\"]\n",
    "X = df[\"text\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "pipe1 = Pipeline([ \n",
    "                    #(\"vect\", vec_cv),\n",
    "                    (\"vect\", vec_tf),\n",
    "                    (\"model\", model_svc)\n",
    "])\n",
    "\n",
    "params = [\"vec_cv\"]\n",
    "\n",
    "pipe1.fit(X_train, y_train.ravel())\n",
    "preds = pipe1.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, preds))\n",
    "sns.heatmap(confusion_matrix(y_test, preds), annot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "Our predictions are pretty good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Elaborate Language\n",
    "\n",
    "In the example above we've done a \"base\" level of modelling - we transform the free text into something that we can process (the bag of words), and we can make predictions from it much like we would with any other one-hot encoded data. This process works fine, and it does deliver some pretty accurate results on our test data. \n",
    "\n",
    "To create NLP models that are more functional we can add some layers to our processing of the text to improve our understanding of the nuances of our text. Some things we can do are:\n",
    "<ul>\n",
    "<li> Remove Stop Words - common words like \"it\", \"a\", \"the\" are normally not all that useful in predicting the meaning, we can filter these out. \n",
    "<li> Stemming - coverting words down to their \"stem\". E.g. \"reasoning\" to \"reason\"\n",
    "<li> Lemmatization - similar to stemming, but tries to identify the correct stem contextually. E.g. \"Operating systems\" probably shouldn't become \"operate\" and \"system\"\n",
    "</ul>\n",
    "\n",
    "In general, stemming increases recall while harming precision. Lemmatization has similar impacts, but tends to be less aggressive, so the effects are smaller. The specific results are highly variable depending on the exact text that is used. Something that uses specific variations of words to mean specific things (e.g. science) is more likely to get no benefit or be negatively impacted - e.g. \"conditonally\" used in the context of a \"conditionally approved loan\" is probably not well represented by changing it to \"condition\". \n",
    "\n",
    "### NLTK\n",
    "\n",
    "NLTK provides a bunch of language processing stuff that we can use such as stop words and tokenizers. We'll leverage it here to make custom tokenizers to incorporate some of those features above. \n",
    "\n",
    "The \"for package\" part there downloads the wordsets to your computer. NLTK has these prebuilt libraries of data that allow for the functions to do the stop words, stemming, and lemmatization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "for package in ['stopwords','punkt','wordnet']:\n",
    "    nltk.download(package)\n",
    "    \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words    = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customized Tokenizers\n",
    "\n",
    "The vecorization libraries in sklearn allow you do specify the function to use to do tokenization. We can use this to include other processing that we'd like as part of the process, such as removing stop words or stemming. \n",
    "\n",
    "The tokenizer functions below can, potentially, contain anything you'd like. As long as the call function returns a list of tokens, it should work. \n",
    "\n",
    "Note: if you look up examples, these functions will often be written into one lines, I broke them out so they're hopefully easier to read. \n",
    "\n",
    "#### Stop Words\n",
    "\n",
    "First, we will try to make a stop word tokenizer. If something is a stop word, we shall leave it out. As noted above, we can build this into the vectorizer, so why do it? This will allow for customizing the stopwords used - some applications may have a different usage of words, so changing stopwords makes sense. \n",
    "\n",
    "This is also the most simple example we can try :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class swTokenizer(object):\n",
    "    def __init__(self, stop_words):\n",
    "        self.stop_words = stop_words\n",
    "    def __call__(self, doc):\n",
    "        tokens = word_tokenize(doc)\n",
    "        filtered_tok = []\n",
    "        for tok in tokens:\n",
    "            if tok not in stop_words:\n",
    "                filtered_tok.append(tok)\n",
    "        return filtered_tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming and Lemmatization\n",
    "\n",
    "Stemming and lemmatization are similar - they both aim to break words down to their \"root\". For example, the word \"shoes\" probably has the same meaning as the word \"shoe\" for our purposes. \n",
    "\n",
    "##### Stemming\n",
    "\n",
    "Stemming is the most simple, it just removes common prefixes and suffixes to extract the root of the word. \n",
    "\n",
    "##### Lemmatization\n",
    "\n",
    "Lemmatization is slightly more sophisticated, it attempts to find the semantic root, called the lemma, of a word using a search of a dictionary (we provide one from NLTK). For example, the lemma of \"better\" is \"good\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stemTokenizer(object):\n",
    "    def __init__(self, stop_words):\n",
    "        self.stop_words = stop_words\n",
    "        from nltk.stem import SnowballStemmer\n",
    "        self.stemmer = SnowballStemmer(language='english')\n",
    "    def __call__(self, doc):\n",
    "        tokens = word_tokenize(doc)\n",
    "        filtered_tok = []\n",
    "        for tok in tokens:\n",
    "            if tok not in stop_words:\n",
    "                filtered_tok.append(self.stemmer.stem(tok))\n",
    "        return filtered_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lemmaTokenizer(object):\n",
    "    def __init__(self, stop_words):\n",
    "        self.stop_words = stop_words\n",
    "        from nltk.stem import WordNetLemmatizer\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        tokens = word_tokenize(doc)\n",
    "        filtered_tok = []\n",
    "        for tok in tokens:\n",
    "            if tok not in stop_words:\n",
    "                filtered_tok.append(self.lemmatizer.lemmatize(tok))\n",
    "        return filtered_tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions with More Processing and Grid Search\n",
    "\n",
    "We can try to see which processing setup works best. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_tf = TfidfVectorizer()\n",
    "\n",
    "y = df[\"target\"]\n",
    "X = df[\"text\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "pipe2 = Pipeline([ \n",
    "                    #(\"vect\", vec_cv),\n",
    "                    (\"vect\", vec_tf),\n",
    "                    (\"model\", model_svc)\n",
    "])\n",
    "\n",
    "params = {\"vect__max_features\":[100,500,1000,1500,2000],\n",
    "            \"vect__tokenizer\":(swTokenizer(stop_words), stemTokenizer(stop_words), lemmaTokenizer(stop_words) ),\n",
    "            \"vect__norm\":[\"l1\",\"l2\"]\n",
    "            }\n",
    "\n",
    "grid = GridSearchCV(estimator  = pipe2, \n",
    "                               param_grid = params, \n",
    "                               scoring    = \"balanced_accuracy\",\n",
    "                               cv         = 5,\n",
    "                               n_jobs     =-1)\n",
    "\n",
    "grid.fit(X_train, y_train.ravel())\n",
    "best = grid.best_estimator_\n",
    "preds = best.predict(X_test)\n",
    "print(best)\n",
    "print(classification_report(y_test, preds))\n",
    "sns.heatmap(confusion_matrix(y_test, preds), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Categorize the following newsgroups. The data are posts from different newgroup boards. Try to categorize the data in either the atheism or religion groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "remove = (\"headers\", \"footers\", \"quotes\")\n",
    "categories = [\"alt.atheism\", \"talk.religion.misc\"]\n",
    "\n",
    "data_train = fetch_20newsgroups(\n",
    "    subset=\"train\", categories=categories, shuffle=True, random_state=42, remove=remove)\n",
    "\n",
    "data_test = fetch_20newsgroups(\n",
    "    subset=\"test\", categories=categories, shuffle=True, random_state=42, remove=remove)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d722d3adfa415172c1f5238b519fb86b488acdae450fd691ab06c09f4ca9173"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ml3950': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
