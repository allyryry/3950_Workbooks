{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "PTC5srJ2bYG9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras \n",
        "from tensorflow.keras import layers\n",
        "#from tensorflow.keras import Sequential\n",
        "#from tensorflow.keras import Dense, Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, InputLayer, Reshape\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from keras.utils import np_utils\n",
        "\n",
        "import matplotlib as mpl\n",
        "#import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import TruncatedSVD"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper to plot loss\n",
        "def plot_loss(history):\n",
        "  plt.plot(history.history['loss'], label='loss')\n",
        "  plt.plot(history.history['val_loss'], label='val_loss')\n",
        "  plt.legend()\n",
        "  plt.grid(True)"
      ],
      "metadata": {
        "id": "kt2r1tfRg0cj"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SS8tDLiFbYG_"
      },
      "source": [
        "# Keras and Tensorflow Optimizations\n",
        "\n",
        "There are several things that we can do to make our networks a bit better. Unfortunately for much of this there aren't definitive answers for \"what is the best choice\", so we do have to do some trial and error, but we can use some guidelines to get us started in the right direction. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQLYUA1XbYHB"
      },
      "source": [
        "## Load MNIST Data\n",
        "\n",
        "We can use the MNIST digit dataset for testing, since it is reasonably large. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGygrQlobYHC",
        "outputId": "ec493133-483d-4887-9343-be4184cbef51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28)\n",
            "Epoch 1/10\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.2202 - accuracy: 0.9356 - val_loss: 0.1135 - val_accuracy: 0.9663\n",
            "Epoch 2/10\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.0891 - accuracy: 0.9729 - val_loss: 0.0992 - val_accuracy: 0.9716\n",
            "Epoch 3/10\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0554 - accuracy: 0.9829 - val_loss: 0.0909 - val_accuracy: 0.9712\n",
            "Epoch 4/10\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.0384 - accuracy: 0.9883 - val_loss: 0.0767 - val_accuracy: 0.9777\n",
            "Epoch 5/10\n",
            "1500/1500 [==============================] - 7s 4ms/step - loss: 0.0286 - accuracy: 0.9912 - val_loss: 0.0765 - val_accuracy: 0.9784\n",
            "Epoch 6/10\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.0223 - accuracy: 0.9924 - val_loss: 0.0833 - val_accuracy: 0.9783\n",
            "Epoch 7/10\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.0158 - accuracy: 0.9953 - val_loss: 0.1032 - val_accuracy: 0.9734\n",
            "Epoch 8/10\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.0150 - accuracy: 0.9948 - val_loss: 0.0944 - val_accuracy: 0.9766\n",
            "Epoch 9/10\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.0118 - accuracy: 0.9961 - val_loss: 0.0867 - val_accuracy: 0.9802\n",
            "Epoch 10/10\n",
            "1500/1500 [==============================] - 7s 4ms/step - loss: 0.0115 - accuracy: 0.9959 - val_loss: 0.0955 - val_accuracy: 0.9787\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0690c28850>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# Load MNIST dataset\n",
        "mnist = keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Normalize the input image so that each pixel value is between 0 and 1.\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# Define the model architecture.\n",
        "#model = keras.Sequential([\n",
        "#  keras.layers.InputLayer(input_shape=(28, 28)),\n",
        "#  keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
        "#  keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
        "#  keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "#  keras.layers.Flatten(),\n",
        "#  keras.layers.Dense(10)\n",
        "#])\n",
        "print(train_images.shape)\n",
        "#train_images = train_images.reshape((train_images.shape[0], 28*28)).astype('float32')\n",
        "#test_images = test_images.reshape((test_images.shape[0], 28*28)).astype('float32')\n",
        "\n",
        "train_labels = np_utils.to_categorical(train_labels)\n",
        "test_labels = np_utils.to_categorical(test_labels)\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(InputLayer(input_shape=(28, 28)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "# Train the digit classification model\n",
        "model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics='accuracy')\n",
        "\n",
        "model.fit(\n",
        "  train_images,\n",
        "  train_labels,\n",
        "  epochs=10,\n",
        "  validation_split=0.2,\n",
        ")\n",
        "print(model.evaluate(test_images, test_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsycEGiGbYHE"
      },
      "source": [
        "## Prequel - Saving and Loading Models\n",
        "\n",
        "As we've seen, models can take a long time to train in many cases. Like with the sklearn models, we can save and load ours as they are trained and reused. This is a pretty integral part of making neural network models usable, so it is pretty easy. \n",
        "\n",
        "In addition to this we often see models saved in the h5 format, which just saves slightly less stuff along with the model. If we are using models trained elsewhere this format is very common. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2PSIZDbbYHE"
      },
      "outputs": [],
      "source": [
        "# Save my model\n",
        "model.save('model_path')\n",
        "model = keras.models.load_model('model_path')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEJgmNFYbYHE"
      },
      "outputs": [],
      "source": [
        "# Calling `save('my_model.h5')` creates a h5 file `my_model.h5`.\n",
        "model.save(\"my_h5_model.h5\")\n",
        "\n",
        "# It can be used to reconstruct the model identically.\n",
        "reconstructed_model = keras.models.load_model(\"my_h5_model.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoRkV4nTbYHF"
      },
      "source": [
        "## Network Size\n",
        "\n",
        "Probably the first question that we will think of when building networks through Tensorflow is \"how big should it be\"? This is a very big question, and one of those ones without a real answer. We can put some guidelines in place to help us though. \n",
        "\n",
        "### What Does the Size Mean?\n",
        "\n",
        "The size of a neural network is also known as the capacity. We can relate it roughly to the size of our first model, the tree. The larger a network is the higher its capacity to learn. \n",
        "\n",
        "### What Size to Use\n",
        "\n",
        "We can start with a few guidelines to have a reasonably sized neural network. These steps do not ensure an optimal solution, but they'll get us started. There really is not a prescribed method for calculating the optimal network size (beleive me, I've looked), but there are several rules of thumb we can build together:\n",
        "\n",
        "<ul>\n",
        "<li> Start with an input layer that is either\n",
        "    <ul>\n",
        "    <li> The width of the data, if the feature set is relatively small. \n",
        "    <li> A reasonably large number if the feature set is large. \n",
        "    <li> We don't have a true diving line, but 512 is a reasonable value to try for an upper end, at least at first. \n",
        "    </ul>\n",
        "<li> Add 1 or 2 hidden layers of the same size and observe the results. We want to keep the model smaller if making it larger doesn't improve things, so first we shoudl see how good of a job a small model does. If the data is very large, skipping past the 1 layer step may save some time since we can predict that we can do better with a larger model in advance. \n",
        "<li> Increase layers of the same size until we get some overfitting and the training loss flattens. We want to reach the point where the model is getting to be excellent at predicting the training data. This is something we can see in the plot by noticing that the validation loss flatlines or starts to get worse. The training loss flattening is an indication that the model is not getting any better at learning the training data; we can use early stopping with a loose patience setting on training loss and lots of epochs to find this. \n",
        "<li> Add regularization steps to cut down that overfitting. We can try regularization and dropouts to cut down on that overfitting. We probably want to try a few options, parameters, and combinations here, there's not really a way to know in advance which regularization will work best on our data. \n",
        "<li> \"Funnel\" the layer size, potentially adding more layers. The traditional configuration of layers is to gradually decrease the size from the input layer towards the output layer. There is open debate on if this is better than having layers that are all the same size. We can play with this a little to see if results improve or not. \n",
        "<li> Use pruning. Much like a tree we can prune back a model to fight overfitting. \n",
        "</ul>\n",
        "\n",
        "### Height vs Width\n",
        "\n",
        "Another begged question is should we make networks wider (more neurons) or deeper (more layers)? Once again, there's no universal answer, but the general evidence leans towards more layers. There are several reasons for this, none of them definitive, but taken as a whole they add up to a strong case:\n",
        "\n",
        "<ul>\n",
        "<li> Ability to learn different representation of the data - this will be more clear next time when we start to look at some image specific neural networks, but one of the cool features of neural networks is that at each layer the network \"sees\" a different representation of the data, as it goes through each round of transformations. This has the effect of allowing it to identify different features at each layer, and use those features to make more and more accurate predictions. We'll examine this more soon. \n",
        "<li> Avoiding overfitting - extremely wide neural networks tend towards overfitting the training data and not generalizing as well to new data. \n",
        "<li> Ability to add interim steps - with a multi layer network we can add multiple steps such as regularization or dropouts, again to fight overfitting. \n",
        "<li> Automatic feature selection - deep neural networks will automatically perform a type of feature selection as the least important features are minimized in their importance. This is an emerging area of research - some people have argued that well designed neural networks can remove the need for feature selection, and neural networks are being created to be feature selection tools. We can see this illustrated most clearly with images again, we feed a network an entire image, and get a prediction. Note that this isn't a total rejection of feature selection for neural networks, improving the feature set will impact neural network models just as it will for ordinary models; with neural networks we just have the potential for the network to \"cover for mistakes\" in the features. This is more dramatic as data size and network size increase. \n",
        "<li> Results - deep learning has become a common term recently for a reason, due to the success of deep neural networks with many layers. Most of the cool stuff that we see coming from AI such as image recognition, translation, and self navigating robots are the result of deep learning networks. In practice these networks have tended to outperform shallower ones, especially in more complex tasks. \n",
        "</ul>\n",
        "\n",
        "Why not make a model that is both very wide and very deep? This will tend to overfit as it can \"memorize\" the training data. With large datasets we do see very large models in some cases, since the more data we have, the more fitting we can handle. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Big Model\n",
        "model = keras.Sequential()\n",
        "model.add(InputLayer(input_shape=(28, 28)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(784, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(784, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "# Train the digit classification model\n",
        "model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics='accuracy')\n",
        "\n",
        "model.fit(\n",
        "  train_images,\n",
        "  train_labels,\n",
        "  epochs=10,\n",
        "  validation_split=0.2,\n",
        ")"
      ],
      "metadata": {
        "id": "e-CRvieaiZI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJkWGpsmbYHG",
        "outputId": "90c2628b-5210-488e-d17a-60a5232802c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "1500/1500 [==============================] - 10s 6ms/step - loss: 0.2887 - accuracy: 0.9139 - val_loss: 0.1424 - val_accuracy: 0.9595\n",
            "Epoch 2/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.1444 - accuracy: 0.9584 - val_loss: 0.1212 - val_accuracy: 0.9641\n",
            "Epoch 3/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.1184 - accuracy: 0.9659 - val_loss: 0.1212 - val_accuracy: 0.9678\n",
            "Epoch 4/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.1026 - accuracy: 0.9715 - val_loss: 0.1203 - val_accuracy: 0.9699\n",
            "Epoch 5/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0886 - accuracy: 0.9753 - val_loss: 0.0921 - val_accuracy: 0.9758\n",
            "Epoch 6/40\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 0.0807 - accuracy: 0.9779 - val_loss: 0.1048 - val_accuracy: 0.9741\n",
            "Epoch 7/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0738 - accuracy: 0.9802 - val_loss: 0.1194 - val_accuracy: 0.9749\n",
            "Epoch 8/40\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 0.0730 - accuracy: 0.9806 - val_loss: 0.1002 - val_accuracy: 0.9759\n",
            "Epoch 9/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0633 - accuracy: 0.9831 - val_loss: 0.1195 - val_accuracy: 0.9737\n",
            "Epoch 10/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0610 - accuracy: 0.9847 - val_loss: 0.1042 - val_accuracy: 0.9770\n",
            "Epoch 11/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0651 - accuracy: 0.9837 - val_loss: 0.1007 - val_accuracy: 0.9768\n",
            "Epoch 12/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0563 - accuracy: 0.9860 - val_loss: 0.1072 - val_accuracy: 0.9800\n",
            "Epoch 13/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0533 - accuracy: 0.9861 - val_loss: 0.1247 - val_accuracy: 0.9786\n",
            "Epoch 14/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0546 - accuracy: 0.9864 - val_loss: 0.1257 - val_accuracy: 0.9767\n",
            "Epoch 15/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0532 - accuracy: 0.9869 - val_loss: 0.1065 - val_accuracy: 0.9783\n",
            "Epoch 16/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0489 - accuracy: 0.9882 - val_loss: 0.1401 - val_accuracy: 0.9769\n",
            "Epoch 17/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0516 - accuracy: 0.9876 - val_loss: 0.1109 - val_accuracy: 0.9800\n",
            "Epoch 18/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0543 - accuracy: 0.9879 - val_loss: 0.1193 - val_accuracy: 0.9793\n",
            "Epoch 19/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0458 - accuracy: 0.9891 - val_loss: 0.1156 - val_accuracy: 0.9808\n",
            "Epoch 20/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0454 - accuracy: 0.9891 - val_loss: 0.1382 - val_accuracy: 0.9753\n",
            "Epoch 21/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0455 - accuracy: 0.9895 - val_loss: 0.1254 - val_accuracy: 0.9775\n",
            "Epoch 22/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0407 - accuracy: 0.9901 - val_loss: 0.1208 - val_accuracy: 0.9823\n",
            "Epoch 23/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0461 - accuracy: 0.9896 - val_loss: 0.1710 - val_accuracy: 0.9735\n",
            "Epoch 24/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0459 - accuracy: 0.9896 - val_loss: 0.1453 - val_accuracy: 0.9766\n",
            "Epoch 25/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0462 - accuracy: 0.9901 - val_loss: 0.1569 - val_accuracy: 0.9818\n",
            "Epoch 26/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0439 - accuracy: 0.9901 - val_loss: 0.1956 - val_accuracy: 0.9793\n",
            "Epoch 27/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0463 - accuracy: 0.9902 - val_loss: 0.1464 - val_accuracy: 0.9788\n",
            "Epoch 28/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0400 - accuracy: 0.9906 - val_loss: 0.1371 - val_accuracy: 0.9786\n",
            "Epoch 29/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0392 - accuracy: 0.9914 - val_loss: 0.1422 - val_accuracy: 0.9782\n",
            "Epoch 30/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0419 - accuracy: 0.9913 - val_loss: 0.1394 - val_accuracy: 0.9817\n",
            "Epoch 31/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0401 - accuracy: 0.9920 - val_loss: 0.1538 - val_accuracy: 0.9804\n",
            "Epoch 32/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0460 - accuracy: 0.9908 - val_loss: 0.1576 - val_accuracy: 0.9779\n",
            "Epoch 33/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0418 - accuracy: 0.9914 - val_loss: 0.1577 - val_accuracy: 0.9791\n",
            "Epoch 34/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0349 - accuracy: 0.9921 - val_loss: 0.1415 - val_accuracy: 0.9807\n",
            "Epoch 35/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0393 - accuracy: 0.9924 - val_loss: 0.1571 - val_accuracy: 0.9793\n",
            "Epoch 36/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0387 - accuracy: 0.9922 - val_loss: 0.2065 - val_accuracy: 0.9790\n",
            "Epoch 37/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0479 - accuracy: 0.9918 - val_loss: 0.1900 - val_accuracy: 0.9800\n",
            "Epoch 38/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0384 - accuracy: 0.9926 - val_loss: 0.2015 - val_accuracy: 0.9803\n",
            "Epoch 39/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0376 - accuracy: 0.9927 - val_loss: 0.1577 - val_accuracy: 0.9818\n",
            "Epoch 40/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0350 - accuracy: 0.9929 - val_loss: 0.1634 - val_accuracy: 0.9803\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0690a46dd0>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# Big Model\n",
        "model = keras.Sequential()\n",
        "model.add(InputLayer(input_shape=(28, 28)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "# Train the digit classification model\n",
        "model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics='accuracy')\n",
        "\n",
        "model.fit(\n",
        "  train_images,\n",
        "  train_labels,\n",
        "  epochs=10,\n",
        "  validation_split=0.2,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tapered Model\n",
        "model = keras.Sequential()\n",
        "model.add(InputLayer(input_shape=(28, 28)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(500, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(350, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(200, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "# Train the digit classification model\n",
        "model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics='accuracy')\n",
        "\n",
        "model.fit(\n",
        "  train_images,\n",
        "  train_labels,\n",
        "  epochs=10,\n",
        "  validation_split=0.2,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmRw70jwga-g",
        "outputId": "13f54c1c-6c6b-483b-c265-3409f210f942"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.3825 - accuracy: 0.8887 - val_loss: 0.1608 - val_accuracy: 0.9557\n",
            "Epoch 2/40\n",
            "1500/1500 [==============================] - 8s 6ms/step - loss: 0.1757 - accuracy: 0.9555 - val_loss: 0.1435 - val_accuracy: 0.9614\n",
            "Epoch 3/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.1398 - accuracy: 0.9637 - val_loss: 0.1063 - val_accuracy: 0.9737\n",
            "Epoch 4/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.1158 - accuracy: 0.9696 - val_loss: 0.1220 - val_accuracy: 0.9698\n",
            "Epoch 5/40\n",
            "1500/1500 [==============================] - 8s 6ms/step - loss: 0.1016 - accuracy: 0.9734 - val_loss: 0.1057 - val_accuracy: 0.9733\n",
            "Epoch 6/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0905 - accuracy: 0.9768 - val_loss: 0.0978 - val_accuracy: 0.9774\n",
            "Epoch 7/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0805 - accuracy: 0.9794 - val_loss: 0.0986 - val_accuracy: 0.9768\n",
            "Epoch 8/40\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 0.0718 - accuracy: 0.9813 - val_loss: 0.1019 - val_accuracy: 0.9772\n",
            "Epoch 9/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0637 - accuracy: 0.9834 - val_loss: 0.0957 - val_accuracy: 0.9776\n",
            "Epoch 10/40\n",
            "1500/1500 [==============================] - 8s 6ms/step - loss: 0.0652 - accuracy: 0.9828 - val_loss: 0.1045 - val_accuracy: 0.9789\n",
            "Epoch 11/40\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 0.0633 - accuracy: 0.9841 - val_loss: 0.0973 - val_accuracy: 0.9781\n",
            "Epoch 12/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0568 - accuracy: 0.9859 - val_loss: 0.0932 - val_accuracy: 0.9792\n",
            "Epoch 13/40\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 0.0518 - accuracy: 0.9862 - val_loss: 0.1117 - val_accuracy: 0.9787\n",
            "Epoch 14/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0497 - accuracy: 0.9871 - val_loss: 0.1000 - val_accuracy: 0.9784\n",
            "Epoch 15/40\n",
            "1500/1500 [==============================] - 10s 6ms/step - loss: 0.0454 - accuracy: 0.9885 - val_loss: 0.1148 - val_accuracy: 0.9759\n",
            "Epoch 16/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0455 - accuracy: 0.9890 - val_loss: 0.1291 - val_accuracy: 0.9769\n",
            "Epoch 17/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0479 - accuracy: 0.9872 - val_loss: 0.1250 - val_accuracy: 0.9768\n",
            "Epoch 18/40\n",
            "1500/1500 [==============================] - 8s 6ms/step - loss: 0.0401 - accuracy: 0.9899 - val_loss: 0.1092 - val_accuracy: 0.9799\n",
            "Epoch 19/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0390 - accuracy: 0.9897 - val_loss: 0.1248 - val_accuracy: 0.9783\n",
            "Epoch 20/40\n",
            "1500/1500 [==============================] - 8s 6ms/step - loss: 0.0379 - accuracy: 0.9912 - val_loss: 0.1228 - val_accuracy: 0.9785\n",
            "Epoch 21/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0409 - accuracy: 0.9898 - val_loss: 0.1034 - val_accuracy: 0.9778\n",
            "Epoch 22/40\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 0.0382 - accuracy: 0.9906 - val_loss: 0.1299 - val_accuracy: 0.9807\n",
            "Epoch 23/40\n",
            "1500/1500 [==============================] - 14s 9ms/step - loss: 0.0342 - accuracy: 0.9920 - val_loss: 0.1239 - val_accuracy: 0.9833\n",
            "Epoch 24/40\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 0.0357 - accuracy: 0.9918 - val_loss: 0.1353 - val_accuracy: 0.9803\n",
            "Epoch 25/40\n",
            "1500/1500 [==============================] - 8s 6ms/step - loss: 0.0348 - accuracy: 0.9917 - val_loss: 0.1066 - val_accuracy: 0.9826\n",
            "Epoch 26/40\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 0.0295 - accuracy: 0.9925 - val_loss: 0.1322 - val_accuracy: 0.9803\n",
            "Epoch 27/40\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 0.0303 - accuracy: 0.9924 - val_loss: 0.1480 - val_accuracy: 0.9812\n",
            "Epoch 28/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0354 - accuracy: 0.9920 - val_loss: 0.1183 - val_accuracy: 0.9823\n",
            "Epoch 29/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0343 - accuracy: 0.9923 - val_loss: 0.1145 - val_accuracy: 0.9800\n",
            "Epoch 30/40\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 0.0286 - accuracy: 0.9929 - val_loss: 0.1326 - val_accuracy: 0.9815\n",
            "Epoch 31/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0272 - accuracy: 0.9935 - val_loss: 0.1289 - val_accuracy: 0.9813\n",
            "Epoch 32/40\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 0.0310 - accuracy: 0.9927 - val_loss: 0.1249 - val_accuracy: 0.9822\n",
            "Epoch 33/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0336 - accuracy: 0.9933 - val_loss: 0.1282 - val_accuracy: 0.9814\n",
            "Epoch 34/40\n",
            "1500/1500 [==============================] - 8s 6ms/step - loss: 0.0270 - accuracy: 0.9944 - val_loss: 0.1385 - val_accuracy: 0.9783\n",
            "Epoch 35/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0312 - accuracy: 0.9936 - val_loss: 0.1342 - val_accuracy: 0.9814\n",
            "Epoch 36/40\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 0.0272 - accuracy: 0.9939 - val_loss: 0.1497 - val_accuracy: 0.9818\n",
            "Epoch 37/40\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 0.0259 - accuracy: 0.9945 - val_loss: 0.1566 - val_accuracy: 0.9806\n",
            "Epoch 38/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0225 - accuracy: 0.9950 - val_loss: 0.1630 - val_accuracy: 0.9817\n",
            "Epoch 39/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0237 - accuracy: 0.9949 - val_loss: 0.1805 - val_accuracy: 0.9809\n",
            "Epoch 40/40\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0309 - accuracy: 0.9939 - val_loss: 0.1635 - val_accuracy: 0.9800\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0685fea910>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwRjUyBMbYHG"
      },
      "source": [
        "## Epochs and Batch Sizes\n",
        "\n",
        "### Epochs\n",
        "\n",
        "Each epoch is a run through all of the training data. Epochs are simple, we can set a large number and use early stopping to cut things off when we've reached the best result. \n",
        "\n",
        "### Batch Sizes\n",
        "\n",
        "Batch size determines how many records are processed before the gradients are updated - i.e. the number of records between one forward and backwards pass.\n",
        "\n",
        "The batch sizes are a matter of very open debate for the optimal solution. At the high end, batch sizes are limited by what can fit in memory. When dealing with very large data this may matter as a batch that is a small fraction of the data may be a massive absolute size. At the lower end using smaller batches gives the same effect as it does when we looked at regular gradient descent - the gradients become less stable as we are relying on a smaller number of records. In reading more about batch sizes I want to update my recommendation to be even smaller than the 50 to 150 I suggested before, down to less than 100, even as small as into the single digits. There is research that smaller batch sizes tend to produce models that generalize better than ones with larger batches. \n",
        "\n",
        "Larger batch sizes do tend to be processed more quickly, sometimes substantially so, as the hardware is better able to be \"saturated\" with data to process. \n",
        "\n",
        "Dont' stress too much on batch size, this is really something that needs to be grid searched to find a great answer. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMFYDxPdbYHG",
        "outputId": "6986e26a-f543-4ec2-9ba8-5805d4871211"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "10/10 [==============================] - 1s 53ms/step - loss: 1.3543 - accuracy: 0.6221 - val_loss: 0.4702 - val_accuracy: 0.8680\n",
            "Epoch 2/10\n",
            "10/10 [==============================] - 0s 28ms/step - loss: 0.4829 - accuracy: 0.8537 - val_loss: 0.3105 - val_accuracy: 0.9085\n",
            "Epoch 3/10\n",
            "10/10 [==============================] - 0s 28ms/step - loss: 0.3535 - accuracy: 0.8961 - val_loss: 0.2437 - val_accuracy: 0.9295\n",
            "Epoch 4/10\n",
            "10/10 [==============================] - 0s 30ms/step - loss: 0.2751 - accuracy: 0.9197 - val_loss: 0.2063 - val_accuracy: 0.9403\n",
            "Epoch 5/10\n",
            "10/10 [==============================] - 0s 30ms/step - loss: 0.2300 - accuracy: 0.9331 - val_loss: 0.1785 - val_accuracy: 0.9482\n",
            "Epoch 6/10\n",
            "10/10 [==============================] - 0s 31ms/step - loss: 0.1980 - accuracy: 0.9423 - val_loss: 0.1580 - val_accuracy: 0.9525\n",
            "Epoch 7/10\n",
            "10/10 [==============================] - 0s 30ms/step - loss: 0.1723 - accuracy: 0.9497 - val_loss: 0.1449 - val_accuracy: 0.9568\n",
            "Epoch 8/10\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.1544 - accuracy: 0.9542 - val_loss: 0.1330 - val_accuracy: 0.9599\n",
            "Epoch 9/10\n",
            "10/10 [==============================] - 0s 26ms/step - loss: 0.1376 - accuracy: 0.9606 - val_loss: 0.1235 - val_accuracy: 0.9629\n",
            "Epoch 10/10\n",
            "10/10 [==============================] - 0s 24ms/step - loss: 0.1247 - accuracy: 0.9641 - val_loss: 0.1157 - val_accuracy: 0.9664\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0685ebcfd0>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "# Big Batch\n",
        "model = keras.Sequential()\n",
        "model.add(InputLayer(input_shape=(28, 28)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(500, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(350, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "# Train the digit classification model\n",
        "model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics='accuracy')\n",
        "\n",
        "model.fit(\n",
        "  train_images,\n",
        "  train_labels,\n",
        "  epochs=10,\n",
        "  batch_size=5000,\n",
        "  validation_split=0.2,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Small Batch\n",
        "model = keras.Sequential()\n",
        "model.add(InputLayer(input_shape=(28, 28)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(500, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(350, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "# Train the digit classification model\n",
        "model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics='accuracy')\n",
        "\n",
        "model.fit(\n",
        "  train_images,\n",
        "  train_labels,\n",
        "  epochs=10,\n",
        "  batch_size=2,\n",
        "  validation_split=0.2,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbJeAtnDipXo",
        "outputId": "6181bb71-7234-4854-b2a6-586384b6ab06"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "24000/24000 [==============================] - 106s 4ms/step - loss: 0.3083 - accuracy: 0.9146 - val_loss: 0.1766 - val_accuracy: 0.9500\n",
            "Epoch 2/10\n",
            "24000/24000 [==============================] - 105s 4ms/step - loss: 0.2268 - accuracy: 0.9441 - val_loss: 0.1681 - val_accuracy: 0.9578\n",
            "Epoch 3/10\n",
            "24000/24000 [==============================] - 117s 5ms/step - loss: 0.2132 - accuracy: 0.9523 - val_loss: 0.2054 - val_accuracy: 0.9598\n",
            "Epoch 4/10\n",
            "24000/24000 [==============================] - 106s 4ms/step - loss: 0.2119 - accuracy: 0.9539 - val_loss: 0.1963 - val_accuracy: 0.9642\n",
            "Epoch 5/10\n",
            "24000/24000 [==============================] - 130s 5ms/step - loss: 0.1973 - accuracy: 0.9569 - val_loss: 0.1966 - val_accuracy: 0.9668\n",
            "Epoch 6/10\n",
            "24000/24000 [==============================] - 107s 4ms/step - loss: 0.2046 - accuracy: 0.9590 - val_loss: 0.1686 - val_accuracy: 0.9663\n",
            "Epoch 7/10\n",
            "24000/24000 [==============================] - 106s 4ms/step - loss: 0.1919 - accuracy: 0.9614 - val_loss: 0.2586 - val_accuracy: 0.9622\n",
            "Epoch 8/10\n",
            "24000/24000 [==============================] - 106s 4ms/step - loss: 0.1956 - accuracy: 0.9613 - val_loss: 0.1743 - val_accuracy: 0.9720\n",
            "Epoch 9/10\n",
            "24000/24000 [==============================] - 114s 5ms/step - loss: 0.1852 - accuracy: 0.9637 - val_loss: 0.1873 - val_accuracy: 0.9716\n",
            "Epoch 10/10\n",
            "24000/24000 [==============================] - 105s 4ms/step - loss: 0.1872 - accuracy: 0.9642 - val_loss: 0.2073 - val_accuracy: 0.9628\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f069f526410>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUFvtavwbYHH"
      },
      "source": [
        "## Optimizer\n",
        "\n",
        "Of all options the optimizer is the one we will care about the least. Each different optimizer is a different algorithm for doing the gradient descent. The optimizers have different results with respect to speed, memory usage, computational expense, and likelyhood to get stuck in a local minima. \n",
        "\n",
        "Adam is a good compromise between all factors and is very commonly used. We'll just use this for our work. One other common one is RMSprop, if you're feeling spicy, give that a try and see if there are any imporvements. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qIRrRF2bYHH"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXb18AySbYHH"
      },
      "source": [
        "## Activation \n",
        "\n",
        "Activation functions are the key to adding non-linearity to the network allowing it to learn complex and non-linear relationships in the data. We've used ReLU as the default and that is a solid choice in most cases. \n",
        "\n",
        "ReLU has one issue, the dying ReLU problem. This can happen when we get inputs to the activation function fall in the negative area. In short there can be neurons that \"die\" and never get updated again. \n",
        "\n",
        "To combat the dying ReLU problem there are a couple of other activation functions that avoid that issue - Leaky ReLU and ELU. Each one changes the negative values to something other than 0 - Leaky ReLU uses a slight linear gradient, ELU uses an exponential function for a similar, but curved, slight gradient. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQ81hFfxbYHH",
        "outputId": "ae09411d-4c7f-4b47-efef-57d33e71c7f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "24000/24000 [==============================] - 114s 5ms/step - loss: 0.4397 - accuracy: 0.8867 - val_loss: 0.3563 - val_accuracy: 0.9218\n",
            "Epoch 2/10\n",
            "24000/24000 [==============================] - 104s 4ms/step - loss: 0.4086 - accuracy: 0.9161 - val_loss: 0.5587 - val_accuracy: 0.9029\n",
            "Epoch 3/10\n",
            "24000/24000 [==============================] - 105s 4ms/step - loss: 0.4283 - accuracy: 0.9231 - val_loss: 0.3020 - val_accuracy: 0.9463\n",
            "Epoch 4/10\n",
            "24000/24000 [==============================] - 102s 4ms/step - loss: 0.4715 - accuracy: 0.9267 - val_loss: 0.2907 - val_accuracy: 0.9537\n",
            "Epoch 5/10\n",
            "24000/24000 [==============================] - 115s 5ms/step - loss: 0.4909 - accuracy: 0.9324 - val_loss: 0.4184 - val_accuracy: 0.9472\n",
            "Epoch 6/10\n",
            "24000/24000 [==============================] - 103s 4ms/step - loss: 0.5237 - accuracy: 0.9354 - val_loss: 0.4313 - val_accuracy: 0.9508\n",
            "Epoch 7/10\n",
            "24000/24000 [==============================] - 112s 5ms/step - loss: 0.5526 - accuracy: 0.9358 - val_loss: 0.5097 - val_accuracy: 0.9566\n",
            "Epoch 8/10\n",
            "24000/24000 [==============================] - 112s 5ms/step - loss: 0.5872 - accuracy: 0.9380 - val_loss: 0.5838 - val_accuracy: 0.9513\n",
            "Epoch 9/10\n",
            "24000/24000 [==============================] - 103s 4ms/step - loss: 0.6101 - accuracy: 0.9403 - val_loss: 0.4845 - val_accuracy: 0.9572\n",
            "Epoch 10/10\n",
            "24000/24000 [==============================] - 102s 4ms/step - loss: 0.6290 - accuracy: 0.9414 - val_loss: 0.6564 - val_accuracy: 0.9464\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f069fa93250>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "# Take a leak \n",
        "model = keras.Sequential()\n",
        "model.add(InputLayer(input_shape=(28, 28)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(500, activation='leaky_relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(350, activation='leaky_relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "# Train the digit classification model\n",
        "model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics='accuracy')\n",
        "\n",
        "model.fit(\n",
        "  train_images,\n",
        "  train_labels,\n",
        "  epochs=10,\n",
        "  batch_size=64,\n",
        "  validation_split=0.2,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gi7DxgCsbYHH"
      },
      "source": [
        "## Initialization\n",
        "\n",
        "The initialization provides the starting point for all the weights and bias values that we start out with. We initially started with random values in the scratch network - this is generally fine, but we can sometimes do better. \n",
        "\n",
        "### Imbalanced Weighting\n",
        "\n",
        "One application where initialization can help significantly is when dealing with imbalanced data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "jf84UJGPbYHI",
        "outputId": "e3aacde8-116a-4198-db20-77e32d0c23cc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
              "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
              "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
              "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
              "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
              "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
              "\n",
              "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
              "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
              "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
              "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
              "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
              "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
              "\n",
              "        V26       V27       V28  Amount  Class  \n",
              "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
              "1  0.125895 -0.008983  0.014724    2.69      0  \n",
              "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
              "3 -0.221929  0.062723  0.061458  123.50      0  \n",
              "4  0.502292  0.219422  0.215153   69.99      0  \n",
              "\n",
              "[5 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f143efb6-ea0e-4000-9bc2-5cea882a734e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>...</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.62</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>...</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>378.66</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>123.50</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.0</td>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>69.99</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  31 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f143efb6-ea0e-4000-9bc2-5cea882a734e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f143efb6-ea0e-4000-9bc2-5cea882a734e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f143efb6-ea0e-4000-9bc2-5cea882a734e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "file = tf.keras.utils\n",
        "raw_df = pd.read_csv('https://storage.googleapis.com/download.tensorflow.org/data/creditcard.csv')\n",
        "raw_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "neg, pos = np.bincount(raw_df['Class'])\n",
        "total = neg + pos\n",
        "print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n",
        "    total, pos, 100 * pos / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ec08WeQEi_Fz",
        "outputId": "480f3240-a7bb-4bca-b1b5-0a375c0fbcd8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Examples:\n",
            "    Total: 284807\n",
            "    Positive: 492 (0.17% of total)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We Have an Imbalance"
      ],
      "metadata": {
        "id": "9gr-lqY0tHwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df = raw_df.copy()\n",
        "\n",
        "# You don't want the `Time` column.\n",
        "cleaned_df.pop('Time')\n",
        "\n",
        "# The `Amount` column covers a huge range. Convert to log-space.\n",
        "eps = 0.001 # 0 => 0.1\n",
        "cleaned_df['Log Ammount'] = np.log(cleaned_df.pop('Amount')+eps)"
      ],
      "metadata": {
        "id": "w5S02N45tGfu"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a utility from sklearn to split and shuffle your dataset.\n",
        "train_df, test_df = train_test_split(cleaned_df, test_size=0.2)\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.2)\n",
        "\n",
        "# Form np arrays of labels and features.\n",
        "train_labels = np.array(train_df.pop('Class'))\n",
        "bool_train_labels = train_labels != 0\n",
        "val_labels = np.array(val_df.pop('Class'))\n",
        "test_labels = np.array(test_df.pop('Class'))\n",
        "\n",
        "train_features = np.array(train_df)\n",
        "val_features = np.array(val_df)\n",
        "test_features = np.array(test_df)"
      ],
      "metadata": {
        "id": "-3538h2djFHF"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "train_features = scaler.fit_transform(train_features)\n",
        "\n",
        "val_features = scaler.transform(val_features)\n",
        "test_features = scaler.transform(test_features)\n",
        "\n",
        "train_features = np.clip(train_features, -5, 5)\n",
        "val_features = np.clip(val_features, -5, 5)\n",
        "test_features = np.clip(test_features, -5, 5)\n",
        "\n",
        "\n",
        "print('Training labels shape:', train_labels.shape)\n",
        "print('Validation labels shape:', val_labels.shape)\n",
        "print('Test labels shape:', test_labels.shape)\n",
        "\n",
        "print('Training features shape:', train_features.shape)\n",
        "print('Validation features shape:', val_features.shape)\n",
        "print('Test features shape:', test_features.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GC5P3kaBjMwN",
        "outputId": "f9b7f315-87dd-4e8a-f7ec-98a045bd64b3"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training labels shape: (182276,)\n",
            "Validation labels shape: (45569,)\n",
            "Test labels shape: (56962,)\n",
            "Training features shape: (182276, 29)\n",
            "Validation features shape: (45569, 29)\n",
            "Test features shape: (56962, 29)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "METRICS = [\n",
        "      keras.metrics.TruePositives(name='tp'),\n",
        "      keras.metrics.FalsePositives(name='fp'),\n",
        "      keras.metrics.TrueNegatives(name='tn'),\n",
        "      keras.metrics.FalseNegatives(name='fn'), \n",
        "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      keras.metrics.Precision(name='precision'),\n",
        "      keras.metrics.Recall(name='recall'),\n",
        "      keras.metrics.AUC(name='auc'),\n",
        "      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
        "]\n",
        "\n",
        "def make_model(metrics=METRICS, output_bias=None):\n",
        "  if output_bias is not None:\n",
        "    output_bias = tf.keras.initializers.Constant(output_bias)\n",
        "  model = keras.Sequential([\n",
        "      keras.layers.Dense(\n",
        "          16, activation='relu',\n",
        "          input_shape=(train_features.shape[-1],)),\n",
        "      keras.layers.Dropout(0.5),\n",
        "      keras.layers.Dense(1, activation='sigmoid',\n",
        "                         bias_initializer=output_bias),\n",
        "  ])\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "      loss=keras.losses.BinaryCrossentropy(),\n",
        "      metrics=metrics)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "nO7R0L7XjTZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdPzxFgXbYHI"
      },
      "source": [
        "## Pruning\n",
        "\n",
        "We can also use pruning to improve our networks, which is built into Tensorflow and is similar in concept to the tree pruning we did earlier. \n",
        "\n",
        "### Smaller Models\n",
        "\n",
        "One other consideration is that we can use pruning to create smaller models that are better able to be executed on weaker hardware. In the context of a full computer, creating a prediction with a neural network is pretty fast. If we want to move the model to small embedded devices though, the memory and processing needs can still be excessive. \n",
        "\n",
        "Some scenarios where this comes up are things like security cameras that can recognize images, robots that can navigate themselves, or evern small computers like a Raspberry Pi. This challenge is magnified if you are dealing with something like video, which can generate 30+ images per second. Small models that are almost as good, but can be run with less compouting power allow the power of neural networks to be expanded to more devices - train on a powerful computer, us on a small and weak computer. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_model_optimization"
      ],
      "metadata": {
        "id": "0FzYRlh3vtFH",
        "outputId": "5a0b6cea-0796-4bed-ba21-41ae2cda0228",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_model_optimization\n",
            "  Downloading tensorflow_model_optimization-0.7.2-py2.py3-none-any.whl (237 kB)\n",
            "\u001b[?25l\r\u001b[K     |                              | 10 kB 23.8 MB/s eta 0:00:01\r\u001b[K     |                             | 20 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |                           | 30 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |                          | 40 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |                         | 51 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |                       | 61 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |                      | 71 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |                     | 81 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |                   | 92 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |                  | 102 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |                | 112 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |               | 122 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |              | 133 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |            | 143 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |           | 153 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |          | 163 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |        | 174 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |       | 184 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |     | 194 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |    | 204 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |   | 215 kB 5.0 MB/s eta 0:00:01\r\u001b[K     | | 225 kB 5.0 MB/s eta 0:00:01\r\u001b[K     || 235 kB 5.0 MB/s eta 0:00:01\r\u001b[K     || 237 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_model_optimization) (0.1.6)\n",
            "Requirement already satisfied: six~=1.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow_model_optimization) (1.15.0)\n",
            "Requirement already satisfied: numpy~=1.14 in /usr/local/lib/python3.7/dist-packages (from tensorflow_model_optimization) (1.21.5)\n",
            "Installing collected packages: tensorflow-model-optimization\n",
            "Successfully installed tensorflow-model-optimization-0.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrXm0AC9bYHI",
        "outputId": "dc180694-33be-400f-f64f-0d584191e9b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:238: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  trainable=False)\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:218: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  aggregation=tf.VariableAggregation.MEAN)\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:225: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  aggregation=tf.VariableAggregation.MEAN)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " prune_low_magnitude_flatten  (None, 784)              1         \n",
            " _16 (PruneLowMagnitude)                                         \n",
            "                                                                 \n",
            " prune_low_magnitude_dense_5  (None, 500)              784502    \n",
            " 6 (PruneLowMagnitude)                                           \n",
            "                                                                 \n",
            " prune_low_magnitude_dropout  (None, 500)              1         \n",
            " _13 (PruneLowMagnitude)                                         \n",
            "                                                                 \n",
            " prune_low_magnitude_dense_5  (None, 350)              350352    \n",
            " 7 (PruneLowMagnitude)                                           \n",
            "                                                                 \n",
            " prune_low_magnitude_dropout  (None, 350)              1         \n",
            " _14 (PruneLowMagnitude)                                         \n",
            "                                                                 \n",
            " prune_low_magnitude_dense_5  (None, 10)               7012      \n",
            " 8 (PruneLowMagnitude)                                           \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,141,869\n",
            "Trainable params: 571,360\n",
            "Non-trainable params: 570,509\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
        "\n",
        "model_for_pruning = prune_low_magnitude(model)\n",
        "\n",
        "# `prune_low_magnitude` requires a recompile.\n",
        "model_for_pruning.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model_for_pruning.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3sPhB8EbYHI"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "  tfmot.sparsity.keras.UpdatePruningStep()\n",
        "]\n",
        "\n",
        "model_for_pruning.fit(train_images, train_labels,\n",
        "                  batch_size=64, epochs=10, validation_split=.2,\n",
        "                  callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQSax3SDbYHJ"
      },
      "outputs": [],
      "source": [
        "_, model_for_pruning_accuracy = model_for_pruning.evaluate(\n",
        "   test_images, test_labels, verbose=0)"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "97ae724bfa85b9b34df7982b8bb8c7216f435b92902d749e4263f71162bea840"
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 64-bit (conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "keras_optimizations_sol.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}