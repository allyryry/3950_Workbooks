{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()  # for plot styling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "Early on we looked at one type of ensemble model - bagging, specifically with Forests made from several trees. When bagging we take a bunch of copies of the data, slice it up into random subsets, let each model make predictions, then combine those predictions (via vote/average) into a final answer. Bagging is effective at combatting overfitting, and this is especially visible with those trees since we could see how overfitted a tree model could get if we allowed it to grow large. \n",
    "\n",
    "Boosting is another type of ensemble that takes a different approach to combining several models. Boosting is sequential - it takes the results from one model, and uses those results to guide the training of a subsequent model. In order to explain this, we need to first examine one other concept - weak and strong learners. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weak Learners and Strong Learners\n",
    "\n",
    "We can split predictive models into two classes, depending on their performance:\n",
    "<ul>\n",
    "<li> Weak learners - models that perform only slightly better than guessing. \n",
    "<li> Strong learners - models that perform \"much\" better than guessing.\n",
    "</ul>\n",
    "\n",
    "We've been looking at strong learners when building models so far - regression, SVM, SGD - all able to achieve a high level of accuracy if we tune them with appropriate hyperparameters, thus all strong learners.\n",
    "\n",
    "What's a weak learner? The easiest example is a tiny decision tree - if we were to limit the depth to 2 levels - representing one decision. Technically we can create weak learners out of almost any model by tuning them to eliminate variance. Trees are common though. \n",
    "\n",
    "### Why Would Anyone Want a Weak Learner?\n",
    "\n",
    "We generally want accuracy, so conciously moving to a less accurate model seems... odd. Following from the performance of weak and strong learners, we can draw another conclusion that should be intuitive:\n",
    "<ul>\n",
    "<li> Weak learners are simple to compute. E.g. a one-decision decision tree is simple. This is also called a stump. \n",
    "<li> Strong learners are hard to compute. \n",
    "</ul>\n",
    "\n",
    "The \"magic\" comes from combining weak learners with the concept of boosting. We can combine many simple and quick weak learning models together through boosting, and acheive an ensemble that can acheive high accuracy, like a strong learner. These boosted ensembles can deliver very good performance in practice, with low amounts of overfitting and high levels of accuracy. We'll look at two examples that are popular - Adaboost and XGboost. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost\n",
    "\n",
    "Adaboost is short for adaptive boosting, it is one of, if not the, most commonly used boosting algorithms. Adaboost is conceptually fairly simple in how it learns:\n",
    "<ul>\n",
    "<li> Generate predictions for the data in the training set with the first model.\n",
    "    <ul>\n",
    "    <li> Predictions that were incorrect get their weights increased, so they are selected more often. \n",
    "    <li> Predictions that are correct have their weights decreased. \n",
    "    </ul>\n",
    "<li> Pass the data, along with the weights, on to the next model. \n",
    "<li> As the predictions are being made, the individual models are also evaluated and weighted. \n",
    "    <ul>\n",
    "    <li> Models that predict correctly get weighted higher. \n",
    "    <li> Models that do not predict correctly are weighted lower.  \n",
    "    </ul>\n",
    "<li> Repeat until everything is correct, or you've hit the limit. \n",
    "<li> Predictions are a weighted sum (using the weights of the models) of the predictions of all the models. \n",
    "</ul>\n",
    "\n",
    "We can think of the process roughly like this:\n",
    "<ul>\n",
    "<li> We make predictions with a bunch of quick but simple models, those that are the most accurate have their importance to the final predictions increased. \n",
    "<li> We take the data that hasn't been correctly classified, and predict it with more models until we get it correct. All the ones that we've already correctly predicted need less attention. \n",
    "</ul>\n",
    "\n",
    "It is kind of like if you were to look at a room of people, and classify them as either NFL players or gymnasts using some simple tests. First you'd look at weight, that would do a very good job and get a high score as a model. Then you may take the leftover people and split them by height. Then you'd take the rest and split them by net worth.... Eventually you may split them by number of broken bones or something similarly obscure. The models that do the best job of splitting the groups would get the biggest impact on the final classification. The records that are easy to classify would be done immediately with one of the important models, those that are harder would filter through more and more models until they are able to be accurately classified. \n",
    "\n",
    "Most examples of boosting look at classification because it is more illustrative. Regression works the same way, we just replace the accuracy rate with a metric like MSE. Accurate predictions are like correct ones, and the most accurate models are promoted in importance. \n",
    "\n",
    "### Using Adaboost\n",
    "\n",
    "Using adabost is pretty similar to using every other model. Adaboost can perform both classification and regression, the mechanics of each rely on the underlying algorithm. That algorithm is normally a decision tree classifier/regressor, but other estimators can be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ada = pd.read_csv(\"data/titanic_train.csv\")\n",
    "y_ada = df_ada[\"Survived\"]\n",
    "X_ada = df_ada.drop(columns={\"Survived\"})\n",
    "df_ada.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ada = AdaBoostClassifier()\n",
    "\n",
    "column_trans = ColumnTransformer([\n",
    "    ('categories', OneHotEncoder(), [\"Sex\", \"Embarked\"]),\n",
    "    ('title_bow', SimpleImputer(strategy=\"median\"), [\"Age\", \"Pclass\", \"SibSp\", \"Parch\", \"Fare\"])\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "ada_pipe = Pipeline([\n",
    "    (\"ct\", column_trans),\n",
    "    (\"ada\", ada)\n",
    "])\n",
    "Xtr, Xte, ytr, yte = train_test_split(X_ada, y_ada)\n",
    "ada_pipe.fit(X=Xtr, y=ytr.ravel())\n",
    "\n",
    "ada_preds = ada_pipe.predict(Xte)\n",
    "accuracy_score(yte, ada_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Predict quality. Try swapping some other estimator in the adaboost. If you have extra time, do a grid search for different estimators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = pd.read_csv(\"data/WineQT.csv\")\n",
    "wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_wine = wine[\"quality\"]\n",
    "X_wine = wine.drop(columns={\"Id\", \"quality\"})\n",
    "X_wine_tr, X_wine_te, y_wine_tr, y_wine_te = train_test_split(X_wine, y_wine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGboost and Gradient Boosting\n",
    "\n",
    "XGboost is short for extreme gradient boosting, a new and ofter very accurate ensemble method. \n",
    "\n",
    "#### Installing XGboost\n",
    "\n",
    "XGboost isn't part of SK Learn, we have to install it. The package name is xgboost and it can be installed through whatever means works on your computer:\n",
    "<ul>\n",
    "<li> pip install xgboost\n",
    "<li> conda install -c conda-forge xgboost\n",
    "<li> conda gui installation, if it is available there. \n",
    "<li> <b>If install is giving you issues, use the sklearn HGB classifier and regressor: https://scikit-learn.org/stable/modules/ensemble.html#histogram-based-gradient-boosting </b>\n",
    "</ul>\n",
    "\n",
    "I installed it via pip, and got weird errors (kernel died) when using it. Running:\n",
    "\n",
    "conda install -c conda-forge xgboost\n",
    "\n",
    "did fix it. \n",
    "\n",
    "There are dependencies, so the process may require installing other items to make it work. On my work Mac, after installing XGboost, I got an error and had to install another library with the command: \"brew install libomp\". This command was given to me by the error message when I tried to import xgboost in code. Based on past experience, there may be some variety \n",
    "\n",
    "Install documentation is here: https://xgboost.readthedocs.io/en/stable/install.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "\n",
    "Before we can get eXtreme!!!!! with xgboost, we need to look at one of it's main component parts - gradient boosting. Gradient boosting is another type of boosting, similar to Adabost, however with gradient boosting the subsequent models are trained on the residual error of the previous model. \n",
    "\n",
    "The big difference is that gradient boosting is trained on the residual errors and this is the factor that increases importance for the next training. In Adaboost the high weight records are given more importance, in gradient boosting the higher gradients are given more importance. \n",
    "\n",
    "The process that gradient boosting uses is:\n",
    "<ul>\n",
    "<li> Make an initial set of predictions - this is often done by just making a generic prediction for all records, such as the average. This will generate residuals, or more generally, some measure of loss defined by the loss function. These residuals are the starting point. \n",
    "<li> Fit a weak learner with the feature set and the residuals of the previous round. In gradient boosting the weak learners (normally trees) tend to be larger - 8 to 32 nodes. \n",
    "<li> Repeat the previous step repeatedly until the limit of number of estimators is reached. \n",
    "<li> Predictions are the initial prediction, plus all of the residual predictions averaged together (scaled by the learning rate). This is called shrinkage. \n",
    "    <ul>\n",
    "    <li> y(pred) = y1 + (eta *  r1) + (eta * r2) + ....... + (eta * rN)\n",
    "    </ul>\n",
    "</ul>\n",
    "\n",
    "![Gradient](images/gradientboosting.png \"Gradient\" )\n",
    "\n",
    "Gradient boosting also works for regression and classification. With the algorithm here it is easier to think of as a regression problem since it is based on residuals. When used for classification the algorithm uses the sigmoid to translate to classification, just as logistic regression does. \n",
    "\n",
    "For the most part the parameters for gradient boosting are ones that we are familiar with - error metrics, learning rate, tree options (#leafs etc...), and the number of estimators used. Gradient boosting is typically fairly resistant to overfitting, so a large number of estimators will often be better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gb = pd.read_csv(\"data/bodyfat.csv\")\n",
    "y_gbr = df_gb[\"BodyFat\"]\n",
    "X_gbr = df_gb.drop(columns={\"BodyFat\"})\n",
    "df_gb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor()\n",
    "gbr_pipe = Pipeline([\n",
    "    (\"scale\", MinMaxScaler()),\n",
    "    (\"gbc\", gbr)\n",
    "])\n",
    "\n",
    "Xtr_r, Xte_r, ytr_r, yte_r = train_test_split(X_gbr, y_gbr)\n",
    "gbr_pipe.fit(X=Xtr_r, y=ytr_r.ravel())\n",
    "\n",
    "gbr_preds = gbr_pipe.predict(Xte_r)\n",
    "mean_squared_error(yte_r, gbr_preds, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Use the classifier version of gradient boosting to predict titanic survival. \n",
    "\n",
    "#### HistGradientBoosting\n",
    "\n",
    "SKlearn also provides a package called HistGradientBoosting which is inspired by LightGBM. It can be much faster when datasets get large and has the side benefit of automatically handling missing values. Try with this one if you have a moment, details for the classifier are: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html The underlying change is that these algorithms bin (discreetize) the data up front, which reduces the number of splits and allows the math to be done on integers, which is faster. HGB also defaults to utilizing early stopping to speed processing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise - Predict Titanic with GBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titan_gb_mod = GradientBoostingClassifier()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting Classification and Stochastic Gradient Boosting\n",
    "\n",
    "Using gradient boosting for classification is similar. One additional hyperparameter we can utilize here is \"subsample\", which controls the fraction of records used for each learner. Setting this to be less than 1 causes the algorithm to use stochastic gradient boosting - utilizing a randomized subset of the data for each tree. This tends to protect against overfitting, similarly to how it works in a forest model. If the dataset is large, this can also speed things up. A split of 30% to 70% is pretty typical to try, typically towards the lower-middle of that range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier(subsample=.3)\n",
    "\n",
    "column_trans2 = ColumnTransformer([\n",
    "    ('categories', OneHotEncoder(), [\"Sex\", \"Embarked\"]),\n",
    "    ('title_bow', SimpleImputer(strategy=\"median\"), [\"Age\", \"Pclass\", \"SibSp\", \"Parch\", \"Fare\"])\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "gbc_pipe = Pipeline([\n",
    "    (\"ct\", column_trans2),\n",
    "    (\"gbc\", gbc)\n",
    "])\n",
    "\n",
    "Xtr, Xte, ytr, yte = train_test_split(X_ada, y_ada)\n",
    "gbc_pipe.fit(X=Xtr, y=ytr.ravel())\n",
    "\n",
    "gbc_preds = gbc_pipe.predict(Xte)\n",
    "accuracy_score(yte, gbc_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGboost\n",
    "\n",
    "Ok, now we're ready to be eXtreme!!!! and use xgboost. XGboost is basically an implementation of gradient boosting that adds in many of the concpets that we've previously touched on to improve its speed, accuracy, and resiliance to overfitting - thus making it eXtreme!!! Some of the things xgboost incorporates are:\n",
    "\n",
    "<ul>\n",
    "<li> Parallelization - boosting is generally a sequential process, so it is hard to run in parallel. Xgboost is carefully written to allow the parts that can be run in parallel, to be run in parallel. This makes xgboost faster than it would be with a traditional boosted implementation. \n",
    "<li> Cross validation - xgboost builds in cross-validation as part of it's standard execution. \n",
    "<li> Regularization - xgboost applies regularization (L1 or L2) to limit overfitting. \n",
    "<li> Sparse and missing data handling - xgboost smartly deals with data that has missing values, or data that is sparse. \n",
    "<li> Optimization - xgboost is written to utilize hardware resources very efficiently, including both processing power and memory limits. \n",
    "</ul>\n",
    "\n",
    "In short, xgboost is effectively a compilation of many of the tools that we've looked at to build better models, all wrapped into one simple package. Outside of neural networks with very large datasets, xgboost is generally the most likely algorithm to be the \"best\", both in terms of accuracy and speed. This isn't universal, of course, different data will perform differently with different algorithms, but xgboost is likely to be a winner, probably more than any other algorithm. \n",
    "\n",
    "<b>If you don't want to, or are having troubles trying to, install xgboost, use HBC/HBR from sklearn.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "#Load Data\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGboost and eXtreme!!! Speed\n",
    "\n",
    "One of the benefits of xgboost is that it is fast. The 70,000 digit images was very slow to process in past attempts with other algorithms, to the point that we cut it down to 10,000 or so to make the time manageable. We can see what xgboost can do for us with the full 70,000 images along with a very gentle PCA that should keep the vast majority of the varaiance. Recall we had a 150 component PCA that only slightly blurred the images. \n",
    "\n",
    "Some parameters that we may want to look at for xbgoost are:\n",
    "<ul>\n",
    "<li> booster: can be changed to gblinear to swap the default tree models to linear ones. \n",
    "<li> max_depth: same as in trees. \n",
    "<li> lambda/alpha: L2 and L1 regularization strength, respectively. \n",
    "<li> eta: amount of pruning. \n",
    "</ul>\n",
    "The full set is well documented, and fairly easy to understand, located: https://xgboost.readthedocs.io/en/stable/parameter.html \n",
    "\n",
    "Note: xgboost defaults to creating the maximal number of threads, similar to doing n_jobs=-1 for things like a forest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(max_depth=2)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    #('standard_scaler', StandardScaler()), \n",
    "    ('pca', PCA(200)), \n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "xgb_preds = pipeline.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test, xgb_preds))\n",
    "sns.heatmap(confusion_matrix(y_test, xgb_preds), annot=True, cmap=\"BrBG_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that's... <h1>eXtreme!!!!!!!!!!!!!!!</h1> We can process the whole set of 70,000 in a reasonable amount of time, at a high accuracy. XGboost is currently the overall \"king\" of the non-neural network algorithms for most problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Predict the categories of the newsgroups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = (\"headers\", \"footers\", \"quotes\")\n",
    "\n",
    "data_train = fetch_20newsgroups(subset=\"train\", shuffle=True, remove=remove)\n",
    "\n",
    "data_test = fetch_20newsgroups(subset=\"test\", shuffle=True, remove=remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize - try to use ngrams > 1, so we can see the speed impact. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Models / Pipeline"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d722d3adfa415172c1f5238b519fb86b488acdae450fd691ab06c09f4ca9173"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ml3950': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
