{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to plot loss\n",
    "def plot_loss(history):\n",
    "  plt.plot(history.history['loss'], label='loss')\n",
    "  plt.plot(history.history['val_loss'], label='val_loss')\n",
    "  plt.legend()\n",
    "  plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras, TensorFlow, and Neural Network Regression\n",
    "\n",
    "As we have seen, neural networks aren't quite as complex as they appear at first, however we still generally don't want to have to build them from scratch very often. The libraries that we will primarily use for creating neural network models are Tensorflow and Keras. \n",
    "\n",
    "### Tensorflow\n",
    "\n",
    "Tensorflow, developed by Google, is one of the most popular libraries for neural networks. \n",
    "\n",
    "### Keras\n",
    "\n",
    "Keras is another package that provides an an API offering an easier to use interface to Tensorflow, allowing us to use it with code that is higher level, avoiding much of the linear math that can make Tensorflow frustrating. Since its introduction Keras has been wrapped in with Tensorflow and the two are normally now blended together as far as we are concerned. \n",
    "\n",
    "### Other Alternatives\n",
    "\n",
    "Keras and Tensorflow are not the only libraries of neural networks, the primary competitor to Tensorflow is PyTorch, which was developed by Facebook. PyTorch does pretty much the same thing as Tensorflow, we won't look at it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21608</th>\n",
       "      <td>360000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1530</td>\n",
       "      <td>1131</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1530</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98103</td>\n",
       "      <td>47.6993</td>\n",
       "      <td>-122.346</td>\n",
       "      <td>1530</td>\n",
       "      <td>1509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21609</th>\n",
       "      <td>400000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2310</td>\n",
       "      <td>5813</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>2310</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98146</td>\n",
       "      <td>47.5107</td>\n",
       "      <td>-122.362</td>\n",
       "      <td>1830</td>\n",
       "      <td>7200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21610</th>\n",
       "      <td>402101.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1020</td>\n",
       "      <td>1350</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1020</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98144</td>\n",
       "      <td>47.5944</td>\n",
       "      <td>-122.299</td>\n",
       "      <td>1020</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21611</th>\n",
       "      <td>400000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1600</td>\n",
       "      <td>2388</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1600</td>\n",
       "      <td>0</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>98027</td>\n",
       "      <td>47.5345</td>\n",
       "      <td>-122.069</td>\n",
       "      <td>1410</td>\n",
       "      <td>1287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21612</th>\n",
       "      <td>325000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1020</td>\n",
       "      <td>1076</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1020</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98144</td>\n",
       "      <td>47.5941</td>\n",
       "      <td>-122.299</td>\n",
       "      <td>1020</td>\n",
       "      <td>1357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          price  bedrooms  bathrooms  sqft_living  sqft_lot  floors  \\\n",
       "21608  360000.0         3       2.50         1530      1131     3.0   \n",
       "21609  400000.0         4       2.50         2310      5813     2.0   \n",
       "21610  402101.0         2       0.75         1020      1350     2.0   \n",
       "21611  400000.0         3       2.50         1600      2388     2.0   \n",
       "21612  325000.0         2       0.75         1020      1076     2.0   \n",
       "\n",
       "       waterfront  view  condition  grade  sqft_above  sqft_basement  \\\n",
       "21608           0     0          3      8        1530              0   \n",
       "21609           0     0          3      8        2310              0   \n",
       "21610           0     0          3      7        1020              0   \n",
       "21611           0     0          3      8        1600              0   \n",
       "21612           0     0          3      7        1020              0   \n",
       "\n",
       "       yr_built  yr_renovated  zipcode      lat     long  sqft_living15  \\\n",
       "21608      2009             0    98103  47.6993 -122.346           1530   \n",
       "21609      2014             0    98146  47.5107 -122.362           1830   \n",
       "21610      2009             0    98144  47.5944 -122.299           1020   \n",
       "21611      2004             0    98027  47.5345 -122.069           1410   \n",
       "21612      2008             0    98144  47.5941 -122.299           1020   \n",
       "\n",
       "       sqft_lot15  \n",
       "21608        1509  \n",
       "21609        7200  \n",
       "21610        2007  \n",
       "21611        1287  \n",
       "21612        1357  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/house_data.csv\")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21613 entries, 0 to 21612\n",
      "Data columns (total 19 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   price          21613 non-null  float64\n",
      " 1   bedrooms       21613 non-null  int64  \n",
      " 2   bathrooms      21613 non-null  float64\n",
      " 3   sqft_living    21613 non-null  int64  \n",
      " 4   sqft_lot       21613 non-null  int64  \n",
      " 5   floors         21613 non-null  float64\n",
      " 6   waterfront     21613 non-null  int64  \n",
      " 7   view           21613 non-null  int64  \n",
      " 8   condition      21613 non-null  int64  \n",
      " 9   grade          21613 non-null  int64  \n",
      " 10  sqft_above     21613 non-null  int64  \n",
      " 11  sqft_basement  21613 non-null  int64  \n",
      " 12  yr_built       21613 non-null  int64  \n",
      " 13  yr_renovated   21613 non-null  int64  \n",
      " 14  zipcode        21613 non-null  int64  \n",
      " 15  lat            21613 non-null  float64\n",
      " 16  long           21613 non-null  float64\n",
      " 17  sqft_living15  21613 non-null  int64  \n",
      " 18  sqft_lot15     21613 non-null  int64  \n",
      "dtypes: float64(5), int64(14)\n",
      "memory usage: 3.1 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21613, 18) (21613, 1)\n"
     ]
    }
   ],
   "source": [
    "y = np.array(df[\"price\"]).reshape(-1,1)\n",
    "X = np.array(df.drop(columns={\"price\"}))\n",
    "print(X.shape, y.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model\n",
    "\n",
    "Creating a NN model is slightly different from the normal process that we are used to in sklearn. We need to do a little more work to set it up. \n",
    "\n",
    "### Create Model and Add Layers\n",
    "\n",
    "First we need to make a NN model, it comes \"empty\". We will use a sequential model, which is the most simple type but is less configurable (which we don't care about much right now). The limitation of sequential models is that they can only take in one tensor and only output one tensor. The other options here are \"functional\", which allows for the structure of the model to be configured, and \"model subclassing\", which allows you to build almost everything from scratch. \n",
    "\n",
    "#### Layers\n",
    "\n",
    "Next we need to add some layers. We will start simple with only two \"thinking\" layers, and one to do some processing. We can think of the layers roughly like steps of the sklearn pipeline, with data entering at the first layer and predictions flowing out of the final layer. \n",
    "\n",
    "In addition to \"normal\" neural network layers, there are many other types that can do all kinds of other stuff. One example we will use here is the normalization one at the front. This layer does exactly what you'd expect - it normalizes our data so the rest of the network can use it. The normalize layer will also automatically handle the 2D nature of the data that we are used to, so we don't need to worry about that aspect here. Other layers can do everything from regularization to image processing, they are also commonly inhierited for developers to create custom layers targeting specific tasks. We'll use a few of the other ones as we move through things. \n",
    "\n",
    "#### Dense Layers\n",
    "\n",
    "We'll use dense layers here. When adding the layer we need to specify a couple of things. One is the input dimensions - we need to tell the network what the shape of the incomming data is. \n",
    "\n",
    "The other argument is the units, which represents the output dimension. When using these Keras dense layers we don't need to specify each layer's input/output like we did when we made it by hand. We specify both, using units and input_dim, for the first layer that takes in the input; for subsequent layers we can just specify the output and Keras will automatically figure the rest out. \n",
    "\n",
    "Note that there is also an input layer that can be added, we can avoid the need for it by using the input_dim or input_shape as shown below. The two examples there do the same thing, since the input is flat - 18 features. If we are dealing with inputs that do not start out as flat, such as in an image, use the input_shape since you can specify all dimensions; we will see an example of this next time with some images. \n",
    "\n",
    "#### Activation Function\n",
    "\n",
    "For each of our layers we need to define which activation function to use. For now we will use the ReLU function, which is probably the most popular. We'll look at other ones later on. \n",
    "\n",
    "Note that we've left the activation function off of the final layer - we are doing regression so we want that raw value.\n",
    "\n",
    "#### Summary\n",
    "\n",
    "After we've constructed the model, the summary command give us, well, a summary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are dealing with a bunch of numerical inputs here, so we can add a normalization layer at the front end. Like with sklearn, we want to fit the normalization to the training data only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizatio  (None, 18)               37        \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 18)                342       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 19        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 398\n",
      "Trainable params: 361\n",
      "Non-trainable params: 37\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "normalizer.adapt(np.array(X_train))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(normalizer)\n",
    "model.add(Dense(18, input_shape=(18,), activation='relu'))\n",
    "#model.add(Dense(18, input_dim=18, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile Model\n",
    "\n",
    "Once a model is created we need to compile it. The complie step basically builds the layers we specified above and the loss and optimization parameters below together into a usable model object. When compiling the model we are providing it with the things it needs to calculate error:\n",
    "\n",
    "<ul>\n",
    "<li> Loss - we can provide a loss function that we'd like to use. \n",
    "<li> Optimizer - the optimizer is the algorithm that the model will use to perform the gradient descent to find the lowest error. Adam is a very common choice.\n",
    "<li> Learning rate - the learning rate is provided as a parameter of the optimizer. \n",
    "</ul>\n",
    "\n",
    "##### Optimizing Adam\n",
    "\n",
    "The optimizer is the algorithm used to perform the gradient descent and minimize error. For the most part this isn't something we need to be concerned about. The choice of optimizer is much more important if dealing with very large datasets because different optimizers have different levels of efficiency. For our purposes, we can use Adam and be pretty happy. Adam stands for Adaptive Moment Estimation which means basically that it will adjust itself depending on current gradients. It tends to be efficient both in time and memory, so it is very commonly used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_absolute_error', optimizer=tf.optimizers.Adam(learning_rate=.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Model\n",
    "\n",
    "The fit command does the same thing that we are used to, it trains the model, however there are some differences. The main difference is that batch_size is almost always set in neural networks, while the sklearn models just take all the data at once. \n",
    "\n",
    "What's a batch? Batches are just subsets of the data, so if the batch size is 100 the algorithm will grab 100 rows at a time before making an update to the weights and bias. There are a few reasons this exists:\n",
    "\n",
    "<ul>\n",
    "<li> Memory constraints - it is common with neural networks to deal with datasets that are extremely large. Processing data that can't fit entirely in RAM is very slow (the computer must swap data from the hard drive to RAM as it is needed) compared to data that is in RAM. Cutting the batch size can avoid this issue. \n",
    "<li> Speed - the math involved in the back propagation can sometimes be very computationally intensive. \n",
    "<li> Accuracy - batch size can have an impact on accuracy, though that impact is not very predictable. For the most part finding an optimal batch size will need to be grid-searched. \n",
    "</ul>\n",
    "\n",
    "The fit command also has the epoch paramater, which instructs on how many times to work through ALL of the data. We want to ensure we have enough epochs to find the optimal solution. \n",
    "\n",
    "#### Plot the Loss\n",
    "\n",
    "One very common visualization we see with neural networks is a plot of both training and validation loss vs number of epochs. Generally we'll see the training loss drop - first sharply as the model initially fits itself, then more slowly as it becomes more fitted. The validation loss will usually somewhat mirror the training loss, except it will often reach a minimum at some point before again increasing. This minimum point is our best model, when the validation loss starts increasing again, that is a sign that the model has become overfitted - customized to the training data, but less and less generalizable to new data. \n",
    "\n",
    "Set the verbosity to 1 in the fit to get a full list of the loss for each epoch to pinpoint the exact \"ideal\" number of epochs. We'll look more at this in a minute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "130/130 [==============================] - 1s 2ms/step - loss: 541852.8125 - val_loss: 534252.4375\n",
      "Epoch 2/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 541502.5625 - val_loss: 533676.5625\n",
      "Epoch 3/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 540709.1250 - val_loss: 532669.7500\n",
      "Epoch 4/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 539499.7500 - val_loss: 531262.5625\n",
      "Epoch 5/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 537898.2500 - val_loss: 529478.0625\n",
      "Epoch 6/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 535927.4375 - val_loss: 527327.5000\n",
      "Epoch 7/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 533602.3750 - val_loss: 524833.1875\n",
      "Epoch 8/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 530938.1250 - val_loss: 522012.5312\n",
      "Epoch 9/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 527951.3750 - val_loss: 518870.3438\n",
      "Epoch 10/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 524652.3125 - val_loss: 515427.7812\n",
      "Epoch 11/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 521052.9688 - val_loss: 511690.3750\n",
      "Epoch 12/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 517164.8125 - val_loss: 507673.0312\n",
      "Epoch 13/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 513000.9375 - val_loss: 503382.9688\n",
      "Epoch 14/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 508569.5312 - val_loss: 498820.4375\n",
      "Epoch 15/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 503879.1562 - val_loss: 494016.0000\n",
      "Epoch 16/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 498945.6250 - val_loss: 488965.8125\n",
      "Epoch 17/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 493779.7188 - val_loss: 483704.8125\n",
      "Epoch 18/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 488395.8750 - val_loss: 478225.7812\n",
      "Epoch 19/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 482798.6562 - val_loss: 472533.9062\n",
      "Epoch 20/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 476991.7812 - val_loss: 466658.2188\n",
      "Epoch 21/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 470994.6562 - val_loss: 460580.7188\n",
      "Epoch 22/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 464823.9062 - val_loss: 454352.9062\n",
      "Epoch 23/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 458484.2188 - val_loss: 447988.1875\n",
      "Epoch 24/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 451996.9688 - val_loss: 441534.6875\n",
      "Epoch 25/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 445398.0000 - val_loss: 435002.9062\n",
      "Epoch 26/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 438689.5000 - val_loss: 428384.2188\n",
      "Epoch 27/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 431925.5625 - val_loss: 421746.5312\n",
      "Epoch 28/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 425121.0312 - val_loss: 415080.0000\n",
      "Epoch 29/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 418280.9688 - val_loss: 408421.1562\n",
      "Epoch 30/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 411472.1250 - val_loss: 401786.5000\n",
      "Epoch 31/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 404719.2500 - val_loss: 395189.4688\n",
      "Epoch 32/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 398011.5000 - val_loss: 388652.3438\n",
      "Epoch 33/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 391371.3438 - val_loss: 382194.3438\n",
      "Epoch 34/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 384793.6562 - val_loss: 375801.2812\n",
      "Epoch 35/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 378247.4062 - val_loss: 369467.5000\n",
      "Epoch 36/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 371764.9062 - val_loss: 363224.4375\n",
      "Epoch 37/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 365372.7500 - val_loss: 357042.5000\n",
      "Epoch 38/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 359071.4062 - val_loss: 350957.5625\n",
      "Epoch 39/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 352840.9688 - val_loss: 344972.2500\n",
      "Epoch 40/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 346733.4062 - val_loss: 339157.7812\n",
      "Epoch 41/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 340700.5625 - val_loss: 333275.9375\n",
      "Epoch 42/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 334612.2500 - val_loss: 327430.5625\n",
      "Epoch 43/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 328603.0312 - val_loss: 321738.8438\n",
      "Epoch 44/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 322693.0938 - val_loss: 316117.4062\n",
      "Epoch 45/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 316911.7812 - val_loss: 310594.8750\n",
      "Epoch 46/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 311282.0938 - val_loss: 305236.0625\n",
      "Epoch 47/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 305836.6562 - val_loss: 300079.0312\n",
      "Epoch 48/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 300604.3750 - val_loss: 295100.7812\n",
      "Epoch 49/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 295573.0312 - val_loss: 290302.0625\n",
      "Epoch 50/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 290757.2500 - val_loss: 285707.3125\n",
      "Epoch 51/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 286176.2500 - val_loss: 281279.4375\n",
      "Epoch 52/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 281794.7500 - val_loss: 277074.5625\n",
      "Epoch 53/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 277584.5938 - val_loss: 273038.6562\n",
      "Epoch 54/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 273549.8750 - val_loss: 269153.3125\n",
      "Epoch 55/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 269676.2812 - val_loss: 265342.0625\n",
      "Epoch 56/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 265976.9375 - val_loss: 261693.7656\n",
      "Epoch 57/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 262411.0625 - val_loss: 258154.9219\n",
      "Epoch 58/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 258959.9375 - val_loss: 254725.9844\n",
      "Epoch 59/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 255614.9688 - val_loss: 251310.5938\n",
      "Epoch 60/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 252359.6875 - val_loss: 248032.5781\n",
      "Epoch 61/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 249203.8438 - val_loss: 244923.8594\n",
      "Epoch 62/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 246181.6250 - val_loss: 241941.0312\n",
      "Epoch 63/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 243306.1094 - val_loss: 239160.1875\n",
      "Epoch 64/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 240571.0156 - val_loss: 236549.2188\n",
      "Epoch 65/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 237973.2188 - val_loss: 234075.9688\n",
      "Epoch 66/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 235512.7500 - val_loss: 231727.4375\n",
      "Epoch 67/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 233188.5000 - val_loss: 229480.5469\n",
      "Epoch 68/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 230977.8594 - val_loss: 227336.7656\n",
      "Epoch 69/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 228863.2969 - val_loss: 225284.4688\n",
      "Epoch 70/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 226819.4219 - val_loss: 223316.0000\n",
      "Epoch 71/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 224835.8906 - val_loss: 221426.5781\n",
      "Epoch 72/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 222905.2031 - val_loss: 219585.1406\n",
      "Epoch 73/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 221022.1562 - val_loss: 217785.1875\n",
      "Epoch 74/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 219172.9844 - val_loss: 216043.2188\n",
      "Epoch 75/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 217355.0000 - val_loss: 214328.3750\n",
      "Epoch 76/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 215554.5000 - val_loss: 212634.9219\n",
      "Epoch 77/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 213769.0625 - val_loss: 210965.8125\n",
      "Epoch 78/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 212023.4688 - val_loss: 209340.2344\n",
      "Epoch 79/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 210298.5938 - val_loss: 207715.4531\n",
      "Epoch 80/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 208612.1406 - val_loss: 206129.9219\n",
      "Epoch 81/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 206972.5000 - val_loss: 204554.3281\n",
      "Epoch 82/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 205369.3281 - val_loss: 202984.6094\n",
      "Epoch 83/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 203801.0312 - val_loss: 201432.2344\n",
      "Epoch 84/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 202266.5156 - val_loss: 199912.0781\n",
      "Epoch 85/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 200746.6094 - val_loss: 198427.2812\n",
      "Epoch 86/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 199261.4531 - val_loss: 196937.2812\n",
      "Epoch 87/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 197803.2031 - val_loss: 195478.7812\n",
      "Epoch 88/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 196378.2188 - val_loss: 194032.4219\n",
      "Epoch 89/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 194969.0781 - val_loss: 192616.5469\n",
      "Epoch 90/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 193573.5938 - val_loss: 191230.4062\n",
      "Epoch 91/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 192200.3438 - val_loss: 189855.4844\n",
      "Epoch 92/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 190837.0000 - val_loss: 188505.1875\n",
      "Epoch 93/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 189493.1719 - val_loss: 187184.1250\n",
      "Epoch 94/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 188196.6406 - val_loss: 185877.8906\n",
      "Epoch 95/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 186920.8281 - val_loss: 184584.8906\n",
      "Epoch 96/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 185663.6094 - val_loss: 183325.4531\n",
      "Epoch 97/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 184441.9219 - val_loss: 182119.8438\n",
      "Epoch 98/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 183242.6094 - val_loss: 180945.9688\n",
      "Epoch 99/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 182073.3750 - val_loss: 179780.5469\n",
      "Epoch 100/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 180943.2969 - val_loss: 178634.8125\n",
      "Epoch 101/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 179836.7344 - val_loss: 177507.9688\n",
      "Epoch 102/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 178750.6875 - val_loss: 176381.2812\n",
      "Epoch 103/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 177676.3750 - val_loss: 175272.4688\n",
      "Epoch 104/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 176607.9375 - val_loss: 174180.1406\n",
      "Epoch 105/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 175552.1406 - val_loss: 173091.1719\n",
      "Epoch 106/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 174518.5000 - val_loss: 172021.5156\n",
      "Epoch 107/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 173491.5938 - val_loss: 170967.1406\n",
      "Epoch 108/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 172483.4375 - val_loss: 169899.0781\n",
      "Epoch 109/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 171484.2812 - val_loss: 168850.3594\n",
      "Epoch 110/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 170488.2812 - val_loss: 167811.6875\n",
      "Epoch 111/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 169493.4062 - val_loss: 166778.7344\n",
      "Epoch 112/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 168506.6094 - val_loss: 165779.2812\n",
      "Epoch 113/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 167530.1094 - val_loss: 164783.9531\n",
      "Epoch 114/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 166568.1094 - val_loss: 163783.0625\n",
      "Epoch 115/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 165604.6094 - val_loss: 162804.5625\n",
      "Epoch 116/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 164648.9531 - val_loss: 161811.1875\n",
      "Epoch 117/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 163690.6250 - val_loss: 160827.4688\n",
      "Epoch 118/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 162739.8906 - val_loss: 159857.0312\n",
      "Epoch 119/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 161796.5156 - val_loss: 158890.8438\n",
      "Epoch 120/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 160855.7500 - val_loss: 157941.6250\n",
      "Epoch 121/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 159911.9062 - val_loss: 156990.7031\n",
      "Epoch 122/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 158972.5000 - val_loss: 156061.9688\n",
      "Epoch 123/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 158045.0781 - val_loss: 155150.4844\n",
      "Epoch 124/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 157122.6719 - val_loss: 154243.2344\n",
      "Epoch 125/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 156208.2969 - val_loss: 153330.3125\n",
      "Epoch 126/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 155307.7344 - val_loss: 152433.8594\n",
      "Epoch 127/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 154418.1562 - val_loss: 151550.0781\n",
      "Epoch 128/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 153540.2656 - val_loss: 150666.6250\n",
      "Epoch 129/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 152671.8438 - val_loss: 149801.9844\n",
      "Epoch 130/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 151810.7812 - val_loss: 148950.2812\n",
      "Epoch 131/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 150958.3125 - val_loss: 148110.0781\n",
      "Epoch 132/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 150121.2344 - val_loss: 147291.2188\n",
      "Epoch 133/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 149302.2188 - val_loss: 146473.3438\n",
      "Epoch 134/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 148497.2188 - val_loss: 145680.1250\n",
      "Epoch 135/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 147717.1875 - val_loss: 144898.9531\n",
      "Epoch 136/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 146952.2031 - val_loss: 144134.3594\n",
      "Epoch 137/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 146205.6875 - val_loss: 143379.7969\n",
      "Epoch 138/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 145474.2188 - val_loss: 142642.1094\n",
      "Epoch 139/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 144760.2188 - val_loss: 141920.6875\n",
      "Epoch 140/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 144059.2031 - val_loss: 141212.2500\n",
      "Epoch 141/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 143371.8125 - val_loss: 140537.0938\n",
      "Epoch 142/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 142708.3281 - val_loss: 139860.1875\n",
      "Epoch 143/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 142055.2188 - val_loss: 139227.6250\n",
      "Epoch 144/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 141425.1875 - val_loss: 138595.1406\n",
      "Epoch 145/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 140811.5469 - val_loss: 137981.9688\n",
      "Epoch 146/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 140225.3906 - val_loss: 137384.9844\n",
      "Epoch 147/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 139656.2188 - val_loss: 136806.6562\n",
      "Epoch 148/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 139106.5938 - val_loss: 136239.7969\n",
      "Epoch 149/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 138572.8750 - val_loss: 135688.0781\n",
      "Epoch 150/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 138047.7969 - val_loss: 135150.3125\n",
      "Epoch 151/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 137542.6406 - val_loss: 134631.0312\n",
      "Epoch 152/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 137051.8125 - val_loss: 134130.1094\n",
      "Epoch 153/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 136577.7969 - val_loss: 133618.7969\n",
      "Epoch 154/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 136109.9062 - val_loss: 133132.5625\n",
      "Epoch 155/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 135658.4531 - val_loss: 132653.6406\n",
      "Epoch 156/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 135213.3438 - val_loss: 132186.6250\n",
      "Epoch 157/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 134775.9844 - val_loss: 131733.1875\n",
      "Epoch 158/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 134349.9688 - val_loss: 131285.0469\n",
      "Epoch 159/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 133929.3438 - val_loss: 130846.2422\n",
      "Epoch 160/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 133512.8125 - val_loss: 130412.6172\n",
      "Epoch 161/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 133112.7812 - val_loss: 130000.3750\n",
      "Epoch 162/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 132721.2969 - val_loss: 129589.4219\n",
      "Epoch 163/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 132338.9375 - val_loss: 129195.7969\n",
      "Epoch 164/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 131969.2656 - val_loss: 128809.1797\n",
      "Epoch 165/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 131595.0000 - val_loss: 128430.3594\n",
      "Epoch 166/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 131236.3125 - val_loss: 128057.1406\n",
      "Epoch 167/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 130878.9375 - val_loss: 127695.9922\n",
      "Epoch 168/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 130532.5938 - val_loss: 127339.3828\n",
      "Epoch 169/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 130189.8516 - val_loss: 126986.7266\n",
      "Epoch 170/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 129848.5547 - val_loss: 126644.2031\n",
      "Epoch 171/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 129522.1797 - val_loss: 126314.9062\n",
      "Epoch 172/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 129194.4844 - val_loss: 125989.8594\n",
      "Epoch 173/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 128875.6172 - val_loss: 125670.2188\n",
      "Epoch 174/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 128557.2969 - val_loss: 125360.3984\n",
      "Epoch 175/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 128245.8047 - val_loss: 125052.4766\n",
      "Epoch 176/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 127936.3828 - val_loss: 124764.8281\n",
      "Epoch 177/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 127630.5312 - val_loss: 124464.6719\n",
      "Epoch 178/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 127325.3906 - val_loss: 124169.5781\n",
      "Epoch 179/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 127030.3594 - val_loss: 123877.1406\n",
      "Epoch 180/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 126732.5391 - val_loss: 123591.4844\n",
      "Epoch 181/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 126444.6328 - val_loss: 123322.4609\n",
      "Epoch 182/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 126163.1719 - val_loss: 123051.5156\n",
      "Epoch 183/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 125884.4922 - val_loss: 122786.1641\n",
      "Epoch 184/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 125605.4219 - val_loss: 122525.6484\n",
      "Epoch 185/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 125329.6797 - val_loss: 122260.3594\n",
      "Epoch 186/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 125062.6172 - val_loss: 122014.1875\n",
      "Epoch 187/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 124797.4844 - val_loss: 121760.6016\n",
      "Epoch 188/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 124531.9453 - val_loss: 121511.0938\n",
      "Epoch 189/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 124270.1719 - val_loss: 121270.3281\n",
      "Epoch 190/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 124015.2422 - val_loss: 121012.6719\n",
      "Epoch 191/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 123751.7891 - val_loss: 120777.1016\n",
      "Epoch 192/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 123499.7109 - val_loss: 120532.6953\n",
      "Epoch 193/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 123252.3594 - val_loss: 120292.0938\n",
      "Epoch 194/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 123003.2656 - val_loss: 120063.9375\n",
      "Epoch 195/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 122760.4219 - val_loss: 119821.8125\n",
      "Epoch 196/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 122513.7188 - val_loss: 119601.9219\n",
      "Epoch 197/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 122275.7969 - val_loss: 119367.9297\n",
      "Epoch 198/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 122041.8672 - val_loss: 119142.6328\n",
      "Epoch 199/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 121808.0469 - val_loss: 118924.8125\n",
      "Epoch 200/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 121577.2188 - val_loss: 118716.0625\n",
      "Epoch 201/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 121349.6250 - val_loss: 118509.4062\n",
      "Epoch 202/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 121128.8438 - val_loss: 118311.4141\n",
      "Epoch 203/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 120910.3281 - val_loss: 118108.8828\n",
      "Epoch 204/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 120697.2656 - val_loss: 117914.9688\n",
      "Epoch 205/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 120488.6562 - val_loss: 117722.7891\n",
      "Epoch 206/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 120281.5312 - val_loss: 117537.2031\n",
      "Epoch 207/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 120079.3516 - val_loss: 117355.1719\n",
      "Epoch 208/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 119876.9297 - val_loss: 117175.2266\n",
      "Epoch 209/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 119678.6016 - val_loss: 116991.4453\n",
      "Epoch 210/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 119482.9688 - val_loss: 116822.7188\n",
      "Epoch 211/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 119289.5547 - val_loss: 116657.7500\n",
      "Epoch 212/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 119097.8750 - val_loss: 116486.4141\n",
      "Epoch 213/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 118912.7578 - val_loss: 116312.2734\n",
      "Epoch 214/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 118732.3125 - val_loss: 116150.9219\n",
      "Epoch 215/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 118549.5312 - val_loss: 115980.5547\n",
      "Epoch 216/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 118374.5781 - val_loss: 115837.7969\n",
      "Epoch 217/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 118204.3203 - val_loss: 115684.8594\n",
      "Epoch 218/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 118037.2109 - val_loss: 115527.5234\n",
      "Epoch 219/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 117866.2656 - val_loss: 115376.5859\n",
      "Epoch 220/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 117703.3750 - val_loss: 115230.6875\n",
      "Epoch 221/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 117546.8828 - val_loss: 115085.0469\n",
      "Epoch 222/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 117382.7031 - val_loss: 114942.0781\n",
      "Epoch 223/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 117226.0234 - val_loss: 114805.4062\n",
      "Epoch 224/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 117070.2500 - val_loss: 114666.3359\n",
      "Epoch 225/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 116920.2266 - val_loss: 114539.5781\n",
      "Epoch 226/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 116774.2578 - val_loss: 114405.7266\n",
      "Epoch 227/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 116625.0703 - val_loss: 114278.2891\n",
      "Epoch 228/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 116475.5156 - val_loss: 114155.2812\n",
      "Epoch 229/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 116327.0234 - val_loss: 114027.6172\n",
      "Epoch 230/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 116183.7812 - val_loss: 113900.4531\n",
      "Epoch 231/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 116039.0938 - val_loss: 113782.0312\n",
      "Epoch 232/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 115897.9609 - val_loss: 113669.9219\n",
      "Epoch 233/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 115756.4609 - val_loss: 113560.4453\n",
      "Epoch 234/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 115624.8438 - val_loss: 113438.8438\n",
      "Epoch 235/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 115485.6250 - val_loss: 113331.3438\n",
      "Epoch 236/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 115354.5469 - val_loss: 113220.4844\n",
      "Epoch 237/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 115221.9766 - val_loss: 113114.1094\n",
      "Epoch 238/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 115096.3047 - val_loss: 113008.4453\n",
      "Epoch 239/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 114969.1328 - val_loss: 112912.3125\n",
      "Epoch 240/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 114846.2969 - val_loss: 112805.5938\n",
      "Epoch 241/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 114728.1562 - val_loss: 112710.1875\n",
      "Epoch 242/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 114615.1797 - val_loss: 112619.4375\n",
      "Epoch 243/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 114502.4297 - val_loss: 112528.0859\n",
      "Epoch 244/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 114396.8281 - val_loss: 112445.8750\n",
      "Epoch 245/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 114292.9609 - val_loss: 112353.8750\n",
      "Epoch 246/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 114187.2812 - val_loss: 112270.6016\n",
      "Epoch 247/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 114079.9531 - val_loss: 112189.8672\n",
      "Epoch 248/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 113984.7969 - val_loss: 112107.2812\n",
      "Epoch 249/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 113882.8047 - val_loss: 112022.8672\n",
      "Epoch 250/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 113782.4219 - val_loss: 111947.6172\n",
      "Epoch 251/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 113689.2891 - val_loss: 111872.1953\n",
      "Epoch 252/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 113594.4375 - val_loss: 111795.7969\n",
      "Epoch 253/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 113505.7188 - val_loss: 111723.4688\n",
      "Epoch 254/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 113419.6562 - val_loss: 111651.4922\n",
      "Epoch 255/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 113331.1719 - val_loss: 111577.2969\n",
      "Epoch 256/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 113247.0234 - val_loss: 111512.6172\n",
      "Epoch 257/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 113165.7344 - val_loss: 111442.1250\n",
      "Epoch 258/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 113085.5703 - val_loss: 111374.0938\n",
      "Epoch 259/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 113003.2031 - val_loss: 111310.3047\n",
      "Epoch 260/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 112930.8750 - val_loss: 111246.6797\n",
      "Epoch 261/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 112853.9219 - val_loss: 111191.5781\n",
      "Epoch 262/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 112784.2500 - val_loss: 111121.0078\n",
      "Epoch 263/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 112717.4609 - val_loss: 111064.7266\n",
      "Epoch 264/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 112642.5000 - val_loss: 111006.9844\n",
      "Epoch 265/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 112576.0469 - val_loss: 110949.7031\n",
      "Epoch 266/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 112509.3359 - val_loss: 110889.5938\n",
      "Epoch 267/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 112444.6953 - val_loss: 110834.9922\n",
      "Epoch 268/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 112381.8906 - val_loss: 110783.3750\n",
      "Epoch 269/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 112319.0859 - val_loss: 110732.4297\n",
      "Epoch 270/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 112260.7812 - val_loss: 110680.2891\n",
      "Epoch 271/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 112196.6250 - val_loss: 110616.6875\n",
      "Epoch 272/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 112138.7188 - val_loss: 110568.4062\n",
      "Epoch 273/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 112077.9766 - val_loss: 110517.1719\n",
      "Epoch 274/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 112019.6016 - val_loss: 110471.6562\n",
      "Epoch 275/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 111965.9922 - val_loss: 110418.7109\n",
      "Epoch 276/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 111909.4297 - val_loss: 110374.2812\n",
      "Epoch 277/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 111854.6875 - val_loss: 110323.1094\n",
      "Epoch 278/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 111797.3516 - val_loss: 110273.6016\n",
      "Epoch 279/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 111741.3672 - val_loss: 110228.0000\n",
      "Epoch 280/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 111688.3672 - val_loss: 110179.4609\n",
      "Epoch 281/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 111632.9609 - val_loss: 110127.8906\n",
      "Epoch 282/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 111577.8359 - val_loss: 110086.4922\n",
      "Epoch 283/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 111528.3828 - val_loss: 110041.9922\n",
      "Epoch 284/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 111476.0781 - val_loss: 109997.5781\n",
      "Epoch 285/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 111424.9453 - val_loss: 109954.3281\n",
      "Epoch 286/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 111372.2109 - val_loss: 109906.8906\n",
      "Epoch 287/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 111320.2422 - val_loss: 109864.2344\n",
      "Epoch 288/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 111271.3359 - val_loss: 109824.0469\n",
      "Epoch 289/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 111226.3438 - val_loss: 109778.5625\n",
      "Epoch 290/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 111174.4219 - val_loss: 109740.8516\n",
      "Epoch 291/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 111123.9609 - val_loss: 109707.3672\n",
      "Epoch 292/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 111076.2812 - val_loss: 109663.7109\n",
      "Epoch 293/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 111031.7500 - val_loss: 109627.8359\n",
      "Epoch 294/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110987.0234 - val_loss: 109590.7344\n",
      "Epoch 295/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110937.1172 - val_loss: 109546.8750\n",
      "Epoch 296/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110893.3594 - val_loss: 109509.1797\n",
      "Epoch 297/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110848.8750 - val_loss: 109474.2344\n",
      "Epoch 298/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110804.2188 - val_loss: 109443.6406\n",
      "Epoch 299/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110757.6172 - val_loss: 109397.5312\n",
      "Epoch 300/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110716.7812 - val_loss: 109365.4297\n",
      "Epoch 301/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110668.2188 - val_loss: 109333.0859\n",
      "Epoch 302/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110625.3359 - val_loss: 109295.0547\n",
      "Epoch 303/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110583.6406 - val_loss: 109258.7578\n",
      "Epoch 304/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110542.9375 - val_loss: 109223.4297\n",
      "Epoch 305/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110498.1562 - val_loss: 109184.3359\n",
      "Epoch 306/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110460.3750 - val_loss: 109152.1641\n",
      "Epoch 307/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110419.7266 - val_loss: 109119.2422\n",
      "Epoch 308/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110378.0391 - val_loss: 109086.7969\n",
      "Epoch 309/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110342.8594 - val_loss: 109053.7422\n",
      "Epoch 310/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110303.1250 - val_loss: 109027.4375\n",
      "Epoch 311/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110261.8047 - val_loss: 108989.7188\n",
      "Epoch 312/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 110229.0469 - val_loss: 108956.1016\n",
      "Epoch 313/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110189.7344 - val_loss: 108926.7891\n",
      "Epoch 314/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110151.4766 - val_loss: 108893.0078\n",
      "Epoch 315/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110115.8984 - val_loss: 108860.8125\n",
      "Epoch 316/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110080.1484 - val_loss: 108828.8047\n",
      "Epoch 317/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110040.2812 - val_loss: 108795.8672\n",
      "Epoch 318/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110006.0234 - val_loss: 108766.0078\n",
      "Epoch 319/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109969.6562 - val_loss: 108739.4766\n",
      "Epoch 320/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109936.1875 - val_loss: 108710.7109\n",
      "Epoch 321/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109897.4453 - val_loss: 108685.2969\n",
      "Epoch 322/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109863.1406 - val_loss: 108651.7422\n",
      "Epoch 323/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109826.1094 - val_loss: 108622.8359\n",
      "Epoch 324/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109791.5703 - val_loss: 108591.1719\n",
      "Epoch 325/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109754.8984 - val_loss: 108571.5859\n",
      "Epoch 326/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109722.2734 - val_loss: 108532.8984\n",
      "Epoch 327/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109687.1953 - val_loss: 108502.5703\n",
      "Epoch 328/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109651.0156 - val_loss: 108476.4141\n",
      "Epoch 329/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109617.7812 - val_loss: 108452.4219\n",
      "Epoch 330/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109582.8438 - val_loss: 108421.4844\n",
      "Epoch 331/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109551.0547 - val_loss: 108400.6875\n",
      "Epoch 332/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109517.1953 - val_loss: 108371.5234\n",
      "Epoch 333/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109485.8203 - val_loss: 108345.2656\n",
      "Epoch 334/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109450.1641 - val_loss: 108319.9297\n",
      "Epoch 335/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109414.5781 - val_loss: 108290.1875\n",
      "Epoch 336/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109384.5859 - val_loss: 108256.7812\n",
      "Epoch 337/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109351.3672 - val_loss: 108231.5078\n",
      "Epoch 338/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109315.4922 - val_loss: 108204.9688\n",
      "Epoch 339/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109280.0469 - val_loss: 108179.0078\n",
      "Epoch 340/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109250.5781 - val_loss: 108149.8438\n",
      "Epoch 341/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109216.6016 - val_loss: 108126.4609\n",
      "Epoch 342/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109185.8672 - val_loss: 108097.1953\n",
      "Epoch 343/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109150.7969 - val_loss: 108069.7812\n",
      "Epoch 344/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109118.1953 - val_loss: 108047.3750\n",
      "Epoch 345/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109086.7578 - val_loss: 108017.0391\n",
      "Epoch 346/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109054.4375 - val_loss: 107989.3672\n",
      "Epoch 347/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109022.7500 - val_loss: 107969.2969\n",
      "Epoch 348/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108991.5703 - val_loss: 107940.6328\n",
      "Epoch 349/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108965.7344 - val_loss: 107918.1172\n",
      "Epoch 350/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108934.1953 - val_loss: 107885.8672\n",
      "Epoch 351/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108902.8594 - val_loss: 107859.7969\n",
      "Epoch 352/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108872.7734 - val_loss: 107835.9609\n",
      "Epoch 353/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108848.9688 - val_loss: 107805.4922\n",
      "Epoch 354/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108813.1797 - val_loss: 107779.9844\n",
      "Epoch 355/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108782.5859 - val_loss: 107756.1875\n",
      "Epoch 356/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108752.3594 - val_loss: 107728.2969\n",
      "Epoch 357/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108724.1094 - val_loss: 107698.8359\n",
      "Epoch 358/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108694.5078 - val_loss: 107670.7969\n",
      "Epoch 359/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108664.1406 - val_loss: 107645.9531\n",
      "Epoch 360/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108643.1953 - val_loss: 107617.5859\n",
      "Epoch 361/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108608.3516 - val_loss: 107587.1953\n",
      "Epoch 362/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108582.6484 - val_loss: 107568.4531\n",
      "Epoch 363/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108554.9375 - val_loss: 107534.0625\n",
      "Epoch 364/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108523.4375 - val_loss: 107519.8906\n",
      "Epoch 365/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108499.3516 - val_loss: 107490.3125\n",
      "Epoch 366/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108469.2578 - val_loss: 107470.0078\n",
      "Epoch 367/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108440.4375 - val_loss: 107439.4375\n",
      "Epoch 368/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108415.5156 - val_loss: 107412.2812\n",
      "Epoch 369/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 108388.2500 - val_loss: 107390.0469\n",
      "Epoch 370/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108363.5859 - val_loss: 107358.4766\n",
      "Epoch 371/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 108335.8594 - val_loss: 107335.6094\n",
      "Epoch 372/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 108307.7734 - val_loss: 107307.7188\n",
      "Epoch 373/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 108284.8125 - val_loss: 107284.8750\n",
      "Epoch 374/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108259.6719 - val_loss: 107262.8906\n",
      "Epoch 375/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 108232.1016 - val_loss: 107238.5625\n",
      "Epoch 376/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 108204.3750 - val_loss: 107211.7734\n",
      "Epoch 377/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 108180.1953 - val_loss: 107186.7500\n",
      "Epoch 378/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 108157.6641 - val_loss: 107160.6172\n",
      "Epoch 379/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108130.1484 - val_loss: 107137.3828\n",
      "Epoch 380/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108106.0938 - val_loss: 107115.3281\n",
      "Epoch 381/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108080.1016 - val_loss: 107088.6406\n",
      "Epoch 382/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108054.5859 - val_loss: 107066.4141\n",
      "Epoch 383/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108030.3828 - val_loss: 107041.6797\n",
      "Epoch 384/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108004.7734 - val_loss: 107011.3672\n",
      "Epoch 385/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107980.1172 - val_loss: 106987.0938\n",
      "Epoch 386/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107956.9766 - val_loss: 106973.4531\n",
      "Epoch 387/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107931.3203 - val_loss: 106941.1953\n",
      "Epoch 388/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107909.3125 - val_loss: 106912.5625\n",
      "Epoch 389/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107884.2656 - val_loss: 106891.9844\n",
      "Epoch 390/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107861.4453 - val_loss: 106865.9219\n",
      "Epoch 391/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107839.3906 - val_loss: 106845.1172\n",
      "Epoch 392/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 107811.8125 - val_loss: 106818.3672\n",
      "Epoch 393/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 107791.6328 - val_loss: 106793.1406\n",
      "Epoch 394/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107768.9062 - val_loss: 106775.0312\n",
      "Epoch 395/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107741.3750 - val_loss: 106751.6719\n",
      "Epoch 396/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107717.8516 - val_loss: 106721.9922\n",
      "Epoch 397/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107700.0234 - val_loss: 106707.1719\n",
      "Epoch 398/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107675.2969 - val_loss: 106676.7109\n",
      "Epoch 399/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107650.5078 - val_loss: 106653.6797\n",
      "Epoch 400/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107626.8750 - val_loss: 106626.9531\n",
      "Epoch 401/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107606.4141 - val_loss: 106602.7422\n",
      "Epoch 402/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107584.5078 - val_loss: 106572.5781\n",
      "Epoch 403/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107565.9375 - val_loss: 106550.1016\n",
      "Epoch 404/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107542.6953 - val_loss: 106523.2031\n",
      "Epoch 405/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107521.9531 - val_loss: 106504.3203\n",
      "Epoch 406/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107497.2109 - val_loss: 106472.0547\n",
      "Epoch 407/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107477.9766 - val_loss: 106446.7578\n",
      "Epoch 408/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107457.8281 - val_loss: 106429.5781\n",
      "Epoch 409/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 107430.5156 - val_loss: 106405.6328\n",
      "Epoch 410/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107410.9609 - val_loss: 106382.5469\n",
      "Epoch 411/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107389.8672 - val_loss: 106360.5312\n",
      "Epoch 412/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107372.7891 - val_loss: 106338.5781\n",
      "Epoch 413/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107352.4062 - val_loss: 106319.5000\n",
      "Epoch 414/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107332.8203 - val_loss: 106293.4219\n",
      "Epoch 415/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107313.8125 - val_loss: 106276.5078\n",
      "Epoch 416/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107293.5938 - val_loss: 106252.4844\n",
      "Epoch 417/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107275.0156 - val_loss: 106230.7500\n",
      "Epoch 418/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107256.2500 - val_loss: 106214.5234\n",
      "Epoch 419/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107237.2422 - val_loss: 106191.7656\n",
      "Epoch 420/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107218.9609 - val_loss: 106170.0391\n",
      "Epoch 421/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107199.8672 - val_loss: 106153.0312\n",
      "Epoch 422/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107181.2812 - val_loss: 106128.7188\n",
      "Epoch 423/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107164.1719 - val_loss: 106114.8359\n",
      "Epoch 424/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107145.2031 - val_loss: 106093.5391\n",
      "Epoch 425/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107127.5625 - val_loss: 106069.1250\n",
      "Epoch 426/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107110.3906 - val_loss: 106056.6484\n",
      "Epoch 427/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107092.6250 - val_loss: 106039.1797\n",
      "Epoch 428/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107077.7188 - val_loss: 106018.4062\n",
      "Epoch 429/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107058.9141 - val_loss: 106001.1562\n",
      "Epoch 430/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107037.6406 - val_loss: 105978.0781\n",
      "Epoch 431/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107020.1641 - val_loss: 105959.7188\n",
      "Epoch 432/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107003.7969 - val_loss: 105941.9453\n",
      "Epoch 433/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106986.4297 - val_loss: 105924.8750\n",
      "Epoch 434/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106971.0391 - val_loss: 105907.1484\n",
      "Epoch 435/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106950.1328 - val_loss: 105885.4844\n",
      "Epoch 436/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106933.4453 - val_loss: 105863.2188\n",
      "Epoch 437/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106914.8125 - val_loss: 105847.3906\n",
      "Epoch 438/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106898.0703 - val_loss: 105827.9219\n",
      "Epoch 439/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106881.5703 - val_loss: 105809.0078\n",
      "Epoch 440/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106863.5156 - val_loss: 105788.5859\n",
      "Epoch 441/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 106846.6641 - val_loss: 105773.4844\n",
      "Epoch 442/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106831.4453 - val_loss: 105762.1875\n",
      "Epoch 443/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106813.3359 - val_loss: 105737.8516\n",
      "Epoch 444/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 106793.0547 - val_loss: 105719.1953\n",
      "Epoch 445/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 106774.0000 - val_loss: 105701.2969\n",
      "Epoch 446/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106760.2500 - val_loss: 105681.0000\n",
      "Epoch 447/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106741.9219 - val_loss: 105660.2188\n",
      "Epoch 448/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106724.7969 - val_loss: 105641.1172\n",
      "Epoch 449/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106706.2109 - val_loss: 105617.3750\n",
      "Epoch 450/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106690.1172 - val_loss: 105600.9609\n",
      "Epoch 451/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106671.9609 - val_loss: 105588.4922\n",
      "Epoch 452/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106654.9297 - val_loss: 105564.4844\n",
      "Epoch 453/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106635.9844 - val_loss: 105549.9062\n",
      "Epoch 454/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106620.3984 - val_loss: 105530.8359\n",
      "Epoch 455/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106610.2188 - val_loss: 105515.1562\n",
      "Epoch 456/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106588.5000 - val_loss: 105495.3906\n",
      "Epoch 457/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106575.7891 - val_loss: 105480.8906\n",
      "Epoch 458/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106551.4375 - val_loss: 105462.1328\n",
      "Epoch 459/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106540.3984 - val_loss: 105434.0781\n",
      "Epoch 460/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106523.8750 - val_loss: 105421.6875\n",
      "Epoch 461/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106508.4297 - val_loss: 105401.4297\n",
      "Epoch 462/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106490.9141 - val_loss: 105390.0391\n",
      "Epoch 463/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106470.2188 - val_loss: 105365.9297\n",
      "Epoch 464/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106457.6719 - val_loss: 105347.4141\n",
      "Epoch 465/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106441.7266 - val_loss: 105330.0469\n",
      "Epoch 466/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106425.3594 - val_loss: 105313.3984\n",
      "Epoch 467/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 106407.3828 - val_loss: 105305.5938\n",
      "Epoch 468/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106395.6953 - val_loss: 105287.2812\n",
      "Epoch 469/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106381.6250 - val_loss: 105266.7656\n",
      "Epoch 470/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 106363.0078 - val_loss: 105250.2812\n",
      "Epoch 471/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 106348.1875 - val_loss: 105242.3281\n",
      "Epoch 472/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106336.2344 - val_loss: 105221.7656\n",
      "Epoch 473/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 106320.2734 - val_loss: 105209.7812\n",
      "Epoch 474/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 106308.6719 - val_loss: 105196.8984\n",
      "Epoch 475/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106289.7188 - val_loss: 105178.9141\n",
      "Epoch 476/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106275.4609 - val_loss: 105154.0547\n",
      "Epoch 477/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106260.2656 - val_loss: 105140.3594\n",
      "Epoch 478/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106244.2344 - val_loss: 105122.3672\n",
      "Epoch 479/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106227.2734 - val_loss: 105111.6094\n",
      "Epoch 480/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106213.6797 - val_loss: 105094.5234\n",
      "Epoch 481/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106202.7891 - val_loss: 105078.6094\n",
      "Epoch 482/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106186.7656 - val_loss: 105062.5547\n",
      "Epoch 483/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106169.9531 - val_loss: 105044.8750\n",
      "Epoch 484/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106155.7109 - val_loss: 105029.8828\n",
      "Epoch 485/1000\n",
      "130/130 [==============================] - 1s 4ms/step - loss: 106139.7266 - val_loss: 105011.4609\n",
      "Epoch 486/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 106123.3047 - val_loss: 104998.5156\n",
      "Epoch 487/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 106112.3125 - val_loss: 104975.9375\n",
      "Epoch 488/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106094.2422 - val_loss: 104969.5469\n",
      "Epoch 489/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106080.9766 - val_loss: 104954.0781\n",
      "Epoch 490/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106064.0391 - val_loss: 104933.0078\n",
      "Epoch 491/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106050.6875 - val_loss: 104912.6719\n",
      "Epoch 492/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106031.9062 - val_loss: 104898.4219\n",
      "Epoch 493/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106022.1953 - val_loss: 104883.9219\n",
      "Epoch 494/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106007.5156 - val_loss: 104864.3672\n",
      "Epoch 495/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105988.7578 - val_loss: 104852.8047\n",
      "Epoch 496/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105973.4375 - val_loss: 104841.8906\n",
      "Epoch 497/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105959.4766 - val_loss: 104822.4375\n",
      "Epoch 498/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105944.7656 - val_loss: 104807.3359\n",
      "Epoch 499/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105930.5938 - val_loss: 104789.2812\n",
      "Epoch 500/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105916.4844 - val_loss: 104772.1641\n",
      "Epoch 501/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105899.1094 - val_loss: 104753.9219\n",
      "Epoch 502/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105885.2500 - val_loss: 104742.4141\n",
      "Epoch 503/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105870.5312 - val_loss: 104721.9453\n",
      "Epoch 504/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105859.4609 - val_loss: 104709.9219\n",
      "Epoch 505/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105842.1484 - val_loss: 104690.3672\n",
      "Epoch 506/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105830.5781 - val_loss: 104674.8594\n",
      "Epoch 507/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105815.3984 - val_loss: 104664.7969\n",
      "Epoch 508/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105805.2344 - val_loss: 104646.3047\n",
      "Epoch 509/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105785.7344 - val_loss: 104631.2344\n",
      "Epoch 510/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105771.0859 - val_loss: 104612.8203\n",
      "Epoch 511/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105760.3047 - val_loss: 104600.4062\n",
      "Epoch 512/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105745.7578 - val_loss: 104588.3516\n",
      "Epoch 513/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105728.7422 - val_loss: 104570.2188\n",
      "Epoch 514/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105714.5938 - val_loss: 104558.2266\n",
      "Epoch 515/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105702.1406 - val_loss: 104538.4609\n",
      "Epoch 516/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105688.1328 - val_loss: 104524.8047\n",
      "Epoch 517/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105671.9922 - val_loss: 104508.8594\n",
      "Epoch 518/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105663.8438 - val_loss: 104499.3750\n",
      "Epoch 519/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105646.7500 - val_loss: 104488.6328\n",
      "Epoch 520/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105633.0859 - val_loss: 104473.1016\n",
      "Epoch 521/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105622.6328 - val_loss: 104456.3281\n",
      "Epoch 522/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105612.6641 - val_loss: 104443.2031\n",
      "Epoch 523/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105593.8828 - val_loss: 104430.3047\n",
      "Epoch 524/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105578.9062 - val_loss: 104415.2109\n",
      "Epoch 525/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105568.1719 - val_loss: 104401.2109\n",
      "Epoch 526/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105553.1094 - val_loss: 104385.5312\n",
      "Epoch 527/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105540.7734 - val_loss: 104367.9766\n",
      "Epoch 528/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105529.6172 - val_loss: 104352.6875\n",
      "Epoch 529/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105514.9297 - val_loss: 104349.1953\n",
      "Epoch 530/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105501.9922 - val_loss: 104326.5703\n",
      "Epoch 531/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105491.2422 - val_loss: 104322.5078\n",
      "Epoch 532/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105469.9844 - val_loss: 104308.2969\n",
      "Epoch 533/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105459.6797 - val_loss: 104288.8125\n",
      "Epoch 534/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105445.8047 - val_loss: 104275.3594\n",
      "Epoch 535/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105433.2812 - val_loss: 104267.4766\n",
      "Epoch 536/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105422.6641 - val_loss: 104249.2031\n",
      "Epoch 537/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105409.3281 - val_loss: 104239.2734\n",
      "Epoch 538/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105394.2344 - val_loss: 104225.1484\n",
      "Epoch 539/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105382.6484 - val_loss: 104210.3516\n",
      "Epoch 540/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105368.4297 - val_loss: 104193.3047\n",
      "Epoch 541/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105353.9375 - val_loss: 104184.2734\n",
      "Epoch 542/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105344.8672 - val_loss: 104167.7969\n",
      "Epoch 543/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105328.9375 - val_loss: 104154.3438\n",
      "Epoch 544/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105315.6875 - val_loss: 104138.9375\n",
      "Epoch 545/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105301.1797 - val_loss: 104127.0391\n",
      "Epoch 546/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105290.1641 - val_loss: 104107.2891\n",
      "Epoch 547/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105274.2812 - val_loss: 104092.2891\n",
      "Epoch 548/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105264.7969 - val_loss: 104085.3906\n",
      "Epoch 549/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105254.6953 - val_loss: 104067.0078\n",
      "Epoch 550/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105239.0234 - val_loss: 104052.1484\n",
      "Epoch 551/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105228.4453 - val_loss: 104040.2109\n",
      "Epoch 552/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105212.7109 - val_loss: 104029.3828\n",
      "Epoch 553/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105201.4609 - val_loss: 104019.1797\n",
      "Epoch 554/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105187.5234 - val_loss: 104003.8672\n",
      "Epoch 555/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105177.8516 - val_loss: 103990.8906\n",
      "Epoch 556/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105163.7500 - val_loss: 103978.6719\n",
      "Epoch 557/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105153.7422 - val_loss: 103955.9375\n",
      "Epoch 558/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105139.2031 - val_loss: 103951.4375\n",
      "Epoch 559/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105129.2812 - val_loss: 103939.9375\n",
      "Epoch 560/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105116.3828 - val_loss: 103920.6719\n",
      "Epoch 561/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105104.6562 - val_loss: 103914.4609\n",
      "Epoch 562/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105093.7578 - val_loss: 103903.6250\n",
      "Epoch 563/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105077.2188 - val_loss: 103888.8359\n",
      "Epoch 564/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105063.4531 - val_loss: 103877.8750\n",
      "Epoch 565/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105055.1875 - val_loss: 103864.4922\n",
      "Epoch 566/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105040.5469 - val_loss: 103850.5625\n",
      "Epoch 567/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105030.2656 - val_loss: 103838.9609\n",
      "Epoch 568/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105017.5625 - val_loss: 103827.0781\n",
      "Epoch 569/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105007.4609 - val_loss: 103809.9219\n",
      "Epoch 570/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104993.1875 - val_loss: 103802.0938\n",
      "Epoch 571/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104985.7344 - val_loss: 103781.6875\n",
      "Epoch 572/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104967.9844 - val_loss: 103780.2422\n",
      "Epoch 573/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104954.8750 - val_loss: 103767.7969\n",
      "Epoch 574/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104943.9062 - val_loss: 103755.2812\n",
      "Epoch 575/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104932.2188 - val_loss: 103733.5312\n",
      "Epoch 576/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104921.1172 - val_loss: 103721.9219\n",
      "Epoch 577/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104909.5000 - val_loss: 103709.5391\n",
      "Epoch 578/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104897.6328 - val_loss: 103700.2812\n",
      "Epoch 579/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104886.0234 - val_loss: 103686.7500\n",
      "Epoch 580/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104877.3516 - val_loss: 103671.3203\n",
      "Epoch 581/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 104863.0625 - val_loss: 103663.4844\n",
      "Epoch 582/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 104848.4531 - val_loss: 103653.3438\n",
      "Epoch 583/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 104838.1328 - val_loss: 103643.9531\n",
      "Epoch 584/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 104827.3594 - val_loss: 103634.5391\n",
      "Epoch 585/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 104813.4062 - val_loss: 103618.6641\n",
      "Epoch 586/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104806.5234 - val_loss: 103610.1406\n",
      "Epoch 587/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 104789.3984 - val_loss: 103596.7734\n",
      "Epoch 588/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 104780.1328 - val_loss: 103582.5391\n",
      "Epoch 589/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 104771.9531 - val_loss: 103565.6094\n",
      "Epoch 590/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 104760.2266 - val_loss: 103561.6641\n",
      "Epoch 591/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 104744.5625 - val_loss: 103549.4219\n",
      "Epoch 592/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 104735.6406 - val_loss: 103540.7109\n",
      "Epoch 593/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 104720.9766 - val_loss: 103529.0000\n",
      "Epoch 594/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 104712.0625 - val_loss: 103515.4688\n",
      "Epoch 595/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104697.2812 - val_loss: 103504.3672\n",
      "Epoch 596/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104689.1328 - val_loss: 103495.3516\n",
      "Epoch 597/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104674.9688 - val_loss: 103475.8594\n",
      "Epoch 598/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104669.2266 - val_loss: 103471.1406\n",
      "Epoch 599/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104657.0625 - val_loss: 103458.1641\n",
      "Epoch 600/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104644.6250 - val_loss: 103443.9297\n",
      "Epoch 601/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104632.3438 - val_loss: 103433.5781\n",
      "Epoch 602/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104621.9297 - val_loss: 103422.7891\n",
      "Epoch 603/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 104611.1484 - val_loss: 103414.1953\n",
      "Epoch 604/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104599.8125 - val_loss: 103401.1797\n",
      "Epoch 605/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104586.8125 - val_loss: 103387.8516\n",
      "Epoch 606/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104575.5469 - val_loss: 103379.0859\n",
      "Epoch 607/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104562.9297 - val_loss: 103367.9766\n",
      "Epoch 608/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 104551.5469 - val_loss: 103357.3750\n",
      "Epoch 609/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104544.5078 - val_loss: 103344.1562\n",
      "Epoch 610/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104534.9531 - val_loss: 103332.7578\n",
      "Epoch 611/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104521.7344 - val_loss: 103318.1562\n",
      "Epoch 612/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104509.1406 - val_loss: 103312.9688\n",
      "Epoch 613/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104496.3125 - val_loss: 103300.9062\n",
      "Epoch 614/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104486.7734 - val_loss: 103293.9531\n",
      "Epoch 615/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104473.9219 - val_loss: 103277.0312\n",
      "Epoch 616/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104463.7578 - val_loss: 103268.1484\n",
      "Epoch 617/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104452.3906 - val_loss: 103256.9297\n",
      "Epoch 618/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104439.7109 - val_loss: 103245.3828\n",
      "Epoch 619/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104430.3594 - val_loss: 103240.0156\n",
      "Epoch 620/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104420.9219 - val_loss: 103223.0703\n",
      "Epoch 621/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104409.4219 - val_loss: 103217.2109\n",
      "Epoch 622/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104402.0000 - val_loss: 103204.8594\n",
      "Epoch 623/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104390.6719 - val_loss: 103188.6875\n",
      "Epoch 624/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104375.7109 - val_loss: 103183.1484\n",
      "Epoch 625/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104365.1328 - val_loss: 103175.7422\n",
      "Epoch 626/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104357.8281 - val_loss: 103159.0547\n",
      "Epoch 627/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104345.5781 - val_loss: 103148.1797\n",
      "Epoch 628/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104332.1328 - val_loss: 103138.7109\n",
      "Epoch 629/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104326.0391 - val_loss: 103136.5156\n",
      "Epoch 630/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104311.3750 - val_loss: 103120.7031\n",
      "Epoch 631/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104301.8828 - val_loss: 103107.8594\n",
      "Epoch 632/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104289.1797 - val_loss: 103096.0859\n",
      "Epoch 633/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104278.4531 - val_loss: 103089.6562\n",
      "Epoch 634/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104269.1719 - val_loss: 103078.5547\n",
      "Epoch 635/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104260.0312 - val_loss: 103077.0234\n",
      "Epoch 636/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104248.8516 - val_loss: 103056.5625\n",
      "Epoch 637/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104237.0781 - val_loss: 103047.0000\n",
      "Epoch 638/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104227.8750 - val_loss: 103034.7969\n",
      "Epoch 639/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104215.2266 - val_loss: 103026.4766\n",
      "Epoch 640/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104204.0625 - val_loss: 103020.1406\n",
      "Epoch 641/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104197.5469 - val_loss: 103012.6797\n",
      "Epoch 642/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104181.6484 - val_loss: 102996.1484\n",
      "Epoch 643/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104172.0234 - val_loss: 102988.8984\n",
      "Epoch 644/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104161.2109 - val_loss: 102982.5469\n",
      "Epoch 645/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104153.5781 - val_loss: 102968.8750\n",
      "Epoch 646/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104142.5547 - val_loss: 102963.8984\n",
      "Epoch 647/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104128.8125 - val_loss: 102937.2656\n",
      "Epoch 648/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104119.4375 - val_loss: 102933.6562\n",
      "Epoch 649/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104105.7578 - val_loss: 102928.6094\n",
      "Epoch 650/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104095.7969 - val_loss: 102915.3359\n",
      "Epoch 651/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104081.5938 - val_loss: 102907.7188\n",
      "Epoch 652/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104076.3750 - val_loss: 102896.3984\n",
      "Epoch 653/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104066.0000 - val_loss: 102881.9062\n",
      "Epoch 654/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104051.0078 - val_loss: 102874.2344\n",
      "Epoch 655/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104041.8672 - val_loss: 102864.5312\n",
      "Epoch 656/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104032.6562 - val_loss: 102856.4609\n",
      "Epoch 657/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104022.9531 - val_loss: 102847.4766\n",
      "Epoch 658/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104008.4766 - val_loss: 102834.0703\n",
      "Epoch 659/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103997.6172 - val_loss: 102829.2891\n",
      "Epoch 660/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103987.0703 - val_loss: 102817.8125\n",
      "Epoch 661/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103977.2188 - val_loss: 102810.1484\n",
      "Epoch 662/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103969.4375 - val_loss: 102796.1719\n",
      "Epoch 663/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103955.8438 - val_loss: 102791.1641\n",
      "Epoch 664/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103947.2656 - val_loss: 102783.4609\n",
      "Epoch 665/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103939.6719 - val_loss: 102768.5000\n",
      "Epoch 666/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103927.3750 - val_loss: 102759.7422\n",
      "Epoch 667/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103916.1797 - val_loss: 102751.2891\n",
      "Epoch 668/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103908.3984 - val_loss: 102745.4766\n",
      "Epoch 669/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103897.5156 - val_loss: 102736.7891\n",
      "Epoch 670/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103889.8594 - val_loss: 102728.6875\n",
      "Epoch 671/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103877.4609 - val_loss: 102718.8438\n",
      "Epoch 672/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103866.8438 - val_loss: 102706.8906\n",
      "Epoch 673/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103859.4688 - val_loss: 102698.5312\n",
      "Epoch 674/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103850.2578 - val_loss: 102688.0078\n",
      "Epoch 675/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103841.6484 - val_loss: 102682.9297\n",
      "Epoch 676/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103831.7578 - val_loss: 102669.2266\n",
      "Epoch 677/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103820.3594 - val_loss: 102663.7812\n",
      "Epoch 678/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103812.5312 - val_loss: 102656.9375\n",
      "Epoch 679/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103804.1562 - val_loss: 102648.4453\n",
      "Epoch 680/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103795.0078 - val_loss: 102637.2656\n",
      "Epoch 681/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103783.2109 - val_loss: 102627.1562\n",
      "Epoch 682/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103775.6719 - val_loss: 102615.6797\n",
      "Epoch 683/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103766.5781 - val_loss: 102610.1641\n",
      "Epoch 684/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103758.2422 - val_loss: 102601.3906\n",
      "Epoch 685/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103750.0078 - val_loss: 102595.6797\n",
      "Epoch 686/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103739.2500 - val_loss: 102583.0078\n",
      "Epoch 687/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103731.0156 - val_loss: 102569.6953\n",
      "Epoch 688/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103721.5547 - val_loss: 102562.8750\n",
      "Epoch 689/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103713.6797 - val_loss: 102553.7656\n",
      "Epoch 690/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103704.0078 - val_loss: 102550.2656\n",
      "Epoch 691/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103699.5234 - val_loss: 102539.3984\n",
      "Epoch 692/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103689.5625 - val_loss: 102532.4141\n",
      "Epoch 693/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103679.1328 - val_loss: 102524.2500\n",
      "Epoch 694/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103669.0234 - val_loss: 102514.1250\n",
      "Epoch 695/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103658.6172 - val_loss: 102504.9922\n",
      "Epoch 696/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103650.2266 - val_loss: 102494.3047\n",
      "Epoch 697/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103645.0625 - val_loss: 102491.2500\n",
      "Epoch 698/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103636.8125 - val_loss: 102488.5703\n",
      "Epoch 699/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103625.2578 - val_loss: 102479.3516\n",
      "Epoch 700/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103620.1875 - val_loss: 102470.4219\n",
      "Epoch 701/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103609.0938 - val_loss: 102463.4453\n",
      "Epoch 702/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103599.0469 - val_loss: 102455.8438\n",
      "Epoch 703/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103596.6250 - val_loss: 102439.7891\n",
      "Epoch 704/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103585.7812 - val_loss: 102439.0312\n",
      "Epoch 705/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103577.8516 - val_loss: 102433.1875\n",
      "Epoch 706/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103569.1484 - val_loss: 102429.8984\n",
      "Epoch 707/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103559.9453 - val_loss: 102418.8594\n",
      "Epoch 708/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103553.9531 - val_loss: 102408.6641\n",
      "Epoch 709/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103541.4062 - val_loss: 102399.5078\n",
      "Epoch 710/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103535.6328 - val_loss: 102388.5703\n",
      "Epoch 711/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103528.9375 - val_loss: 102387.4609\n",
      "Epoch 712/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103521.8047 - val_loss: 102377.0703\n",
      "Epoch 713/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103509.5703 - val_loss: 102368.8125\n",
      "Epoch 714/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103501.1016 - val_loss: 102363.0391\n",
      "Epoch 715/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103495.9297 - val_loss: 102355.1094\n",
      "Epoch 716/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103488.1016 - val_loss: 102346.1484\n",
      "Epoch 717/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103473.8672 - val_loss: 102349.3672\n",
      "Epoch 718/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103465.0312 - val_loss: 102332.8984\n",
      "Epoch 719/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103459.5547 - val_loss: 102324.9453\n",
      "Epoch 720/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103449.7266 - val_loss: 102314.0312\n",
      "Epoch 721/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103444.1016 - val_loss: 102310.6406\n",
      "Epoch 722/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103432.6406 - val_loss: 102304.7969\n",
      "Epoch 723/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103421.5703 - val_loss: 102304.4609\n",
      "Epoch 724/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103415.9922 - val_loss: 102293.4688\n",
      "Epoch 725/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103408.4688 - val_loss: 102281.0156\n",
      "Epoch 726/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103398.1562 - val_loss: 102271.9297\n",
      "Epoch 727/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103390.1016 - val_loss: 102267.7031\n",
      "Epoch 728/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103378.6406 - val_loss: 102259.5078\n",
      "Epoch 729/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103369.5625 - val_loss: 102251.6875\n",
      "Epoch 730/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103362.1484 - val_loss: 102245.4688\n",
      "Epoch 731/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103353.3750 - val_loss: 102233.0391\n",
      "Epoch 732/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103345.7891 - val_loss: 102235.4922\n",
      "Epoch 733/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103338.8906 - val_loss: 102222.4297\n",
      "Epoch 734/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103330.3047 - val_loss: 102210.5625\n",
      "Epoch 735/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103321.5000 - val_loss: 102204.5156\n",
      "Epoch 736/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103310.7500 - val_loss: 102195.7422\n",
      "Epoch 737/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103301.2500 - val_loss: 102188.8516\n",
      "Epoch 738/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103295.7422 - val_loss: 102182.0781\n",
      "Epoch 739/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103285.7422 - val_loss: 102177.2500\n",
      "Epoch 740/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103276.3594 - val_loss: 102169.2891\n",
      "Epoch 741/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103266.7812 - val_loss: 102156.4141\n",
      "Epoch 742/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103260.4141 - val_loss: 102155.4531\n",
      "Epoch 743/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103252.9844 - val_loss: 102148.4219\n",
      "Epoch 744/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103243.8828 - val_loss: 102140.4766\n",
      "Epoch 745/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103234.2344 - val_loss: 102132.4922\n",
      "Epoch 746/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103227.9609 - val_loss: 102129.5391\n",
      "Epoch 747/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103220.0938 - val_loss: 102122.3359\n",
      "Epoch 748/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103210.2266 - val_loss: 102108.3906\n",
      "Epoch 749/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103204.4141 - val_loss: 102105.6953\n",
      "Epoch 750/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103196.0703 - val_loss: 102098.6719\n",
      "Epoch 751/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103187.2969 - val_loss: 102093.8750\n",
      "Epoch 752/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103181.4219 - val_loss: 102091.6094\n",
      "Epoch 753/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103172.2188 - val_loss: 102080.8906\n",
      "Epoch 754/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103164.2031 - val_loss: 102064.2266\n",
      "Epoch 755/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103151.3594 - val_loss: 102065.0000\n",
      "Epoch 756/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103146.6719 - val_loss: 102063.4844\n",
      "Epoch 757/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103135.8906 - val_loss: 102054.2500\n",
      "Epoch 758/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103130.9375 - val_loss: 102043.9141\n",
      "Epoch 759/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103122.0781 - val_loss: 102037.2344\n",
      "Epoch 760/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103114.7422 - val_loss: 102032.1250\n",
      "Epoch 761/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103112.1875 - val_loss: 102028.2656\n",
      "Epoch 762/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103098.2812 - val_loss: 102018.6484\n",
      "Epoch 763/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103093.2969 - val_loss: 102013.8203\n",
      "Epoch 764/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103084.7188 - val_loss: 102003.2266\n",
      "Epoch 765/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103077.9609 - val_loss: 101998.7656\n",
      "Epoch 766/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103070.0703 - val_loss: 101994.5625\n",
      "Epoch 767/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103065.1641 - val_loss: 101982.2500\n",
      "Epoch 768/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103055.1016 - val_loss: 101969.4219\n",
      "Epoch 769/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103049.4062 - val_loss: 101970.2188\n",
      "Epoch 770/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103040.6094 - val_loss: 101960.1562\n",
      "Epoch 771/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103032.8906 - val_loss: 101956.2109\n",
      "Epoch 772/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103027.4375 - val_loss: 101952.7422\n",
      "Epoch 773/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103019.2422 - val_loss: 101941.4766\n",
      "Epoch 774/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103011.3828 - val_loss: 101941.2422\n",
      "Epoch 775/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103001.6406 - val_loss: 101930.1875\n",
      "Epoch 776/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102999.5703 - val_loss: 101922.6875\n",
      "Epoch 777/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102986.0703 - val_loss: 101913.8438\n",
      "Epoch 778/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102983.4219 - val_loss: 101900.9766\n",
      "Epoch 779/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102972.5859 - val_loss: 101892.4141\n",
      "Epoch 780/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102965.9297 - val_loss: 101890.7812\n",
      "Epoch 781/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102957.1484 - val_loss: 101881.3594\n",
      "Epoch 782/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102952.0312 - val_loss: 101877.0703\n",
      "Epoch 783/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102941.3984 - val_loss: 101868.7500\n",
      "Epoch 784/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102935.8281 - val_loss: 101857.2344\n",
      "Epoch 785/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102927.5312 - val_loss: 101853.8750\n",
      "Epoch 786/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102920.3594 - val_loss: 101845.0859\n",
      "Epoch 787/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102909.9141 - val_loss: 101841.0547\n",
      "Epoch 788/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102902.2656 - val_loss: 101825.8438\n",
      "Epoch 789/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102898.1875 - val_loss: 101819.6953\n",
      "Epoch 790/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102893.6094 - val_loss: 101812.6953\n",
      "Epoch 791/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102883.2578 - val_loss: 101809.6953\n",
      "Epoch 792/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102872.1797 - val_loss: 101802.3281\n",
      "Epoch 793/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102865.5938 - val_loss: 101798.4688\n",
      "Epoch 794/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102860.4922 - val_loss: 101794.4375\n",
      "Epoch 795/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102856.1094 - val_loss: 101776.0781\n",
      "Epoch 796/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102846.8672 - val_loss: 101770.6328\n",
      "Epoch 797/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102837.8594 - val_loss: 101769.3281\n",
      "Epoch 798/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102831.6953 - val_loss: 101758.5078\n",
      "Epoch 799/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102824.9375 - val_loss: 101747.5312\n",
      "Epoch 800/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102817.3516 - val_loss: 101744.1797\n",
      "Epoch 801/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102806.2656 - val_loss: 101736.5938\n",
      "Epoch 802/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102800.1094 - val_loss: 101727.2422\n",
      "Epoch 803/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102794.1562 - val_loss: 101720.1172\n",
      "Epoch 804/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102789.6953 - val_loss: 101718.5547\n",
      "Epoch 805/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102781.1875 - val_loss: 101710.5781\n",
      "Epoch 806/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102773.0234 - val_loss: 101703.3438\n",
      "Epoch 807/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102766.5859 - val_loss: 101691.3516\n",
      "Epoch 808/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102759.9609 - val_loss: 101689.4297\n",
      "Epoch 809/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102750.4766 - val_loss: 101675.0938\n",
      "Epoch 810/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102744.3594 - val_loss: 101673.4531\n",
      "Epoch 811/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102737.0703 - val_loss: 101669.9297\n",
      "Epoch 812/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102731.0781 - val_loss: 101663.1484\n",
      "Epoch 813/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102723.9219 - val_loss: 101660.4375\n",
      "Epoch 814/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102714.8828 - val_loss: 101647.3516\n",
      "Epoch 815/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102710.6641 - val_loss: 101645.4141\n",
      "Epoch 816/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102701.8828 - val_loss: 101628.8672\n",
      "Epoch 817/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102695.6016 - val_loss: 101632.0078\n",
      "Epoch 818/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102691.0703 - val_loss: 101622.4375\n",
      "Epoch 819/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102684.1562 - val_loss: 101620.5469\n",
      "Epoch 820/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102673.9062 - val_loss: 101612.6172\n",
      "Epoch 821/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102665.9141 - val_loss: 101612.3672\n",
      "Epoch 822/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102667.0859 - val_loss: 101606.7969\n",
      "Epoch 823/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102661.8047 - val_loss: 101602.0156\n",
      "Epoch 824/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102649.6406 - val_loss: 101592.1172\n",
      "Epoch 825/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102641.0469 - val_loss: 101577.4688\n",
      "Epoch 826/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102634.6016 - val_loss: 101564.0234\n",
      "Epoch 827/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102630.8125 - val_loss: 101561.7500\n",
      "Epoch 828/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102624.1172 - val_loss: 101560.6953\n",
      "Epoch 829/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102616.5156 - val_loss: 101548.6172\n",
      "Epoch 830/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102610.4844 - val_loss: 101551.7812\n",
      "Epoch 831/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102606.1719 - val_loss: 101539.4531\n",
      "Epoch 832/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102600.4844 - val_loss: 101528.4375\n",
      "Epoch 833/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102591.6875 - val_loss: 101521.6094\n",
      "Epoch 834/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102584.8750 - val_loss: 101520.1250\n",
      "Epoch 835/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102579.4062 - val_loss: 101512.9297\n",
      "Epoch 836/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102570.9766 - val_loss: 101502.2344\n",
      "Epoch 837/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102568.5781 - val_loss: 101498.7578\n",
      "Epoch 838/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102560.0234 - val_loss: 101488.0078\n",
      "Epoch 839/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102550.8594 - val_loss: 101482.5000\n",
      "Epoch 840/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102544.3672 - val_loss: 101474.5625\n",
      "Epoch 841/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102539.0391 - val_loss: 101470.0547\n",
      "Epoch 842/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102531.4609 - val_loss: 101458.2578\n",
      "Epoch 843/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102526.5781 - val_loss: 101453.8359\n",
      "Epoch 844/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102518.0703 - val_loss: 101444.0547\n",
      "Epoch 845/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102513.3203 - val_loss: 101440.2969\n",
      "Epoch 846/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102506.6016 - val_loss: 101431.5703\n",
      "Epoch 847/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102503.1172 - val_loss: 101427.0000\n",
      "Epoch 848/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102494.3281 - val_loss: 101416.6875\n",
      "Epoch 849/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102489.8203 - val_loss: 101419.0234\n",
      "Epoch 850/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102484.6328 - val_loss: 101402.4609\n",
      "Epoch 851/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102476.9453 - val_loss: 101396.0391\n",
      "Epoch 852/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102470.1953 - val_loss: 101394.0234\n",
      "Epoch 853/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102466.1172 - val_loss: 101391.3594\n",
      "Epoch 854/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102459.1250 - val_loss: 101379.0625\n",
      "Epoch 855/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102452.8281 - val_loss: 101366.7969\n",
      "Epoch 856/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102444.5625 - val_loss: 101361.3750\n",
      "Epoch 857/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102438.8203 - val_loss: 101360.2656\n",
      "Epoch 858/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102430.5391 - val_loss: 101349.5156\n",
      "Epoch 859/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102426.8672 - val_loss: 101341.2344\n",
      "Epoch 860/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102418.5000 - val_loss: 101337.5547\n",
      "Epoch 861/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102411.1484 - val_loss: 101332.8750\n",
      "Epoch 862/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102406.4297 - val_loss: 101324.1719\n",
      "Epoch 863/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102401.5625 - val_loss: 101314.6719\n",
      "Epoch 864/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102395.4297 - val_loss: 101316.9375\n",
      "Epoch 865/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102389.1875 - val_loss: 101298.6875\n",
      "Epoch 866/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102383.6719 - val_loss: 101296.4531\n",
      "Epoch 867/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102377.2734 - val_loss: 101282.3125\n",
      "Epoch 868/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102368.1953 - val_loss: 101283.1797\n",
      "Epoch 869/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102364.6641 - val_loss: 101276.5859\n",
      "Epoch 870/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102361.1797 - val_loss: 101270.2969\n",
      "Epoch 871/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102350.5547 - val_loss: 101263.0156\n",
      "Epoch 872/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102346.5703 - val_loss: 101260.7344\n",
      "Epoch 873/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102339.1406 - val_loss: 101248.3672\n",
      "Epoch 874/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102334.2344 - val_loss: 101244.7344\n",
      "Epoch 875/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102329.0625 - val_loss: 101238.1562\n",
      "Epoch 876/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102321.3594 - val_loss: 101227.9453\n",
      "Epoch 877/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102317.4141 - val_loss: 101225.8672\n",
      "Epoch 878/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102310.6094 - val_loss: 101213.9531\n",
      "Epoch 879/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102307.6172 - val_loss: 101206.0234\n",
      "Epoch 880/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102298.5938 - val_loss: 101198.0234\n",
      "Epoch 881/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102292.2266 - val_loss: 101194.2578\n",
      "Epoch 882/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102283.6875 - val_loss: 101190.3438\n",
      "Epoch 883/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102278.5078 - val_loss: 101179.9453\n",
      "Epoch 884/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102273.6562 - val_loss: 101174.1484\n",
      "Epoch 885/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102269.4609 - val_loss: 101157.6562\n",
      "Epoch 886/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102261.7969 - val_loss: 101157.6641\n",
      "Epoch 887/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102255.0781 - val_loss: 101152.5703\n",
      "Epoch 888/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102249.7500 - val_loss: 101147.4688\n",
      "Epoch 889/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102244.0625 - val_loss: 101144.7734\n",
      "Epoch 890/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102240.5469 - val_loss: 101138.2188\n",
      "Epoch 891/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102231.3594 - val_loss: 101126.7500\n",
      "Epoch 892/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102225.3594 - val_loss: 101121.5703\n",
      "Epoch 893/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102223.3125 - val_loss: 101114.0703\n",
      "Epoch 894/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102215.0000 - val_loss: 101109.0938\n",
      "Epoch 895/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102211.6875 - val_loss: 101110.7969\n",
      "Epoch 896/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102203.9844 - val_loss: 101100.2188\n",
      "Epoch 897/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102199.9531 - val_loss: 101088.0078\n",
      "Epoch 898/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102192.7266 - val_loss: 101082.8281\n",
      "Epoch 899/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102191.7969 - val_loss: 101075.1484\n",
      "Epoch 900/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102187.1406 - val_loss: 101076.4141\n",
      "Epoch 901/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102178.8672 - val_loss: 101067.3594\n",
      "Epoch 902/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102169.7656 - val_loss: 101066.8906\n",
      "Epoch 903/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102166.7031 - val_loss: 101054.9062\n",
      "Epoch 904/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102162.7812 - val_loss: 101047.6953\n",
      "Epoch 905/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102156.5859 - val_loss: 101037.7578\n",
      "Epoch 906/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102149.3047 - val_loss: 101036.5469\n",
      "Epoch 907/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102144.2500 - val_loss: 101033.6172\n",
      "Epoch 908/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102140.7891 - val_loss: 101027.1484\n",
      "Epoch 909/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102132.4922 - val_loss: 101017.2422\n",
      "Epoch 910/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102131.5391 - val_loss: 101013.4453\n",
      "Epoch 911/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102122.7891 - val_loss: 101005.2031\n",
      "Epoch 912/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102117.7031 - val_loss: 101000.1250\n",
      "Epoch 913/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102110.9141 - val_loss: 100993.4375\n",
      "Epoch 914/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102106.0234 - val_loss: 100983.5547\n",
      "Epoch 915/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102103.9141 - val_loss: 100983.8047\n",
      "Epoch 916/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102096.3594 - val_loss: 100972.0391\n",
      "Epoch 917/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102090.1719 - val_loss: 100971.5703\n",
      "Epoch 918/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102083.3672 - val_loss: 100967.3828\n",
      "Epoch 919/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102076.7969 - val_loss: 100957.6328\n",
      "Epoch 920/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102071.3906 - val_loss: 100951.4219\n",
      "Epoch 921/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102066.3594 - val_loss: 100956.6250\n",
      "Epoch 922/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102061.0938 - val_loss: 100943.6875\n",
      "Epoch 923/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102057.1797 - val_loss: 100934.4844\n",
      "Epoch 924/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102048.8594 - val_loss: 100930.5469\n",
      "Epoch 925/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102046.7891 - val_loss: 100928.5312\n",
      "Epoch 926/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102038.8438 - val_loss: 100920.4219\n",
      "Epoch 927/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102035.6172 - val_loss: 100915.3828\n",
      "Epoch 928/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102032.1797 - val_loss: 100905.3672\n",
      "Epoch 929/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102024.0469 - val_loss: 100909.7188\n",
      "Epoch 930/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102019.4062 - val_loss: 100902.8203\n",
      "Epoch 931/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102014.1719 - val_loss: 100893.7500\n",
      "Epoch 932/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102010.7891 - val_loss: 100891.7891\n",
      "Epoch 933/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102005.8516 - val_loss: 100891.3828\n",
      "Epoch 934/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102002.4453 - val_loss: 100874.4844\n",
      "Epoch 935/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101995.0625 - val_loss: 100876.2578\n",
      "Epoch 936/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101986.7422 - val_loss: 100873.7734\n",
      "Epoch 937/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101982.9922 - val_loss: 100864.0078\n",
      "Epoch 938/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101978.1250 - val_loss: 100862.1328\n",
      "Epoch 939/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101972.2188 - val_loss: 100866.0000\n",
      "Epoch 940/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101966.3984 - val_loss: 100857.6797\n",
      "Epoch 941/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101964.3359 - val_loss: 100855.2656\n",
      "Epoch 942/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101959.6562 - val_loss: 100837.4609\n",
      "Epoch 943/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101954.8750 - val_loss: 100833.8828\n",
      "Epoch 944/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101947.6094 - val_loss: 100833.4844\n",
      "Epoch 945/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101944.0859 - val_loss: 100827.2266\n",
      "Epoch 946/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101937.6328 - val_loss: 100820.4922\n",
      "Epoch 947/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101933.1641 - val_loss: 100821.0625\n",
      "Epoch 948/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101929.5000 - val_loss: 100819.0859\n",
      "Epoch 949/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101925.6562 - val_loss: 100812.8828\n",
      "Epoch 950/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101919.4297 - val_loss: 100805.8359\n",
      "Epoch 951/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101917.5156 - val_loss: 100800.7266\n",
      "Epoch 952/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101911.8594 - val_loss: 100794.4219\n",
      "Epoch 953/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101903.0625 - val_loss: 100791.1328\n",
      "Epoch 954/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101899.9609 - val_loss: 100785.2109\n",
      "Epoch 955/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101897.4297 - val_loss: 100784.1250\n",
      "Epoch 956/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101893.0078 - val_loss: 100783.8828\n",
      "Epoch 957/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101884.3359 - val_loss: 100776.0469\n",
      "Epoch 958/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101882.2969 - val_loss: 100767.0156\n",
      "Epoch 959/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101878.5469 - val_loss: 100766.3281\n",
      "Epoch 960/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101875.7969 - val_loss: 100764.9531\n",
      "Epoch 961/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101867.0156 - val_loss: 100758.2109\n",
      "Epoch 962/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101863.1641 - val_loss: 100758.3828\n",
      "Epoch 963/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101861.2969 - val_loss: 100752.8984\n",
      "Epoch 964/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101859.0938 - val_loss: 100749.9375\n",
      "Epoch 965/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101854.7969 - val_loss: 100740.6406\n",
      "Epoch 966/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101848.8906 - val_loss: 100737.2969\n",
      "Epoch 967/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101845.4688 - val_loss: 100736.5156\n",
      "Epoch 968/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101837.5156 - val_loss: 100729.8516\n",
      "Epoch 969/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101834.3203 - val_loss: 100728.3047\n",
      "Epoch 970/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101832.4766 - val_loss: 100718.1406\n",
      "Epoch 971/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101829.5391 - val_loss: 100719.9297\n",
      "Epoch 972/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101822.4062 - val_loss: 100710.8281\n",
      "Epoch 973/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101820.7578 - val_loss: 100711.8359\n",
      "Epoch 974/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101815.4688 - val_loss: 100705.2812\n",
      "Epoch 975/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101808.3984 - val_loss: 100704.0859\n",
      "Epoch 976/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101803.4141 - val_loss: 100698.8047\n",
      "Epoch 977/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101800.7266 - val_loss: 100697.2656\n",
      "Epoch 978/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101796.3984 - val_loss: 100693.2188\n",
      "Epoch 979/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101792.8828 - val_loss: 100685.4453\n",
      "Epoch 980/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101788.1641 - val_loss: 100683.5469\n",
      "Epoch 981/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101786.8125 - val_loss: 100678.9219\n",
      "Epoch 982/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101779.3984 - val_loss: 100674.3125\n",
      "Epoch 983/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101777.2812 - val_loss: 100670.5078\n",
      "Epoch 984/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101770.7188 - val_loss: 100659.6016\n",
      "Epoch 985/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101768.0156 - val_loss: 100653.4219\n",
      "Epoch 986/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101760.2266 - val_loss: 100651.1406\n",
      "Epoch 987/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101757.6172 - val_loss: 100649.7109\n",
      "Epoch 988/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101753.1016 - val_loss: 100645.5078\n",
      "Epoch 989/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101751.7500 - val_loss: 100641.1875\n",
      "Epoch 990/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101744.2188 - val_loss: 100637.3828\n",
      "Epoch 991/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101741.4688 - val_loss: 100632.8516\n",
      "Epoch 992/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101739.1484 - val_loss: 100625.5625\n",
      "Epoch 993/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101731.7188 - val_loss: 100629.5703\n",
      "Epoch 994/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101730.9766 - val_loss: 100619.4688\n",
      "Epoch 995/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101726.6953 - val_loss: 100620.1016\n",
      "Epoch 996/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101722.3203 - val_loss: 100615.9141\n",
      "Epoch 997/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101714.4219 - val_loss: 100610.9219\n",
      "Epoch 998/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101711.9688 - val_loss: 100605.8984\n",
      "Epoch 999/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101710.1797 - val_loss: 100598.5156\n",
      "Epoch 1000/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101701.9609 - val_loss: 100595.2266\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 101676.3984\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_log = model.fit(X_train, y_train, epochs=1000, batch_size=100, validation_split=.2, verbose=1)\n",
    "model.evaluate(X_test, y_test)\n",
    "plot_loss(train_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Make a simple neural network for predicting the price of homes in California. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  \n",
       "0    -122.23  \n",
       "1    -122.22  \n",
       "2    -122.24  \n",
       "3    -122.25  \n",
       "4    -122.25  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "cal = fetch_california_housing(as_frame=True)\n",
    "Xcal = pd.DataFrame(cal.data)\n",
    "ycal = pd.DataFrame(cal.target)\n",
    "Xcal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 8) (20640, 1)\n"
     ]
    }
   ],
   "source": [
    "y = np.array(ycal).reshape(-1,1)\n",
    "X = np.array(Xcal)\n",
    "print(X.shape, y.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization_5 (Normalizat  (None, 8)                17        \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 18)                162       \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 1)                 19        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 198\n",
      "Trainable params: 181\n",
      "Non-trainable params: 17\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "normalizer.adapt(np.array(X_train))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(normalizer)\n",
    "model.add(Dense(18, input_shape=(18,), activation='relu'))\n",
    "#model.add(Dense(18, input_dim=18, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_absolute_error', optimizer=tf.optimizers.Adam(learning_rate=.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "124/124 [==============================] - 1s 2ms/step - loss: 0.6738 - val_loss: 0.4829\n",
      "Epoch 2/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.4586 - val_loss: 0.4589\n",
      "Epoch 3/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.4341 - val_loss: 0.4421\n",
      "Epoch 4/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.4247 - val_loss: 0.4361\n",
      "Epoch 5/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.4149 - val_loss: 0.4364\n",
      "Epoch 6/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.4151 - val_loss: 0.4262\n",
      "Epoch 7/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.4111 - val_loss: 0.4351\n",
      "Epoch 8/1000\n",
      "124/124 [==============================] - 0s 2ms/step - loss: 0.4063 - val_loss: 0.4180\n",
      "Epoch 9/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.4012 - val_loss: 0.4284\n",
      "Epoch 10/1000\n",
      "124/124 [==============================] - 0s 2ms/step - loss: 0.3996 - val_loss: 0.4114\n",
      "Epoch 11/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3906 - val_loss: 0.4129\n",
      "Epoch 12/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3919 - val_loss: 0.4171\n",
      "Epoch 13/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3921 - val_loss: 0.4135\n",
      "Epoch 14/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3876 - val_loss: 0.4015\n",
      "Epoch 15/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3905 - val_loss: 0.4118\n",
      "Epoch 16/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3905 - val_loss: 0.4144\n",
      "Epoch 17/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3876 - val_loss: 0.4053\n",
      "Epoch 18/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3827 - val_loss: 0.4059\n",
      "Epoch 19/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3855 - val_loss: 0.4078\n",
      "Epoch 20/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3849 - val_loss: 0.4013\n",
      "Epoch 21/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3828 - val_loss: 0.4000\n",
      "Epoch 22/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3808 - val_loss: 0.4075\n",
      "Epoch 23/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3827 - val_loss: 0.4032\n",
      "Epoch 24/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3804 - val_loss: 0.4074\n",
      "Epoch 25/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3790 - val_loss: 0.4063\n",
      "Epoch 26/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3813 - val_loss: 0.4017\n",
      "Epoch 27/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3797 - val_loss: 0.3960\n",
      "Epoch 28/1000\n",
      "124/124 [==============================] - 0s 2ms/step - loss: 0.3823 - val_loss: 0.4011\n",
      "Epoch 29/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3789 - val_loss: 0.4046\n",
      "Epoch 30/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3772 - val_loss: 0.3989\n",
      "Epoch 31/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3788 - val_loss: 0.4014\n",
      "Epoch 32/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3753 - val_loss: 0.4022\n",
      "Epoch 33/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3764 - val_loss: 0.3941\n",
      "Epoch 34/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3776 - val_loss: 0.3963\n",
      "Epoch 35/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3753 - val_loss: 0.3971\n",
      "Epoch 36/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3751 - val_loss: 0.3883\n",
      "Epoch 37/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3718 - val_loss: 0.3912\n",
      "Epoch 38/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3752 - val_loss: 0.4065\n",
      "Epoch 39/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3764 - val_loss: 0.3911\n",
      "Epoch 40/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3750 - val_loss: 0.3895\n",
      "Epoch 41/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3732 - val_loss: 0.4045\n",
      "Epoch 42/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3758 - val_loss: 0.3896\n",
      "Epoch 43/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3726 - val_loss: 0.3945\n",
      "Epoch 44/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3745 - val_loss: 0.3899\n",
      "Epoch 45/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3737 - val_loss: 0.3957\n",
      "Epoch 46/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3717 - val_loss: 0.3994\n",
      "Epoch 47/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3698 - val_loss: 0.3877\n",
      "Epoch 48/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3732 - val_loss: 0.3946\n",
      "Epoch 49/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3725 - val_loss: 0.3872\n",
      "Epoch 50/1000\n",
      "124/124 [==============================] - 0s 2ms/step - loss: 0.3704 - val_loss: 0.3980\n",
      "Epoch 51/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3720 - val_loss: 0.3996\n",
      "Epoch 52/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3712 - val_loss: 0.3914\n",
      "Epoch 53/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3696 - val_loss: 0.3848\n",
      "Epoch 54/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3702 - val_loss: 0.3933\n",
      "Epoch 55/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3708 - val_loss: 0.3895\n",
      "Epoch 56/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3679 - val_loss: 0.3858\n",
      "Epoch 57/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3673 - val_loss: 0.3821\n",
      "Epoch 58/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3683 - val_loss: 0.3850\n",
      "Epoch 59/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3689 - val_loss: 0.3904\n",
      "Epoch 60/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3698 - val_loss: 0.3907\n",
      "Epoch 61/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3738 - val_loss: 0.3867\n",
      "Epoch 62/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3667 - val_loss: 0.3860\n",
      "Epoch 63/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3669 - val_loss: 0.3837\n",
      "Epoch 64/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3674 - val_loss: 0.3813\n",
      "Epoch 65/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3678 - val_loss: 0.3800\n",
      "Epoch 66/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3656 - val_loss: 0.3877\n",
      "Epoch 67/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3684 - val_loss: 0.3824\n",
      "Epoch 68/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3645 - val_loss: 0.3921\n",
      "Epoch 69/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3634 - val_loss: 0.3839\n",
      "Epoch 70/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3684 - val_loss: 0.3844\n",
      "Epoch 71/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3637 - val_loss: 0.3840\n",
      "Epoch 72/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3624 - val_loss: 0.3864\n",
      "Epoch 73/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3631 - val_loss: 0.3794\n",
      "Epoch 74/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3639 - val_loss: 0.3995\n",
      "Epoch 75/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3636 - val_loss: 0.3833\n",
      "Epoch 76/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3635 - val_loss: 0.3788\n",
      "Epoch 77/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3624 - val_loss: 0.3805\n",
      "Epoch 78/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3616 - val_loss: 0.3911\n",
      "Epoch 79/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3605 - val_loss: 0.3812\n",
      "Epoch 80/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3651 - val_loss: 0.3816\n",
      "Epoch 81/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3609 - val_loss: 0.3848\n",
      "Epoch 82/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3617 - val_loss: 0.3772\n",
      "Epoch 83/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3624 - val_loss: 0.3885\n",
      "Epoch 84/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3602 - val_loss: 0.3768\n",
      "Epoch 85/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3604 - val_loss: 0.3905\n",
      "Epoch 86/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3584 - val_loss: 0.3798\n",
      "Epoch 87/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3623 - val_loss: 0.3806\n",
      "Epoch 88/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3599 - val_loss: 0.3801\n",
      "Epoch 89/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3593 - val_loss: 0.3827\n",
      "Epoch 90/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3575 - val_loss: 0.3836\n",
      "Epoch 91/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3648 - val_loss: 0.3811\n",
      "Epoch 92/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3571 - val_loss: 0.3827\n",
      "Epoch 93/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3593 - val_loss: 0.3792\n",
      "Epoch 94/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3576 - val_loss: 0.3734\n",
      "Epoch 95/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3592 - val_loss: 0.3727\n",
      "Epoch 96/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3589 - val_loss: 0.3783\n",
      "Epoch 97/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3603 - val_loss: 0.3857\n",
      "Epoch 98/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3581 - val_loss: 0.3718\n",
      "Epoch 99/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3562 - val_loss: 0.3723\n",
      "Epoch 100/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3578 - val_loss: 0.3840\n",
      "Epoch 101/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3586 - val_loss: 0.3805\n",
      "Epoch 102/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3587 - val_loss: 0.3729\n",
      "Epoch 103/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3619 - val_loss: 0.3725\n",
      "Epoch 104/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3595 - val_loss: 0.3729\n",
      "Epoch 105/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3572 - val_loss: 0.3729\n",
      "Epoch 106/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3556 - val_loss: 0.3665\n",
      "Epoch 107/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3586 - val_loss: 0.3760\n",
      "Epoch 108/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3560 - val_loss: 0.3791\n",
      "Epoch 109/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3569 - val_loss: 0.3845\n",
      "Epoch 110/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3570 - val_loss: 0.3726\n",
      "Epoch 111/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3570 - val_loss: 0.3693\n",
      "Epoch 112/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3586 - val_loss: 0.3797\n",
      "Epoch 113/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3581 - val_loss: 0.3703\n",
      "Epoch 114/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3560 - val_loss: 0.3711\n",
      "Epoch 115/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3540 - val_loss: 0.3711\n",
      "Epoch 116/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3582 - val_loss: 0.3792\n",
      "Epoch 117/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3560 - val_loss: 0.3680\n",
      "Epoch 118/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3584 - val_loss: 0.3675\n",
      "Epoch 119/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3571 - val_loss: 0.3710\n",
      "Epoch 120/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3570 - val_loss: 0.3781\n",
      "Epoch 121/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3555 - val_loss: 0.3815\n",
      "Epoch 122/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3563 - val_loss: 0.3709\n",
      "Epoch 123/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3574 - val_loss: 0.3653\n",
      "Epoch 124/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3548 - val_loss: 0.3789\n",
      "Epoch 125/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3552 - val_loss: 0.3786\n",
      "Epoch 126/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3555 - val_loss: 0.3704\n",
      "Epoch 127/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3553 - val_loss: 0.3689\n",
      "Epoch 128/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3571 - val_loss: 0.3739\n",
      "Epoch 129/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3528 - val_loss: 0.3678\n",
      "Epoch 130/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3543 - val_loss: 0.3822\n",
      "Epoch 131/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3553 - val_loss: 0.3752\n",
      "Epoch 132/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3547 - val_loss: 0.3687\n",
      "Epoch 133/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3546 - val_loss: 0.3861\n",
      "Epoch 134/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3565 - val_loss: 0.3655\n",
      "Epoch 135/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3575 - val_loss: 0.3761\n",
      "Epoch 136/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3581 - val_loss: 0.3711\n",
      "Epoch 137/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3533 - val_loss: 0.3715\n",
      "Epoch 138/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3534 - val_loss: 0.3713\n",
      "Epoch 139/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3551 - val_loss: 0.3686\n",
      "Epoch 140/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3559 - val_loss: 0.3776\n",
      "Epoch 141/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3559 - val_loss: 0.3701\n",
      "Epoch 142/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3547 - val_loss: 0.3697\n",
      "Epoch 143/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3553 - val_loss: 0.3797\n",
      "Epoch 144/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3575 - val_loss: 0.3664\n",
      "Epoch 145/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3539 - val_loss: 0.3652\n",
      "Epoch 146/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3531 - val_loss: 0.3698\n",
      "Epoch 147/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3532 - val_loss: 0.3748\n",
      "Epoch 148/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3540 - val_loss: 0.3667\n",
      "Epoch 149/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3529 - val_loss: 0.3653\n",
      "Epoch 150/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3559 - val_loss: 0.3744\n",
      "Epoch 151/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3529 - val_loss: 0.3689\n",
      "Epoch 152/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3524 - val_loss: 0.3703\n",
      "Epoch 153/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3567 - val_loss: 0.3679\n",
      "Epoch 154/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3536 - val_loss: 0.3820\n",
      "Epoch 155/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3540 - val_loss: 0.3725\n",
      "Epoch 156/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3547 - val_loss: 0.3669\n",
      "Epoch 157/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3541 - val_loss: 0.3719\n",
      "Epoch 158/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3537 - val_loss: 0.3760\n",
      "Epoch 159/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3539 - val_loss: 0.3675\n",
      "Epoch 160/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3537 - val_loss: 0.3737\n",
      "Epoch 161/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3577 - val_loss: 0.3603\n",
      "Epoch 162/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3514 - val_loss: 0.3700\n",
      "Epoch 163/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3578 - val_loss: 0.3639\n",
      "Epoch 164/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3561 - val_loss: 0.3837\n",
      "Epoch 165/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3558 - val_loss: 0.3704\n",
      "Epoch 166/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3588 - val_loss: 0.3698\n",
      "Epoch 167/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3569 - val_loss: 0.3629\n",
      "Epoch 168/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3528 - val_loss: 0.3671\n",
      "Epoch 169/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3536 - val_loss: 0.3664\n",
      "Epoch 170/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3511 - val_loss: 0.3668\n",
      "Epoch 171/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3547 - val_loss: 0.3732\n",
      "Epoch 172/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3531 - val_loss: 0.3796\n",
      "Epoch 173/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3545 - val_loss: 0.3668\n",
      "Epoch 174/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3524 - val_loss: 0.3692\n",
      "Epoch 175/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3570 - val_loss: 0.3677\n",
      "Epoch 176/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3535 - val_loss: 0.3831\n",
      "Epoch 177/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3551 - val_loss: 0.3639\n",
      "Epoch 178/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3527 - val_loss: 0.3730\n",
      "Epoch 179/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3550 - val_loss: 0.3763\n",
      "Epoch 180/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3530 - val_loss: 0.3761\n",
      "Epoch 181/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3574 - val_loss: 0.3802\n",
      "Epoch 182/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3533 - val_loss: 0.3676\n",
      "Epoch 183/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3515 - val_loss: 0.3663\n",
      "Epoch 184/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3529 - val_loss: 0.3723\n",
      "Epoch 185/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3523 - val_loss: 0.3674\n",
      "Epoch 186/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3512 - val_loss: 0.3797\n",
      "Epoch 187/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3550 - val_loss: 0.3695\n",
      "Epoch 188/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3531 - val_loss: 0.3679\n",
      "Epoch 189/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3538 - val_loss: 0.3602\n",
      "Epoch 190/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3513 - val_loss: 0.3669\n",
      "Epoch 191/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3522 - val_loss: 0.3682\n",
      "Epoch 192/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3507 - val_loss: 0.3776\n",
      "Epoch 193/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3549 - val_loss: 0.3676\n",
      "Epoch 194/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3514 - val_loss: 0.3715\n",
      "Epoch 195/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3553 - val_loss: 0.3610\n",
      "Epoch 196/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3525 - val_loss: 0.3643\n",
      "Epoch 197/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3515 - val_loss: 0.3675\n",
      "Epoch 198/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3505 - val_loss: 0.3710\n",
      "Epoch 199/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3527 - val_loss: 0.3659\n",
      "Epoch 200/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3485 - val_loss: 0.3693\n",
      "Epoch 201/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3562 - val_loss: 0.3658\n",
      "Epoch 202/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3530 - val_loss: 0.3619\n",
      "Epoch 203/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3516 - val_loss: 0.3711\n",
      "Epoch 204/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3536 - val_loss: 0.3628\n",
      "Epoch 205/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3501 - val_loss: 0.3616\n",
      "Epoch 206/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3512 - val_loss: 0.3618\n",
      "Epoch 207/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3524 - val_loss: 0.3838\n",
      "Epoch 208/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3516 - val_loss: 0.3576\n",
      "Epoch 209/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3507 - val_loss: 0.3692\n",
      "Epoch 210/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3503 - val_loss: 0.3621\n",
      "Epoch 211/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3521 - val_loss: 0.3620\n",
      "Epoch 212/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3530 - val_loss: 0.3777\n",
      "Epoch 213/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3514 - val_loss: 0.3728\n",
      "Epoch 214/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3511 - val_loss: 0.3644\n",
      "Epoch 215/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3513 - val_loss: 0.3714\n",
      "Epoch 216/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3548 - val_loss: 0.3689\n",
      "Epoch 217/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3503 - val_loss: 0.3658\n",
      "Epoch 218/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3497 - val_loss: 0.3624\n",
      "Epoch 219/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3496 - val_loss: 0.3702\n",
      "Epoch 220/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3505 - val_loss: 0.3640\n",
      "Epoch 221/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3516 - val_loss: 0.3750\n",
      "Epoch 222/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3504 - val_loss: 0.3576\n",
      "Epoch 223/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3511 - val_loss: 0.3603\n",
      "Epoch 224/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3498 - val_loss: 0.3650\n",
      "Epoch 225/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3504 - val_loss: 0.3667\n",
      "Epoch 226/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3500 - val_loss: 0.3652\n",
      "Epoch 227/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3522 - val_loss: 0.3784\n",
      "Epoch 228/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3504 - val_loss: 0.3696\n",
      "Epoch 229/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3513 - val_loss: 0.3617\n",
      "Epoch 230/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3503 - val_loss: 0.3689\n",
      "Epoch 231/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3517 - val_loss: 0.3624\n",
      "Epoch 232/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3492 - val_loss: 0.3672\n",
      "Epoch 233/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3501 - val_loss: 0.3730\n",
      "Epoch 234/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3500 - val_loss: 0.3658\n",
      "Epoch 235/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3527 - val_loss: 0.3693\n",
      "Epoch 236/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3512 - val_loss: 0.3604\n",
      "Epoch 237/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3512 - val_loss: 0.3688\n",
      "Epoch 238/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3537 - val_loss: 0.3620\n",
      "Epoch 239/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3506 - val_loss: 0.3692\n",
      "Epoch 240/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3505 - val_loss: 0.3677\n",
      "Epoch 241/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3503 - val_loss: 0.3623\n",
      "Epoch 242/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3545 - val_loss: 0.3598\n",
      "Epoch 243/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3483 - val_loss: 0.3717\n",
      "Epoch 244/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3499 - val_loss: 0.3678\n",
      "Epoch 245/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3496 - val_loss: 0.3640\n",
      "Epoch 246/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3503 - val_loss: 0.3638\n",
      "Epoch 247/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3536 - val_loss: 0.3843\n",
      "Epoch 248/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3512 - val_loss: 0.3641\n",
      "Epoch 249/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3514 - val_loss: 0.3631\n",
      "Epoch 250/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3500 - val_loss: 0.3733\n",
      "Epoch 251/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3528 - val_loss: 0.3632\n",
      "Epoch 252/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3493 - val_loss: 0.3705\n",
      "Epoch 253/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3521 - val_loss: 0.3618\n",
      "Epoch 254/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3494 - val_loss: 0.3635\n",
      "Epoch 255/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3527 - val_loss: 0.3610\n",
      "Epoch 256/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3488 - val_loss: 0.3743\n",
      "Epoch 257/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3508 - val_loss: 0.3667\n",
      "Epoch 258/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3499 - val_loss: 0.3600\n",
      "Epoch 259/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3499 - val_loss: 0.3631\n",
      "Epoch 260/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3518 - val_loss: 0.3631\n",
      "Epoch 261/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3528 - val_loss: 0.3584\n",
      "Epoch 262/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3492 - val_loss: 0.3708\n",
      "Epoch 263/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3501 - val_loss: 0.3640\n",
      "Epoch 264/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3496 - val_loss: 0.3628\n",
      "Epoch 265/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3510 - val_loss: 0.3687\n",
      "Epoch 266/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3486 - val_loss: 0.3816\n",
      "Epoch 267/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3513 - val_loss: 0.3650\n",
      "Epoch 268/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3492 - val_loss: 0.3623\n",
      "Epoch 269/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3498 - val_loss: 0.3583\n",
      "Epoch 270/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3487 - val_loss: 0.3601\n",
      "Epoch 271/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3503 - val_loss: 0.3612\n",
      "Epoch 272/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3477 - val_loss: 0.3626\n",
      "Epoch 273/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3511 - val_loss: 0.3663\n",
      "Epoch 274/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3509 - val_loss: 0.3657\n",
      "Epoch 275/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3510 - val_loss: 0.3624\n",
      "Epoch 276/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3500 - val_loss: 0.3575\n",
      "Epoch 277/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3499 - val_loss: 0.3695\n",
      "Epoch 278/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3507 - val_loss: 0.3648\n",
      "Epoch 279/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3527 - val_loss: 0.3641\n",
      "Epoch 280/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3499 - val_loss: 0.3655\n",
      "Epoch 281/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3506 - val_loss: 0.3676\n",
      "Epoch 282/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3577 - val_loss: 0.3707\n",
      "Epoch 283/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3487 - val_loss: 0.3670\n",
      "Epoch 284/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3484 - val_loss: 0.3693\n",
      "Epoch 285/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3489 - val_loss: 0.3661\n",
      "Epoch 286/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3499 - val_loss: 0.3660\n",
      "Epoch 287/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3519 - val_loss: 0.3691\n",
      "Epoch 288/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3475 - val_loss: 0.3618\n",
      "Epoch 289/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3488 - val_loss: 0.3626\n",
      "Epoch 290/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3529 - val_loss: 0.3704\n",
      "Epoch 291/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3482 - val_loss: 0.3631\n",
      "Epoch 292/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3474 - val_loss: 0.3687\n",
      "Epoch 293/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3494 - val_loss: 0.3658\n",
      "Epoch 294/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3471 - val_loss: 0.3665\n",
      "Epoch 295/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3499 - val_loss: 0.3666\n",
      "Epoch 296/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3523 - val_loss: 0.3657\n",
      "Epoch 297/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3512 - val_loss: 0.3662\n",
      "Epoch 298/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3533 - val_loss: 0.3647\n",
      "Epoch 299/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3508 - val_loss: 0.3594\n",
      "Epoch 300/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3467 - val_loss: 0.3659\n",
      "Epoch 301/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3491 - val_loss: 0.3650\n",
      "Epoch 302/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3510 - val_loss: 0.3676\n",
      "Epoch 303/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3491 - val_loss: 0.3634\n",
      "Epoch 304/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3497 - val_loss: 0.3666\n",
      "Epoch 305/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3514 - val_loss: 0.3631\n",
      "Epoch 306/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3511 - val_loss: 0.3675\n",
      "Epoch 307/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3504 - val_loss: 0.3605\n",
      "Epoch 308/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3494 - val_loss: 0.3576\n",
      "Epoch 309/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3487 - val_loss: 0.3696\n",
      "Epoch 310/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3519 - val_loss: 0.3646\n",
      "Epoch 311/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3491 - val_loss: 0.3635\n",
      "Epoch 312/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3488 - val_loss: 0.3636\n",
      "Epoch 313/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3502 - val_loss: 0.3670\n",
      "Epoch 314/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3495 - val_loss: 0.3615\n",
      "Epoch 315/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3479 - val_loss: 0.3597\n",
      "Epoch 316/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3493 - val_loss: 0.3698\n",
      "Epoch 317/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3491 - val_loss: 0.3591\n",
      "Epoch 318/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3489 - val_loss: 0.3602\n",
      "Epoch 319/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3476 - val_loss: 0.3654\n",
      "Epoch 320/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3512 - val_loss: 0.3660\n",
      "Epoch 321/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3484 - val_loss: 0.3749\n",
      "Epoch 322/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3536 - val_loss: 0.3583\n",
      "Epoch 323/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3499 - val_loss: 0.3629\n",
      "Epoch 324/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3511 - val_loss: 0.3714\n",
      "Epoch 325/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3466 - val_loss: 0.3582\n",
      "Epoch 326/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3488 - val_loss: 0.3676\n",
      "Epoch 327/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3491 - val_loss: 0.3730\n",
      "Epoch 328/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3488 - val_loss: 0.3633\n",
      "Epoch 329/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3492 - val_loss: 0.3623\n",
      "Epoch 330/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3508 - val_loss: 0.3729\n",
      "Epoch 331/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3484 - val_loss: 0.3634\n",
      "Epoch 332/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3483 - val_loss: 0.3757\n",
      "Epoch 333/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3489 - val_loss: 0.3654\n",
      "Epoch 334/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3483 - val_loss: 0.3707\n",
      "Epoch 335/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3488 - val_loss: 0.3588\n",
      "Epoch 336/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3484 - val_loss: 0.3693\n",
      "Epoch 337/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3488 - val_loss: 0.3653\n",
      "Epoch 338/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3522 - val_loss: 0.3714\n",
      "Epoch 339/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3478 - val_loss: 0.3603\n",
      "Epoch 340/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3516 - val_loss: 0.3620\n",
      "Epoch 341/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3499 - val_loss: 0.3606\n",
      "Epoch 342/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3480 - val_loss: 0.3612\n",
      "Epoch 343/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3479 - val_loss: 0.3574\n",
      "Epoch 344/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3477 - val_loss: 0.3630\n",
      "Epoch 345/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3477 - val_loss: 0.3562\n",
      "Epoch 346/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3470 - val_loss: 0.3648\n",
      "Epoch 347/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3488 - val_loss: 0.3704\n",
      "Epoch 348/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3517 - val_loss: 0.3715\n",
      "Epoch 349/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3479 - val_loss: 0.3718\n",
      "Epoch 350/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3482 - val_loss: 0.3612\n",
      "Epoch 351/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3484 - val_loss: 0.3578\n",
      "Epoch 352/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3473 - val_loss: 0.3567\n",
      "Epoch 353/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3487 - val_loss: 0.3656\n",
      "Epoch 354/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3504 - val_loss: 0.3606\n",
      "Epoch 355/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3474 - val_loss: 0.3635\n",
      "Epoch 356/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3510 - val_loss: 0.3786\n",
      "Epoch 357/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3489 - val_loss: 0.3595\n",
      "Epoch 358/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3476 - val_loss: 0.3563\n",
      "Epoch 359/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3476 - val_loss: 0.3661\n",
      "Epoch 360/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3498 - val_loss: 0.3680\n",
      "Epoch 361/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3479 - val_loss: 0.3687\n",
      "Epoch 362/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3485 - val_loss: 0.3569\n",
      "Epoch 363/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3475 - val_loss: 0.3626\n",
      "Epoch 364/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3459 - val_loss: 0.3680\n",
      "Epoch 365/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3467 - val_loss: 0.3575\n",
      "Epoch 366/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3494 - val_loss: 0.3643\n",
      "Epoch 367/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3501 - val_loss: 0.3719\n",
      "Epoch 368/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3498 - val_loss: 0.3639\n",
      "Epoch 369/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3478 - val_loss: 0.3591\n",
      "Epoch 370/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3464 - val_loss: 0.3592\n",
      "Epoch 371/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3476 - val_loss: 0.3666\n",
      "Epoch 372/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3489 - val_loss: 0.3589\n",
      "Epoch 373/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3477 - val_loss: 0.3673\n",
      "Epoch 374/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3474 - val_loss: 0.3723\n",
      "Epoch 375/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3470 - val_loss: 0.3687\n",
      "Epoch 376/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3513 - val_loss: 0.3585\n",
      "Epoch 377/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3506 - val_loss: 0.3810\n",
      "Epoch 378/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3502 - val_loss: 0.3577\n",
      "Epoch 379/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3484 - val_loss: 0.3658\n",
      "Epoch 380/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3523 - val_loss: 0.3626\n",
      "Epoch 381/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3475 - val_loss: 0.3583\n",
      "Epoch 382/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3488 - val_loss: 0.3630\n",
      "Epoch 383/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3479 - val_loss: 0.3556\n",
      "Epoch 384/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3510 - val_loss: 0.3633\n",
      "Epoch 385/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3484 - val_loss: 0.3616\n",
      "Epoch 386/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3466 - val_loss: 0.3582\n",
      "Epoch 387/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3475 - val_loss: 0.3637\n",
      "Epoch 388/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3482 - val_loss: 0.3674\n",
      "Epoch 389/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3478 - val_loss: 0.3596\n",
      "Epoch 390/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3480 - val_loss: 0.3619\n",
      "Epoch 391/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3476 - val_loss: 0.3623\n",
      "Epoch 392/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3471 - val_loss: 0.3647\n",
      "Epoch 393/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3489 - val_loss: 0.3649\n",
      "Epoch 394/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3483 - val_loss: 0.3727\n",
      "Epoch 395/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3519 - val_loss: 0.3649\n",
      "Epoch 396/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3485 - val_loss: 0.3649\n",
      "Epoch 397/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3475 - val_loss: 0.3594\n",
      "Epoch 398/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3479 - val_loss: 0.3649\n",
      "Epoch 399/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3493 - val_loss: 0.3599\n",
      "Epoch 400/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3466 - val_loss: 0.3580\n",
      "Epoch 401/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3472 - val_loss: 0.3628\n",
      "Epoch 402/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3525 - val_loss: 0.3662\n",
      "Epoch 403/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3478 - val_loss: 0.3638\n",
      "Epoch 404/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3499 - val_loss: 0.3614\n",
      "Epoch 405/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3460 - val_loss: 0.3562\n",
      "Epoch 406/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3486 - val_loss: 0.3616\n",
      "Epoch 407/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3474 - val_loss: 0.3588\n",
      "Epoch 408/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3496 - val_loss: 0.3585\n",
      "Epoch 409/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3488 - val_loss: 0.3599\n",
      "Epoch 410/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3474 - val_loss: 0.3586\n",
      "Epoch 411/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3506 - val_loss: 0.3645\n",
      "Epoch 412/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3476 - val_loss: 0.3747\n",
      "Epoch 413/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3491 - val_loss: 0.3580\n",
      "Epoch 414/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3494 - val_loss: 0.3663\n",
      "Epoch 415/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3474 - val_loss: 0.3643\n",
      "Epoch 416/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3493 - val_loss: 0.3606\n",
      "Epoch 417/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3471 - val_loss: 0.3541\n",
      "Epoch 418/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3464 - val_loss: 0.3602\n",
      "Epoch 419/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3489 - val_loss: 0.3605\n",
      "Epoch 420/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3465 - val_loss: 0.3680\n",
      "Epoch 421/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3479 - val_loss: 0.3676\n",
      "Epoch 422/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3491 - val_loss: 0.3662\n",
      "Epoch 423/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3477 - val_loss: 0.3607\n",
      "Epoch 424/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3495 - val_loss: 0.3620\n",
      "Epoch 425/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3482 - val_loss: 0.3623\n",
      "Epoch 426/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3459 - val_loss: 0.3648\n",
      "Epoch 427/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3474 - val_loss: 0.3593\n",
      "Epoch 428/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3492 - val_loss: 0.3587\n",
      "Epoch 429/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3488 - val_loss: 0.3622\n",
      "Epoch 430/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3466 - val_loss: 0.3663\n",
      "Epoch 431/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3449 - val_loss: 0.3620\n",
      "Epoch 432/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3485 - val_loss: 0.3714\n",
      "Epoch 433/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3475 - val_loss: 0.3598\n",
      "Epoch 434/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3478 - val_loss: 0.3593\n",
      "Epoch 435/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3483 - val_loss: 0.3527\n",
      "Epoch 436/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3461 - val_loss: 0.3574\n",
      "Epoch 437/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3478 - val_loss: 0.3614\n",
      "Epoch 438/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3477 - val_loss: 0.3623\n",
      "Epoch 439/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3464 - val_loss: 0.3625\n",
      "Epoch 440/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3475 - val_loss: 0.3603\n",
      "Epoch 441/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3466 - val_loss: 0.3580\n",
      "Epoch 442/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3493 - val_loss: 0.3604\n",
      "Epoch 443/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3484 - val_loss: 0.3707\n",
      "Epoch 444/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3494 - val_loss: 0.3599\n",
      "Epoch 445/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3479 - val_loss: 0.3730\n",
      "Epoch 446/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3491 - val_loss: 0.3612\n",
      "Epoch 447/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3460 - val_loss: 0.3633\n",
      "Epoch 448/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3498 - val_loss: 0.3670\n",
      "Epoch 449/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3447 - val_loss: 0.3604\n",
      "Epoch 450/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3493 - val_loss: 0.3684\n",
      "Epoch 451/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3495 - val_loss: 0.3588\n",
      "Epoch 452/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3484 - val_loss: 0.3558\n",
      "Epoch 453/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3505 - val_loss: 0.3585\n",
      "Epoch 454/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3477 - val_loss: 0.3651\n",
      "Epoch 455/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3455 - val_loss: 0.3574\n",
      "Epoch 456/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3482 - val_loss: 0.3566\n",
      "Epoch 457/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3461 - val_loss: 0.3579\n",
      "Epoch 458/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3470 - val_loss: 0.3643\n",
      "Epoch 459/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3483 - val_loss: 0.3616\n",
      "Epoch 460/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3474 - val_loss: 0.3631\n",
      "Epoch 461/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3489 - val_loss: 0.3639\n",
      "Epoch 462/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3485 - val_loss: 0.3614\n",
      "Epoch 463/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3473 - val_loss: 0.3634\n",
      "Epoch 464/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3477 - val_loss: 0.3661\n",
      "Epoch 465/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3476 - val_loss: 0.3648\n",
      "Epoch 466/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3491 - val_loss: 0.3588\n",
      "Epoch 467/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3486 - val_loss: 0.3621\n",
      "Epoch 468/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3469 - val_loss: 0.3549\n",
      "Epoch 469/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3472 - val_loss: 0.3585\n",
      "Epoch 470/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3461 - val_loss: 0.3574\n",
      "Epoch 471/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3467 - val_loss: 0.3579\n",
      "Epoch 472/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3490 - val_loss: 0.3648\n",
      "Epoch 473/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3499 - val_loss: 0.3563\n",
      "Epoch 474/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3466 - val_loss: 0.3545\n",
      "Epoch 475/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3498 - val_loss: 0.3597\n",
      "Epoch 476/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3484 - val_loss: 0.3633\n",
      "Epoch 477/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3488 - val_loss: 0.3679\n",
      "Epoch 478/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3481 - val_loss: 0.3611\n",
      "Epoch 479/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3460 - val_loss: 0.3578\n",
      "Epoch 480/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3448 - val_loss: 0.3651\n",
      "Epoch 481/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3474 - val_loss: 0.3615\n",
      "Epoch 482/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3472 - val_loss: 0.3605\n",
      "Epoch 483/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3479 - val_loss: 0.3616\n",
      "Epoch 484/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3473 - val_loss: 0.3575\n",
      "Epoch 485/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3478 - val_loss: 0.3596\n",
      "Epoch 486/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3466 - val_loss: 0.3588\n",
      "Epoch 487/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3474 - val_loss: 0.3592\n",
      "Epoch 488/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3462 - val_loss: 0.3593\n",
      "Epoch 489/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3461 - val_loss: 0.3582\n",
      "Epoch 490/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3513 - val_loss: 0.3639\n",
      "Epoch 491/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3480 - val_loss: 0.3667\n",
      "Epoch 492/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3491 - val_loss: 0.3637\n",
      "Epoch 493/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3485 - val_loss: 0.3674\n",
      "Epoch 494/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3451 - val_loss: 0.3587\n",
      "Epoch 495/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3467 - val_loss: 0.3619\n",
      "Epoch 496/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3483 - val_loss: 0.3665\n",
      "Epoch 497/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3489 - val_loss: 0.3560\n",
      "Epoch 498/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3468 - val_loss: 0.3639\n",
      "Epoch 499/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3483 - val_loss: 0.3602\n",
      "Epoch 500/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3460 - val_loss: 0.3534\n",
      "Epoch 501/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3451 - val_loss: 0.3642\n",
      "Epoch 502/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3463 - val_loss: 0.3674\n",
      "Epoch 503/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3487 - val_loss: 0.3646\n",
      "Epoch 504/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3471 - val_loss: 0.3615\n",
      "Epoch 505/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3494 - val_loss: 0.3707\n",
      "Epoch 506/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3490 - val_loss: 0.3581\n",
      "Epoch 507/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3453 - val_loss: 0.3550\n",
      "Epoch 508/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3458 - val_loss: 0.3623\n",
      "Epoch 509/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3472 - val_loss: 0.3607\n",
      "Epoch 510/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3484 - val_loss: 0.3569\n",
      "Epoch 511/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3483 - val_loss: 0.3623\n",
      "Epoch 512/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3449 - val_loss: 0.3630\n",
      "Epoch 513/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3462 - val_loss: 0.3664\n",
      "Epoch 514/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3479 - val_loss: 0.3608\n",
      "Epoch 515/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3471 - val_loss: 0.3575\n",
      "Epoch 516/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3458 - val_loss: 0.3674\n",
      "Epoch 517/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3476 - val_loss: 0.3588\n",
      "Epoch 518/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3483 - val_loss: 0.3547\n",
      "Epoch 519/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3468 - val_loss: 0.3605\n",
      "Epoch 520/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3457 - val_loss: 0.3568\n",
      "Epoch 521/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3489 - val_loss: 0.3636\n",
      "Epoch 522/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3458 - val_loss: 0.3651\n",
      "Epoch 523/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3480 - val_loss: 0.3611\n",
      "Epoch 524/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3465 - val_loss: 0.3589\n",
      "Epoch 525/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3444 - val_loss: 0.3605\n",
      "Epoch 526/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3493 - val_loss: 0.3568\n",
      "Epoch 527/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3471 - val_loss: 0.3544\n",
      "Epoch 528/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3465 - val_loss: 0.3626\n",
      "Epoch 529/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3460 - val_loss: 0.3634\n",
      "Epoch 530/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3494 - val_loss: 0.3596\n",
      "Epoch 531/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3473 - val_loss: 0.3590\n",
      "Epoch 532/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3465 - val_loss: 0.3583\n",
      "Epoch 533/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3471 - val_loss: 0.3574\n",
      "Epoch 534/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3514 - val_loss: 0.3687\n",
      "Epoch 535/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3466 - val_loss: 0.3664\n",
      "Epoch 536/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3490 - val_loss: 0.3618\n",
      "Epoch 537/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3464 - val_loss: 0.3618\n",
      "Epoch 538/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3469 - val_loss: 0.3632\n",
      "Epoch 539/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3466 - val_loss: 0.3556\n",
      "Epoch 540/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3474 - val_loss: 0.3691\n",
      "Epoch 541/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3474 - val_loss: 0.3565\n",
      "Epoch 542/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3482 - val_loss: 0.3535\n",
      "Epoch 543/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3449 - val_loss: 0.3550\n",
      "Epoch 544/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3468 - val_loss: 0.3612\n",
      "Epoch 545/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3491 - val_loss: 0.3644\n",
      "Epoch 546/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3484 - val_loss: 0.3606\n",
      "Epoch 547/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3465 - val_loss: 0.3624\n",
      "Epoch 548/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3457 - val_loss: 0.3716\n",
      "Epoch 549/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3473 - val_loss: 0.3704\n",
      "Epoch 550/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3509 - val_loss: 0.3610\n",
      "Epoch 551/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3493 - val_loss: 0.3593\n",
      "Epoch 552/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3467 - val_loss: 0.3645\n",
      "Epoch 553/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3451 - val_loss: 0.3602\n",
      "Epoch 554/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3479 - val_loss: 0.3589\n",
      "Epoch 555/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3461 - val_loss: 0.3555\n",
      "Epoch 556/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3483 - val_loss: 0.3681\n",
      "Epoch 557/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3471 - val_loss: 0.3623\n",
      "Epoch 558/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3457 - val_loss: 0.3620\n",
      "Epoch 559/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3480 - val_loss: 0.3644\n",
      "Epoch 560/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3475 - val_loss: 0.3594\n",
      "Epoch 561/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3499 - val_loss: 0.3673\n",
      "Epoch 562/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3474 - val_loss: 0.3771\n",
      "Epoch 563/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3478 - val_loss: 0.3606\n",
      "Epoch 564/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3464 - val_loss: 0.3652\n",
      "Epoch 565/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3493 - val_loss: 0.3622\n",
      "Epoch 566/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3458 - val_loss: 0.3626\n",
      "Epoch 567/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3455 - val_loss: 0.3568\n",
      "Epoch 568/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3468 - val_loss: 0.3570\n",
      "Epoch 569/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3466 - val_loss: 0.3607\n",
      "Epoch 570/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3491 - val_loss: 0.3574\n",
      "Epoch 571/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3451 - val_loss: 0.3611\n",
      "Epoch 572/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3471 - val_loss: 0.3563\n",
      "Epoch 573/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3469 - val_loss: 0.3714\n",
      "Epoch 574/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3477 - val_loss: 0.3577\n",
      "Epoch 575/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3464 - val_loss: 0.3586\n",
      "Epoch 576/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3476 - val_loss: 0.3595\n",
      "Epoch 577/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3478 - val_loss: 0.3531\n",
      "Epoch 578/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3463 - val_loss: 0.3596\n",
      "Epoch 579/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3479 - val_loss: 0.3607\n",
      "Epoch 580/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3464 - val_loss: 0.3674\n",
      "Epoch 581/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3480 - val_loss: 0.3592\n",
      "Epoch 582/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3477 - val_loss: 0.3677\n",
      "Epoch 583/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3456 - val_loss: 0.3576\n",
      "Epoch 584/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3462 - val_loss: 0.3622\n",
      "Epoch 585/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3498 - val_loss: 0.3733\n",
      "Epoch 586/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3465 - val_loss: 0.3686\n",
      "Epoch 587/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3458 - val_loss: 0.3685\n",
      "Epoch 588/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3490 - val_loss: 0.3748\n",
      "Epoch 589/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3487 - val_loss: 0.3652\n",
      "Epoch 590/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3513 - val_loss: 0.3595\n",
      "Epoch 591/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3455 - val_loss: 0.3629\n",
      "Epoch 592/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3476 - val_loss: 0.3637\n",
      "Epoch 593/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3503 - val_loss: 0.3608\n",
      "Epoch 594/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3465 - val_loss: 0.3569\n",
      "Epoch 595/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3466 - val_loss: 0.3542\n",
      "Epoch 596/1000\n",
      "124/124 [==============================] - 0s 2ms/step - loss: 0.3465 - val_loss: 0.3588\n",
      "Epoch 597/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3448 - val_loss: 0.3608\n",
      "Epoch 598/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3471 - val_loss: 0.3605\n",
      "Epoch 599/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3467 - val_loss: 0.3565\n",
      "Epoch 600/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3465 - val_loss: 0.3591\n",
      "Epoch 601/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3475 - val_loss: 0.3611\n",
      "Epoch 602/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3472 - val_loss: 0.3609\n",
      "Epoch 603/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3490 - val_loss: 0.3640\n",
      "Epoch 604/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3474 - val_loss: 0.3684\n",
      "Epoch 605/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3481 - val_loss: 0.3652\n",
      "Epoch 606/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3461 - val_loss: 0.3618\n",
      "Epoch 607/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3478 - val_loss: 0.3555\n",
      "Epoch 608/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3457 - val_loss: 0.3603\n",
      "Epoch 609/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3473 - val_loss: 0.3622\n",
      "Epoch 610/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3491 - val_loss: 0.3566\n",
      "Epoch 611/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3458 - val_loss: 0.3667\n",
      "Epoch 612/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3460 - val_loss: 0.3671\n",
      "Epoch 613/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3467 - val_loss: 0.3598\n",
      "Epoch 614/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3475 - val_loss: 0.3550\n",
      "Epoch 615/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3470 - val_loss: 0.3697\n",
      "Epoch 616/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3512 - val_loss: 0.3614\n",
      "Epoch 617/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3465 - val_loss: 0.3626\n",
      "Epoch 618/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3465 - val_loss: 0.3579\n",
      "Epoch 619/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3482 - val_loss: 0.3748\n",
      "Epoch 620/1000\n",
      "124/124 [==============================] - 0s 2ms/step - loss: 0.3511 - val_loss: 0.3630\n",
      "Epoch 621/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3461 - val_loss: 0.3548\n",
      "Epoch 622/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3492 - val_loss: 0.3577\n",
      "Epoch 623/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3488 - val_loss: 0.3595\n",
      "Epoch 624/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3487 - val_loss: 0.3603\n",
      "Epoch 625/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3471 - val_loss: 0.3569\n",
      "Epoch 626/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3468 - val_loss: 0.3537\n",
      "Epoch 627/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3476 - val_loss: 0.3606\n",
      "Epoch 628/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3472 - val_loss: 0.3591\n",
      "Epoch 629/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3468 - val_loss: 0.3596\n",
      "Epoch 630/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3467 - val_loss: 0.3565\n",
      "Epoch 631/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3475 - val_loss: 0.3616\n",
      "Epoch 632/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3465 - val_loss: 0.3590\n",
      "Epoch 633/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3482 - val_loss: 0.3543\n",
      "Epoch 634/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3489 - val_loss: 0.3655\n",
      "Epoch 635/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3475 - val_loss: 0.3575\n",
      "Epoch 636/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3458 - val_loss: 0.3612\n",
      "Epoch 637/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3463 - val_loss: 0.3614\n",
      "Epoch 638/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3486 - val_loss: 0.3677\n",
      "Epoch 639/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3479 - val_loss: 0.3587\n",
      "Epoch 640/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3463 - val_loss: 0.3637\n",
      "Epoch 641/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3462 - val_loss: 0.3624\n",
      "Epoch 642/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3473 - val_loss: 0.3660\n",
      "Epoch 643/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3484 - val_loss: 0.3645\n",
      "Epoch 644/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3492 - val_loss: 0.3674\n",
      "Epoch 645/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3476 - val_loss: 0.3691\n",
      "Epoch 646/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3462 - val_loss: 0.3537\n",
      "Epoch 647/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3469 - val_loss: 0.3634\n",
      "Epoch 648/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3478 - val_loss: 0.3600\n",
      "Epoch 649/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3468 - val_loss: 0.3588\n",
      "Epoch 650/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3479 - val_loss: 0.3604\n",
      "Epoch 651/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3461 - val_loss: 0.3593\n",
      "Epoch 652/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3475 - val_loss: 0.3625\n",
      "Epoch 653/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3477 - val_loss: 0.3583\n",
      "Epoch 654/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3463 - val_loss: 0.3684\n",
      "Epoch 655/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3491 - val_loss: 0.3644\n",
      "Epoch 656/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3506 - val_loss: 0.3643\n",
      "Epoch 657/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3484 - val_loss: 0.3558\n",
      "Epoch 658/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3467 - val_loss: 0.3604\n",
      "Epoch 659/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3477 - val_loss: 0.3653\n",
      "Epoch 660/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3464 - val_loss: 0.3592\n",
      "Epoch 661/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3462 - val_loss: 0.3572\n",
      "Epoch 662/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3461 - val_loss: 0.3580\n",
      "Epoch 663/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3450 - val_loss: 0.3635\n",
      "Epoch 664/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3453 - val_loss: 0.3621\n",
      "Epoch 665/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3467 - val_loss: 0.3666\n",
      "Epoch 666/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3459 - val_loss: 0.3595\n",
      "Epoch 667/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3472 - val_loss: 0.3522\n",
      "Epoch 668/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3478 - val_loss: 0.3601\n",
      "Epoch 669/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3473 - val_loss: 0.3548\n",
      "Epoch 670/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3476 - val_loss: 0.3577\n",
      "Epoch 671/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3477 - val_loss: 0.3631\n",
      "Epoch 672/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3477 - val_loss: 0.3655\n",
      "Epoch 673/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3457 - val_loss: 0.3596\n",
      "Epoch 674/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3460 - val_loss: 0.3583\n",
      "Epoch 675/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3488 - val_loss: 0.3681\n",
      "Epoch 676/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3500 - val_loss: 0.3601\n",
      "Epoch 677/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3480 - val_loss: 0.3563\n",
      "Epoch 678/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3479 - val_loss: 0.3622\n",
      "Epoch 679/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3461 - val_loss: 0.3560\n",
      "Epoch 680/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3472 - val_loss: 0.3737\n",
      "Epoch 681/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3472 - val_loss: 0.3625\n",
      "Epoch 682/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3458 - val_loss: 0.3631\n",
      "Epoch 683/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3460 - val_loss: 0.3603\n",
      "Epoch 684/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3470 - val_loss: 0.3632\n",
      "Epoch 685/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3471 - val_loss: 0.3597\n",
      "Epoch 686/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3447 - val_loss: 0.3607\n",
      "Epoch 687/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3473 - val_loss: 0.3562\n",
      "Epoch 688/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3486 - val_loss: 0.3580\n",
      "Epoch 689/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3467 - val_loss: 0.3568\n",
      "Epoch 690/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3474 - val_loss: 0.3570\n",
      "Epoch 691/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3463 - val_loss: 0.3616\n",
      "Epoch 692/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3480 - val_loss: 0.3577\n",
      "Epoch 693/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3445 - val_loss: 0.3574\n",
      "Epoch 694/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3450 - val_loss: 0.3654\n",
      "Epoch 695/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3509 - val_loss: 0.3557\n",
      "Epoch 696/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3473 - val_loss: 0.3637\n",
      "Epoch 697/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3462 - val_loss: 0.3685\n",
      "Epoch 698/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3464 - val_loss: 0.3553\n",
      "Epoch 699/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3462 - val_loss: 0.3540\n",
      "Epoch 700/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3481 - val_loss: 0.3535\n",
      "Epoch 701/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3466 - val_loss: 0.3627\n",
      "Epoch 702/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3465 - val_loss: 0.3621\n",
      "Epoch 703/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3467 - val_loss: 0.3586\n",
      "Epoch 704/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3465 - val_loss: 0.3636\n",
      "Epoch 705/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3480 - val_loss: 0.3598\n",
      "Epoch 706/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3481 - val_loss: 0.3625\n",
      "Epoch 707/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3453 - val_loss: 0.3661\n",
      "Epoch 708/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3466 - val_loss: 0.3588\n",
      "Epoch 709/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3455 - val_loss: 0.3644\n",
      "Epoch 710/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3463 - val_loss: 0.3575\n",
      "Epoch 711/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3464 - val_loss: 0.3548\n",
      "Epoch 712/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3492 - val_loss: 0.3596\n",
      "Epoch 713/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3462 - val_loss: 0.3589\n",
      "Epoch 714/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3489 - val_loss: 0.3596\n",
      "Epoch 715/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3454 - val_loss: 0.3619\n",
      "Epoch 716/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3451 - val_loss: 0.3561\n",
      "Epoch 717/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3462 - val_loss: 0.3543\n",
      "Epoch 718/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3453 - val_loss: 0.3586\n",
      "Epoch 719/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3475 - val_loss: 0.3592\n",
      "Epoch 720/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3477 - val_loss: 0.3677\n",
      "Epoch 721/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3489 - val_loss: 0.3641\n",
      "Epoch 722/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3470 - val_loss: 0.3651\n",
      "Epoch 723/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3480 - val_loss: 0.3608\n",
      "Epoch 724/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3466 - val_loss: 0.3655\n",
      "Epoch 725/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3460 - val_loss: 0.3612\n",
      "Epoch 726/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3459 - val_loss: 0.3627\n",
      "Epoch 727/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3467 - val_loss: 0.3655\n",
      "Epoch 728/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3451 - val_loss: 0.3613\n",
      "Epoch 729/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3496 - val_loss: 0.3632\n",
      "Epoch 730/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3482 - val_loss: 0.3620\n",
      "Epoch 731/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3452 - val_loss: 0.3605\n",
      "Epoch 732/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3477 - val_loss: 0.3595\n",
      "Epoch 733/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3464 - val_loss: 0.3676\n",
      "Epoch 734/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3457 - val_loss: 0.3569\n",
      "Epoch 735/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3466 - val_loss: 0.3591\n",
      "Epoch 736/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3465 - val_loss: 0.3627\n",
      "Epoch 737/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3471 - val_loss: 0.3567\n",
      "Epoch 738/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3468 - val_loss: 0.3587\n",
      "Epoch 739/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3455 - val_loss: 0.3570\n",
      "Epoch 740/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3465 - val_loss: 0.3633\n",
      "Epoch 741/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3482 - val_loss: 0.3633\n",
      "Epoch 742/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3494 - val_loss: 0.3582\n",
      "Epoch 743/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3447 - val_loss: 0.3595\n",
      "Epoch 744/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3466 - val_loss: 0.3610\n",
      "Epoch 745/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3488 - val_loss: 0.3528\n",
      "Epoch 746/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3445 - val_loss: 0.3642\n",
      "Epoch 747/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3478 - val_loss: 0.3604\n",
      "Epoch 748/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3494 - val_loss: 0.3609\n",
      "Epoch 749/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3479 - val_loss: 0.3683\n",
      "Epoch 750/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3453 - val_loss: 0.3562\n",
      "Epoch 751/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3479 - val_loss: 0.3614\n",
      "Epoch 752/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3479 - val_loss: 0.3724\n",
      "Epoch 753/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3452 - val_loss: 0.3700\n",
      "Epoch 754/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3457 - val_loss: 0.3602\n",
      "Epoch 755/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3452 - val_loss: 0.3545\n",
      "Epoch 756/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3458 - val_loss: 0.3583\n",
      "Epoch 757/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3472 - val_loss: 0.3655\n",
      "Epoch 758/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3480 - val_loss: 0.3607\n",
      "Epoch 759/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3471 - val_loss: 0.3573\n",
      "Epoch 760/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3460 - val_loss: 0.3595\n",
      "Epoch 761/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3459 - val_loss: 0.3571\n",
      "Epoch 762/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3470 - val_loss: 0.3642\n",
      "Epoch 763/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3481 - val_loss: 0.3547\n",
      "Epoch 764/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3437 - val_loss: 0.3602\n",
      "Epoch 765/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3460 - val_loss: 0.3667\n",
      "Epoch 766/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3460 - val_loss: 0.3654\n",
      "Epoch 767/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3453 - val_loss: 0.3569\n",
      "Epoch 768/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3468 - val_loss: 0.3648\n",
      "Epoch 769/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3456 - val_loss: 0.3564\n",
      "Epoch 770/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3440 - val_loss: 0.3585\n",
      "Epoch 771/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3465 - val_loss: 0.3577\n",
      "Epoch 772/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3448 - val_loss: 0.3632\n",
      "Epoch 773/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3465 - val_loss: 0.3605\n",
      "Epoch 774/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3470 - val_loss: 0.3575\n",
      "Epoch 775/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3465 - val_loss: 0.3566\n",
      "Epoch 776/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3463 - val_loss: 0.3595\n",
      "Epoch 777/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3476 - val_loss: 0.3578\n",
      "Epoch 778/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3451 - val_loss: 0.3629\n",
      "Epoch 779/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3471 - val_loss: 0.3588\n",
      "Epoch 780/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3471 - val_loss: 0.3642\n",
      "Epoch 781/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3486 - val_loss: 0.3752\n",
      "Epoch 782/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3483 - val_loss: 0.3583\n",
      "Epoch 783/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3457 - val_loss: 0.3549\n",
      "Epoch 784/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3486 - val_loss: 0.3611\n",
      "Epoch 785/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3455 - val_loss: 0.3581\n",
      "Epoch 786/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3500 - val_loss: 0.3712\n",
      "Epoch 787/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3458 - val_loss: 0.3578\n",
      "Epoch 788/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3452 - val_loss: 0.3565\n",
      "Epoch 789/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3452 - val_loss: 0.3621\n",
      "Epoch 790/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3470 - val_loss: 0.3579\n",
      "Epoch 791/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3476 - val_loss: 0.3637\n",
      "Epoch 792/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3466 - val_loss: 0.3573\n",
      "Epoch 793/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3464 - val_loss: 0.3609\n",
      "Epoch 794/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3475 - val_loss: 0.3598\n",
      "Epoch 795/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3461 - val_loss: 0.3591\n",
      "Epoch 796/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3473 - val_loss: 0.3642\n",
      "Epoch 797/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3480 - val_loss: 0.3587\n",
      "Epoch 798/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3472 - val_loss: 0.3607\n",
      "Epoch 799/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3478 - val_loss: 0.3539\n",
      "Epoch 800/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3462 - val_loss: 0.3610\n",
      "Epoch 801/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3451 - val_loss: 0.3579\n",
      "Epoch 802/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3452 - val_loss: 0.3608\n",
      "Epoch 803/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3454 - val_loss: 0.3639\n",
      "Epoch 804/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3463 - val_loss: 0.3640\n",
      "Epoch 805/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3465 - val_loss: 0.3552\n",
      "Epoch 806/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3449 - val_loss: 0.3636\n",
      "Epoch 807/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3464 - val_loss: 0.3579\n",
      "Epoch 808/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3446 - val_loss: 0.3579\n",
      "Epoch 809/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3455 - val_loss: 0.3563\n",
      "Epoch 810/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3461 - val_loss: 0.3629\n",
      "Epoch 811/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3503 - val_loss: 0.3579\n",
      "Epoch 812/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3441 - val_loss: 0.3602\n",
      "Epoch 813/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3460 - val_loss: 0.3550\n",
      "Epoch 814/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3455 - val_loss: 0.3703\n",
      "Epoch 815/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3469 - val_loss: 0.3800\n",
      "Epoch 816/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3493 - val_loss: 0.3573\n",
      "Epoch 817/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3460 - val_loss: 0.3600\n",
      "Epoch 818/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3459 - val_loss: 0.3620\n",
      "Epoch 819/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3472 - val_loss: 0.3545\n",
      "Epoch 820/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3450 - val_loss: 0.3714\n",
      "Epoch 821/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3468 - val_loss: 0.3557\n",
      "Epoch 822/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3480 - val_loss: 0.3554\n",
      "Epoch 823/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3466 - val_loss: 0.3597\n",
      "Epoch 824/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3444 - val_loss: 0.3702\n",
      "Epoch 825/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3491 - val_loss: 0.3588\n",
      "Epoch 826/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3462 - val_loss: 0.3590\n",
      "Epoch 827/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3446 - val_loss: 0.3577\n",
      "Epoch 828/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3447 - val_loss: 0.3591\n",
      "Epoch 829/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3484 - val_loss: 0.3597\n",
      "Epoch 830/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3461 - val_loss: 0.3628\n",
      "Epoch 831/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3458 - val_loss: 0.3662\n",
      "Epoch 832/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3464 - val_loss: 0.3563\n",
      "Epoch 833/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3467 - val_loss: 0.3615\n",
      "Epoch 834/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3457 - val_loss: 0.3595\n",
      "Epoch 835/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3469 - val_loss: 0.3646\n",
      "Epoch 836/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3481 - val_loss: 0.3606\n",
      "Epoch 837/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3451 - val_loss: 0.3573\n",
      "Epoch 838/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3441 - val_loss: 0.3588\n",
      "Epoch 839/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3475 - val_loss: 0.3596\n",
      "Epoch 840/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3470 - val_loss: 0.3553\n",
      "Epoch 841/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3466 - val_loss: 0.3580\n",
      "Epoch 842/1000\n",
      "124/124 [==============================] - 0s 2ms/step - loss: 0.3449 - val_loss: 0.3556\n",
      "Epoch 843/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3484 - val_loss: 0.3615\n",
      "Epoch 844/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3462 - val_loss: 0.3610\n",
      "Epoch 845/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3490 - val_loss: 0.3605\n",
      "Epoch 846/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3473 - val_loss: 0.3592\n",
      "Epoch 847/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3491 - val_loss: 0.3612\n",
      "Epoch 848/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3485 - val_loss: 0.3603\n",
      "Epoch 849/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3457 - val_loss: 0.3623\n",
      "Epoch 850/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3482 - val_loss: 0.3586\n",
      "Epoch 851/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3496 - val_loss: 0.3600\n",
      "Epoch 852/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3460 - val_loss: 0.3558\n",
      "Epoch 853/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3480 - val_loss: 0.3595\n",
      "Epoch 854/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3448 - val_loss: 0.3587\n",
      "Epoch 855/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3462 - val_loss: 0.3591\n",
      "Epoch 856/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3462 - val_loss: 0.3550\n",
      "Epoch 857/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3489 - val_loss: 0.3571\n",
      "Epoch 858/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3457 - val_loss: 0.3612\n",
      "Epoch 859/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3450 - val_loss: 0.3583\n",
      "Epoch 860/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3481 - val_loss: 0.3745\n",
      "Epoch 861/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3485 - val_loss: 0.3549\n",
      "Epoch 862/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3486 - val_loss: 0.3538\n",
      "Epoch 863/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3455 - val_loss: 0.3565\n",
      "Epoch 864/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3470 - val_loss: 0.3590\n",
      "Epoch 865/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3483 - val_loss: 0.3608\n",
      "Epoch 866/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3448 - val_loss: 0.3688\n",
      "Epoch 867/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3483 - val_loss: 0.3620\n",
      "Epoch 868/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3455 - val_loss: 0.3593\n",
      "Epoch 869/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3460 - val_loss: 0.3537\n",
      "Epoch 870/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3452 - val_loss: 0.3539\n",
      "Epoch 871/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3500 - val_loss: 0.3598\n",
      "Epoch 872/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3470 - val_loss: 0.3667\n",
      "Epoch 873/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3472 - val_loss: 0.3613\n",
      "Epoch 874/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3470 - val_loss: 0.3563\n",
      "Epoch 875/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3459 - val_loss: 0.3618\n",
      "Epoch 876/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3470 - val_loss: 0.3698\n",
      "Epoch 877/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3478 - val_loss: 0.3567\n",
      "Epoch 878/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3468 - val_loss: 0.3561\n",
      "Epoch 879/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3468 - val_loss: 0.3619\n",
      "Epoch 880/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3478 - val_loss: 0.3802\n",
      "Epoch 881/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3462 - val_loss: 0.3592\n",
      "Epoch 882/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3472 - val_loss: 0.3558\n",
      "Epoch 883/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3464 - val_loss: 0.3559\n",
      "Epoch 884/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3448 - val_loss: 0.3593\n",
      "Epoch 885/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3469 - val_loss: 0.3579\n",
      "Epoch 886/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3446 - val_loss: 0.3597\n",
      "Epoch 887/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3448 - val_loss: 0.3567\n",
      "Epoch 888/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3451 - val_loss: 0.3600\n",
      "Epoch 889/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3491 - val_loss: 0.3636\n",
      "Epoch 890/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3457 - val_loss: 0.3662\n",
      "Epoch 891/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3467 - val_loss: 0.3578\n",
      "Epoch 892/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3454 - val_loss: 0.3605\n",
      "Epoch 893/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3520 - val_loss: 0.3604\n",
      "Epoch 894/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3455 - val_loss: 0.3580\n",
      "Epoch 895/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3486 - val_loss: 0.3541\n",
      "Epoch 896/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3460 - val_loss: 0.3574\n",
      "Epoch 897/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3477 - val_loss: 0.3604\n",
      "Epoch 898/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3460 - val_loss: 0.3589\n",
      "Epoch 899/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3476 - val_loss: 0.3588\n",
      "Epoch 900/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3463 - val_loss: 0.3609\n",
      "Epoch 901/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3495 - val_loss: 0.3683\n",
      "Epoch 902/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3459 - val_loss: 0.3596\n",
      "Epoch 903/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3459 - val_loss: 0.3632\n",
      "Epoch 904/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3471 - val_loss: 0.3669\n",
      "Epoch 905/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3488 - val_loss: 0.3565\n",
      "Epoch 906/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3484 - val_loss: 0.3614\n",
      "Epoch 907/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3470 - val_loss: 0.3563\n",
      "Epoch 908/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3462 - val_loss: 0.3566\n",
      "Epoch 909/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3452 - val_loss: 0.3612\n",
      "Epoch 910/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3481 - val_loss: 0.3685\n",
      "Epoch 911/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3484 - val_loss: 0.3542\n",
      "Epoch 912/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3479 - val_loss: 0.3603\n",
      "Epoch 913/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3468 - val_loss: 0.3573\n",
      "Epoch 914/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3463 - val_loss: 0.3598\n",
      "Epoch 915/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3462 - val_loss: 0.3624\n",
      "Epoch 916/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3468 - val_loss: 0.3553\n",
      "Epoch 917/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3465 - val_loss: 0.3587\n",
      "Epoch 918/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3472 - val_loss: 0.3610\n",
      "Epoch 919/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3460 - val_loss: 0.3613\n",
      "Epoch 920/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3495 - val_loss: 0.3616\n",
      "Epoch 921/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3454 - val_loss: 0.3629\n",
      "Epoch 922/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3458 - val_loss: 0.3542\n",
      "Epoch 923/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3462 - val_loss: 0.3596\n",
      "Epoch 924/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3464 - val_loss: 0.3636\n",
      "Epoch 925/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3475 - val_loss: 0.3581\n",
      "Epoch 926/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3460 - val_loss: 0.3614\n",
      "Epoch 927/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3464 - val_loss: 0.3598\n",
      "Epoch 928/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3456 - val_loss: 0.3598\n",
      "Epoch 929/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3457 - val_loss: 0.3590\n",
      "Epoch 930/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3475 - val_loss: 0.3683\n",
      "Epoch 931/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3458 - val_loss: 0.3591\n",
      "Epoch 932/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3438 - val_loss: 0.3590\n",
      "Epoch 933/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3471 - val_loss: 0.3597\n",
      "Epoch 934/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3463 - val_loss: 0.3566\n",
      "Epoch 935/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3486 - val_loss: 0.3617\n",
      "Epoch 936/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3511 - val_loss: 0.3795\n",
      "Epoch 937/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3455 - val_loss: 0.3636\n",
      "Epoch 938/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3446 - val_loss: 0.3596\n",
      "Epoch 939/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3467 - val_loss: 0.3606\n",
      "Epoch 940/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3458 - val_loss: 0.3553\n",
      "Epoch 941/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3476 - val_loss: 0.3627\n",
      "Epoch 942/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3453 - val_loss: 0.3583\n",
      "Epoch 943/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3458 - val_loss: 0.3592\n",
      "Epoch 944/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3473 - val_loss: 0.3602\n",
      "Epoch 945/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3480 - val_loss: 0.3573\n",
      "Epoch 946/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3476 - val_loss: 0.3580\n",
      "Epoch 947/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3466 - val_loss: 0.3645\n",
      "Epoch 948/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3464 - val_loss: 0.3640\n",
      "Epoch 949/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3479 - val_loss: 0.3611\n",
      "Epoch 950/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3463 - val_loss: 0.3726\n",
      "Epoch 951/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3483 - val_loss: 0.3594\n",
      "Epoch 952/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3482 - val_loss: 0.3596\n",
      "Epoch 953/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3460 - val_loss: 0.3574\n",
      "Epoch 954/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3455 - val_loss: 0.3631\n",
      "Epoch 955/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3470 - val_loss: 0.3677\n",
      "Epoch 956/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3464 - val_loss: 0.3563\n",
      "Epoch 957/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3479 - val_loss: 0.3574\n",
      "Epoch 958/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3457 - val_loss: 0.3562\n",
      "Epoch 959/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3449 - val_loss: 0.3624\n",
      "Epoch 960/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3472 - val_loss: 0.3572\n",
      "Epoch 961/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3459 - val_loss: 0.3587\n",
      "Epoch 962/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3451 - val_loss: 0.3581\n",
      "Epoch 963/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3477 - val_loss: 0.3553\n",
      "Epoch 964/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3482 - val_loss: 0.3607\n",
      "Epoch 965/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3462 - val_loss: 0.3625\n",
      "Epoch 966/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3466 - val_loss: 0.3641\n",
      "Epoch 967/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3464 - val_loss: 0.3583\n",
      "Epoch 968/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3469 - val_loss: 0.3530\n",
      "Epoch 969/1000\n",
      "124/124 [==============================] - 0s 2ms/step - loss: 0.3448 - val_loss: 0.3652\n",
      "Epoch 970/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3466 - val_loss: 0.3555\n",
      "Epoch 971/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3477 - val_loss: 0.3716\n",
      "Epoch 972/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3453 - val_loss: 0.3582\n",
      "Epoch 973/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3446 - val_loss: 0.3591\n",
      "Epoch 974/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3456 - val_loss: 0.3576\n",
      "Epoch 975/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3475 - val_loss: 0.3555\n",
      "Epoch 976/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3472 - val_loss: 0.3589\n",
      "Epoch 977/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3468 - val_loss: 0.3663\n",
      "Epoch 978/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3482 - val_loss: 0.3624\n",
      "Epoch 979/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3452 - val_loss: 0.3597\n",
      "Epoch 980/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3447 - val_loss: 0.3566\n",
      "Epoch 981/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3454 - val_loss: 0.3611\n",
      "Epoch 982/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3447 - val_loss: 0.3633\n",
      "Epoch 983/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3473 - val_loss: 0.3597\n",
      "Epoch 984/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3464 - val_loss: 0.3673\n",
      "Epoch 985/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3478 - val_loss: 0.3717\n",
      "Epoch 986/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3456 - val_loss: 0.3673\n",
      "Epoch 987/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3470 - val_loss: 0.3536\n",
      "Epoch 988/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3464 - val_loss: 0.3541\n",
      "Epoch 989/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3460 - val_loss: 0.3570\n",
      "Epoch 990/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3468 - val_loss: 0.3589\n",
      "Epoch 991/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3451 - val_loss: 0.3635\n",
      "Epoch 992/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3473 - val_loss: 0.3611\n",
      "Epoch 993/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3446 - val_loss: 0.3544\n",
      "Epoch 994/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3472 - val_loss: 0.3545\n",
      "Epoch 995/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3474 - val_loss: 0.3577\n",
      "Epoch 996/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3467 - val_loss: 0.3569\n",
      "Epoch 997/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3473 - val_loss: 0.3594\n",
      "Epoch 998/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3457 - val_loss: 0.3589\n",
      "Epoch 999/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3441 - val_loss: 0.3704\n",
      "Epoch 1000/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3487 - val_loss: 0.3591\n",
      "162/162 [==============================] - 0s 851us/step - loss: 0.3589\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_log = model.fit(X_train, y_train, epochs=1000, batch_size=100, validation_split=.2, verbose=1)\n",
    "model.evaluate(X_test, y_test)\n",
    "plot_loss(train_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics of Overfitting and Underfitting in Neural Networks\n",
    "\n",
    "Just like any other type of model, our primary task in trying to attain an accurate set of predictions is to balance the overfitting and underfitting. In a neural network, the ideas are the same as with standard models, however the tools and their usage can differ slightly. \n",
    "\n",
    "### Add Data\n",
    "\n",
    "Adding data to the training set is the number one way to improve accuracy. As noted above, neural networks are commonly able to acheive very high accuracy levels if provided with very large training sets. For smaller datasets, the probability of a neural network being the best model is much lower than with big data. \n",
    "\n",
    "### Model Capacity\n",
    "\n",
    "The model capacity is the \"size\" of the model - refering to the combination of the number of neurons on each layer and the number of layers. \n",
    "\n",
    "In general the larger a feature set is, the larger a capacity we will need to be able to avoid underfitting and make accurate predictions. However, similar to a decision tree, if the model becomes too large for the data, we are likely to overfit. \n",
    "\n",
    "In big data scenarios (e.g. Google or Tesla training image recognition models) the feature sets can be massive (e.g. a 5 megapixel image is at least 15 million features) so the networks used have a very high capacity. Because there is a lot of training data, the model is able to have a huge capacity, but not overfit. These models can take FOREVER to process (e.g. weeks with the work paralellized on dedicated and fast machines) but they are able to make very accurate predictions since they get all the \"benefits\" of overfitting - predictions highly tailored to the training data; along with all the \"benefits\" of underfitting - since there is so much training data, they are still generalized enough to predict new data. \n",
    "\n",
    "The combination of large datasets, deep networks, and fast processing allows for most of the modern AI that we see or interact with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization_6 (Normalizat  (None, 8)                17        \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 128)               1152      \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50,834\n",
      "Trainable params: 50,817\n",
      "Non-trainable params: 17\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Test Different Model Capacities\n",
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "normalizer.adapt(np.array(X_train))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(normalizer)\n",
    "model.add(Dense(128, input_dim=18, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 821us/step - loss: 0.3791\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.compile(loss='mean_absolute_error', optimizer=tf.optimizers.Adam(learning_rate=.01))\n",
    "train_log = model.fit(X_train, y_train, epochs=1000, batch_size=1000, validation_split=.2, verbose=0)\n",
    "model.evaluate(X_test, y_test)\n",
    "plot_loss(train_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "\n",
    "Early stopping is very common with neural networks, due to the common pattern mentioned above of the optimal balance of over/under fitting occuring at some point within many, potentially thousands, of epochs. Early stopping kills the process after it detects that validation loss is going back up. \n",
    "\n",
    "We can put early stopping in place by using a Keras function called a callback, which has odd syntax, but is quite simple to use. The patience pararmeter controls how many epcohs of worsening scores are tolerated before implementing the stop. The restore_best_weights tells the model to roll back all of its weights to the optimal point - so we automatically get the best model post-training. \n",
    "\n",
    "In most cases we probabyl want to use early stopping along with a high epoch number. We can let the model train, and just tell us when it is finished. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "normalizer.adapt(np.array(X_train))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(normalizer)\n",
    "model.add(Dense(18, input_dim=18, activation='relu'))\n",
    "model.add(Dense(18, activation='relu'))\n",
    "model.add(Dense(18, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 812us/step - loss: 72343.2266\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=50, restore_best_weights=True) \n",
    "\n",
    "model.compile(loss='mean_absolute_error', optimizer=tf.optimizers.Adam(learning_rate=.01))\n",
    "train_log = model.fit(X_train, y_train, epochs=1000, batch_size=100, validation_split=.2, verbose=0, callbacks=[callback])\n",
    "model.evaluate(X_test, y_test)\n",
    "plot_loss(train_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "Like other linear models, we can implement regularization to help tame overfitting. \n",
    "\n",
    "We can use both L2 (Ridge) regularization that will limit growth of coefficients, and L1 (Lasso) regularization that is able to eliminate features by shrinking their coefficients to 0. The functionality is the same as we are used to, a regularization term is added to the loss, and the optimization, such as gradient descent, is then performed as normal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization_3 (Normalizat  (None, 18)               37        \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 128)               2432      \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 52,134\n",
      "Trainable params: 52,097\n",
      "Non-trainable params: 37\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Regularization\n",
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "normalizer.adapt(np.array(X_train))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(normalizer)\n",
    "model.add(Dense(128, input_dim=18, activation='relu'))\n",
    "model.add(Dense(128, activation='relu', kernel_regularizer=\"l1\"))\n",
    "model.add(Dense(128, activation='relu', kernel_regularizer=\"l1\"))\n",
    "model.add(Dense(128, activation='relu', kernel_regularizer=\"l1\"))\n",
    "model.add(Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 1ms/step - loss: 68506.4375\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.compile(loss='mean_absolute_error', optimizer=tf.optimizers.Adam(learning_rate=.01))\n",
    "train_log = model.fit(X_train, y_train, epochs=100, batch_size=100, validation_split=.2, verbose=0)\n",
    "model.evaluate(X_test, y_test)\n",
    "plot_loss(train_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "Neural networks also commonly employ a technique call dropouts to prevent overfitting. This works just like the name says, every time the data is moved from one layer to another some portion of the features are randomly held out from being used. \n",
    "\n",
    "The intuitive explanation for dropout is that because individual nodes in the network cannot rely on the output of the others, each node must output features that are useful on their own. This sounds somewhat weird, but is actually effective. The number of features held out is called the dropout rate, typically between .2 and .5. \n",
    "\n",
    "An analogy can be drawn to the bootstrapping we looked at with trees - some random subset of features is selected each time, resulting in each batch getting \"a slightly different look at the data\", thus preventing overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization_7 (Normalizat  (None, 8)                17        \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 512)               4608      \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 793,106\n",
      "Trainable params: 793,089\n",
      "Non-trainable params: 17\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Dropout\n",
    "#Test Different Model Capacities\n",
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "normalizer.adapt(np.array(X_train))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(normalizer)\n",
    "model.add(Dense(512, input_dim=18, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 3ms/step - loss: 0.3811\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.compile(loss='mean_absolute_error', optimizer=tf.optimizers.Adam(learning_rate=.01))\n",
    "train_log = model.fit(X_train, y_train, epochs=1000, batch_size=100, validation_split=.2, verbose=0)\n",
    "model.evaluate(X_test, y_test)\n",
    "plot_loss(train_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "\n",
    "Once the model is trained, using it is mostly familiar to us from the sklearn stuff. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38107612857694406"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(X_test)\n",
    "mean_absolute_error(y_test, preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Use the California data from previously and try to add some regularization things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customized Loss\n",
    "\n",
    "Most scenarios are totally fine with a standard loss function, but what if we have something odd? What if we are playing on The Price is Right? We want to get as close as we can, without going over. We can write a loss function to mirror that!\n",
    "\n",
    "More practically, some real life scenarios have a disperse impact of different types of error. For example, if you are working for a call centre and predicting the number of agents to staff. Having slightly too many may be an error that costs a little bit of money, but not that big of a deal. Predicting too few might incur serious penalties if callers wait and you violate an SLA. Being off in one direction is bad, being off in the other direction can cause you to \"fall off of a cliff\" so to speak. In cases where the impact of the error is not uniform, custom loss functions may make sense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def priceIsRight(y_true, y_pred):\n",
    "    if y_pred <= y_true:\n",
    "        return (y_true - y_pred) ** 2\n",
    "    else:\n",
    "        return (y_true - y_pred) ** 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Exercise\n",
    "\n",
    "Try to use the California data with a customized loss function. \n",
    "\n",
    "Note: this is a 20 way classification, so you'll probably want that many neurons on the output layer, an appropriate activation (softmax), and the y values will need to be run through np_utils.to_categorical. As well, think about the loss function, try categorical crossentropy.\n",
    "\n",
    "We'll look at activation and loss functions more next week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Exercise - Newsgroup Classification\n",
    "\n",
    "Try to classify the newsgroup data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "remove = (\"headers\", \"footers\", \"quotes\")\n",
    "\n",
    "data_train = fetch_20newsgroups(\n",
    "    subset=\"train\", shuffle=True, remove=remove)\n",
    "\n",
    "data_test = fetch_20newsgroups(\n",
    "    subset=\"test\", shuffle=True, remove=remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d722d3adfa415172c1f5238b519fb86b488acdae450fd691ab06c09f4ca9173"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml3950')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
