{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to plot loss\n",
    "def plot_loss(history):\n",
    "  plt.plot(history.history['loss'], label='loss')\n",
    "  plt.plot(history.history['val_loss'], label='val_loss')\n",
    "  plt.legend()\n",
    "  plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras, TensorFlow, and Neural Network Regression\n",
    "\n",
    "As we have seen, neural networks aren't quite as complex as they appear at first, however we still generally don't want to have to build them from scratch very often. The libraries that we will primarily use for creating neural network models are Tensorflow and Keras. \n",
    "\n",
    "### Tensorflow\n",
    "\n",
    "Tensorflow, developed by Google, is one of the most popular libraries for neural networks. \n",
    "\n",
    "### Keras\n",
    "\n",
    "Keras is another package that provides an an API offering an easier to use interface to Tensorflow, allowing us to use it with code that is higher level, avoiding much of the linear math that can make Tensorflow frustrating. Since its introduction Keras has been wrapped in with Tensorflow and the two are normally now blended together as far as we are concerned. \n",
    "\n",
    "### Other Alternatives\n",
    "\n",
    "Keras and Tensorflow are not the only libraries of neural networks, the primary competitor to Tensorflow is PyTorch, which was developed by Facebook. PyTorch does pretty much the same thing as Tensorflow, we won't look at it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21608</th>\n",
       "      <td>360000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1530</td>\n",
       "      <td>1131</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1530</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98103</td>\n",
       "      <td>47.6993</td>\n",
       "      <td>-122.346</td>\n",
       "      <td>1530</td>\n",
       "      <td>1509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21609</th>\n",
       "      <td>400000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2310</td>\n",
       "      <td>5813</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>2310</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98146</td>\n",
       "      <td>47.5107</td>\n",
       "      <td>-122.362</td>\n",
       "      <td>1830</td>\n",
       "      <td>7200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21610</th>\n",
       "      <td>402101.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1020</td>\n",
       "      <td>1350</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1020</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98144</td>\n",
       "      <td>47.5944</td>\n",
       "      <td>-122.299</td>\n",
       "      <td>1020</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21611</th>\n",
       "      <td>400000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1600</td>\n",
       "      <td>2388</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1600</td>\n",
       "      <td>0</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>98027</td>\n",
       "      <td>47.5345</td>\n",
       "      <td>-122.069</td>\n",
       "      <td>1410</td>\n",
       "      <td>1287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21612</th>\n",
       "      <td>325000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1020</td>\n",
       "      <td>1076</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1020</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98144</td>\n",
       "      <td>47.5941</td>\n",
       "      <td>-122.299</td>\n",
       "      <td>1020</td>\n",
       "      <td>1357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          price  bedrooms  bathrooms  sqft_living  sqft_lot  floors  \\\n",
       "21608  360000.0         3       2.50         1530      1131     3.0   \n",
       "21609  400000.0         4       2.50         2310      5813     2.0   \n",
       "21610  402101.0         2       0.75         1020      1350     2.0   \n",
       "21611  400000.0         3       2.50         1600      2388     2.0   \n",
       "21612  325000.0         2       0.75         1020      1076     2.0   \n",
       "\n",
       "       waterfront  view  condition  grade  sqft_above  sqft_basement  \\\n",
       "21608           0     0          3      8        1530              0   \n",
       "21609           0     0          3      8        2310              0   \n",
       "21610           0     0          3      7        1020              0   \n",
       "21611           0     0          3      8        1600              0   \n",
       "21612           0     0          3      7        1020              0   \n",
       "\n",
       "       yr_built  yr_renovated  zipcode      lat     long  sqft_living15  \\\n",
       "21608      2009             0    98103  47.6993 -122.346           1530   \n",
       "21609      2014             0    98146  47.5107 -122.362           1830   \n",
       "21610      2009             0    98144  47.5944 -122.299           1020   \n",
       "21611      2004             0    98027  47.5345 -122.069           1410   \n",
       "21612      2008             0    98144  47.5941 -122.299           1020   \n",
       "\n",
       "       sqft_lot15  \n",
       "21608        1509  \n",
       "21609        7200  \n",
       "21610        2007  \n",
       "21611        1287  \n",
       "21612        1357  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/house_data.csv\")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21613 entries, 0 to 21612\n",
      "Data columns (total 19 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   price          21613 non-null  float64\n",
      " 1   bedrooms       21613 non-null  int64  \n",
      " 2   bathrooms      21613 non-null  float64\n",
      " 3   sqft_living    21613 non-null  int64  \n",
      " 4   sqft_lot       21613 non-null  int64  \n",
      " 5   floors         21613 non-null  float64\n",
      " 6   waterfront     21613 non-null  int64  \n",
      " 7   view           21613 non-null  int64  \n",
      " 8   condition      21613 non-null  int64  \n",
      " 9   grade          21613 non-null  int64  \n",
      " 10  sqft_above     21613 non-null  int64  \n",
      " 11  sqft_basement  21613 non-null  int64  \n",
      " 12  yr_built       21613 non-null  int64  \n",
      " 13  yr_renovated   21613 non-null  int64  \n",
      " 14  zipcode        21613 non-null  int64  \n",
      " 15  lat            21613 non-null  float64\n",
      " 16  long           21613 non-null  float64\n",
      " 17  sqft_living15  21613 non-null  int64  \n",
      " 18  sqft_lot15     21613 non-null  int64  \n",
      "dtypes: float64(5), int64(14)\n",
      "memory usage: 3.1 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21613, 18) (21613, 1)\n"
     ]
    }
   ],
   "source": [
    "y = np.array(df[\"price\"]).reshape(-1,1)\n",
    "X = np.array(df.drop(columns={\"price\"}))\n",
    "print(X.shape, y.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model\n",
    "\n",
    "Creating a NN model is slightly different from the normal process that we are used to in sklearn. We need to do a little more work to set it up. \n",
    "\n",
    "### Create Model and Add Layers\n",
    "\n",
    "First we need to make a NN model, it comes \"empty\". We will use a sequential model, which is the most simple type but is less configurable (which we don't care about much right now). The limitation of sequential models is that they can only take in one tensor and only output one tensor. The other options here are \"functional\", which allows for the structure of the model to be configured, and \"model subclassing\", which allows you to build almost everything from scratch. \n",
    "\n",
    "#### Layers\n",
    "\n",
    "Next we need to add some layers. We will start simple with only two \"thinking\" layers, and one to do some processing. We can think of the layers roughly like steps of the sklearn pipeline, with data entering at the first layer and predictions flowing out of the final layer. \n",
    "\n",
    "In addition to \"normal\" neural network layers, there are many other types that can do all kinds of other stuff. One example we will use here is the normalization one at the front. This layer does exactly what you'd expect - it normalizes our data so the rest of the network can use it. The normalize layer will also automatically handle the 2D nature of the data that we are used to, so we don't need to worry about that aspect here. Other layers can do everything from regularization to image processing, they are also commonly inhierited for developers to create custom layers targeting specific tasks. We'll use a few of the other ones as we move through things. \n",
    "\n",
    "#### Dense Layers\n",
    "\n",
    "We'll use dense layers here. When adding the layer we need to specify a couple of things. One is the input dimensions - we need to tell the network what the shape of the incomming data is. \n",
    "\n",
    "The other argument is the units, which represents the output dimension. When using these Keras dense layers we don't need to specify each layer's input/output like we did when we made it by hand. We specify both, using units and input_dim, for the first layer that takes in the input; for subsequent layers we can just specify the output and Keras will automatically figure the rest out. \n",
    "\n",
    "Note that there is also an input layer that can be added, we can avoid the need for it by using the input_dim or input_shape as shown below. The two examples there do the same thing, since the input is flat - 18 features. If we are dealing with inputs that do not start out as flat, such as in an image, use the input_shape since you can specify all dimensions; we will see an example of this next time with some images. \n",
    "\n",
    "#### Activation Function\n",
    "\n",
    "For each of our layers we need to define which activation function to use. For now we will use the ReLU function, which is probably the most popular. We'll look at other ones later on. \n",
    "\n",
    "Note that we've left the activation function off of the final layer - we are doing regression so we want that raw value.\n",
    "\n",
    "#### Summary\n",
    "\n",
    "After we've constructed the model, the summary command give us, well, a summary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are dealing with a bunch of numerical inputs here, so we can add a normalization layer at the front end. Like with sklearn, we want to fit the normalization to the training data only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizatio  (None, 18)               37        \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 18)                342       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 19        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 398\n",
      "Trainable params: 361\n",
      "Non-trainable params: 37\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "normalizer.adapt(np.array(X_train))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(normalizer)\n",
    "model.add(Dense(18, input_shape=(18,), activation='relu'))\n",
    "#model.add(Dense(18, input_dim=18, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile Model\n",
    "\n",
    "Once a model is created we need to compile it. The complie step basically builds the layers we specified above and the loss and optimization parameters below together into a usable model object. When compiling the model we are providing it with the things it needs to calculate error:\n",
    "\n",
    "<ul>\n",
    "<li> Loss - we can provide a loss function that we'd like to use. \n",
    "<li> Optimizer - the optimizer is the algorithm that the model will use to perform the gradient descent to find the lowest error. Adam is a very common choice.\n",
    "<li> Learning rate - the learning rate is provided as a parameter of the optimizer. \n",
    "</ul>\n",
    "\n",
    "##### Optimizing Adam\n",
    "\n",
    "The optimizer is the algorithm used to perform the gradient descent and minimize error. For the most part this isn't something we need to be concerned about. The choice of optimizer is much more important if dealing with very large datasets because different optimizers have different levels of efficiency. For our purposes, we can use Adam and be pretty happy. Adam stands for Adaptive Moment Estimation which means basically that it will adjust itself depending on current gradients. It tends to be efficient both in time and memory, so it is very commonly used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_absolute_error', optimizer=tf.optimizers.Adam(learning_rate=.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Model\n",
    "\n",
    "The fit command does the same thing that we are used to, it trains the model, however there are some differences. The main difference is that batch_size is almost always set in neural networks, while the sklearn models just take all the data at once. \n",
    "\n",
    "What's a batch? Batches are just subsets of the data, so if the batch size is 100 the algorithm will grab 100 rows at a time before making an update to the weights and bias. There are a few reasons this exists:\n",
    "\n",
    "<ul>\n",
    "<li> Memory constraints - it is common with neural networks to deal with datasets that are extremely large. Processing data that can't fit entirely in RAM is very slow (the computer must swap data from the hard drive to RAM as it is needed) compared to data that is in RAM. Cutting the batch size can avoid this issue. \n",
    "<li> Speed - the math involved in the back propagation can sometimes be very computationally intensive. \n",
    "<li> Accuracy - batch size can have an impact on accuracy, though that impact is not very predictable. For the most part finding an optimal batch size will need to be grid-searched. \n",
    "</ul>\n",
    "\n",
    "The fit command also has the epoch paramater, which instructs on how many times to work through ALL of the data. We want to ensure we have enough epochs to find the optimal solution. \n",
    "\n",
    "#### Plot the Loss\n",
    "\n",
    "One very common visualization we see with neural networks is a plot of both training and validation loss vs number of epochs. Generally we'll see the training loss drop - first sharply as the model initially fits itself, then more slowly as it becomes more fitted. The validation loss will usually somewhat mirror the training loss, except it will often reach a minimum at some point before again increasing. This minimum point is our best model, when the validation loss starts increasing again, that is a sign that the model has become overfitted - customized to the training data, but less and less generalizable to new data. \n",
    "\n",
    "Set the verbosity to 1 in the fit to get a full list of the loss for each epoch to pinpoint the exact \"ideal\" number of epochs. We'll look more at this in a minute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "130/130 [==============================] - 1s 2ms/step - loss: 541852.8125 - val_loss: 534252.4375\n",
      "Epoch 2/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 541502.5625 - val_loss: 533676.5625\n",
      "Epoch 3/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 540709.1250 - val_loss: 532669.7500\n",
      "Epoch 4/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 539499.7500 - val_loss: 531262.5625\n",
      "Epoch 5/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 537898.2500 - val_loss: 529478.0625\n",
      "Epoch 6/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 535927.4375 - val_loss: 527327.5000\n",
      "Epoch 7/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 533602.3750 - val_loss: 524833.1875\n",
      "Epoch 8/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 530938.1250 - val_loss: 522012.5312\n",
      "Epoch 9/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 527951.3750 - val_loss: 518870.3438\n",
      "Epoch 10/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 524652.3125 - val_loss: 515427.7812\n",
      "Epoch 11/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 521052.9688 - val_loss: 511690.3750\n",
      "Epoch 12/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 517164.8125 - val_loss: 507673.0312\n",
      "Epoch 13/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 513000.9375 - val_loss: 503382.9688\n",
      "Epoch 14/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 508569.5312 - val_loss: 498820.4375\n",
      "Epoch 15/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 503879.1562 - val_loss: 494016.0000\n",
      "Epoch 16/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 498945.6250 - val_loss: 488965.8125\n",
      "Epoch 17/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 493779.7188 - val_loss: 483704.8125\n",
      "Epoch 18/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 488395.8750 - val_loss: 478225.7812\n",
      "Epoch 19/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 482798.6562 - val_loss: 472533.9062\n",
      "Epoch 20/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 476991.7812 - val_loss: 466658.2188\n",
      "Epoch 21/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 470994.6562 - val_loss: 460580.7188\n",
      "Epoch 22/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 464823.9062 - val_loss: 454352.9062\n",
      "Epoch 23/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 458484.2188 - val_loss: 447988.1875\n",
      "Epoch 24/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 451996.9688 - val_loss: 441534.6875\n",
      "Epoch 25/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 445398.0000 - val_loss: 435002.9062\n",
      "Epoch 26/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 438689.5000 - val_loss: 428384.2188\n",
      "Epoch 27/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 431925.5625 - val_loss: 421746.5312\n",
      "Epoch 28/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 425121.0312 - val_loss: 415080.0000\n",
      "Epoch 29/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 418280.9688 - val_loss: 408421.1562\n",
      "Epoch 30/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 411472.1250 - val_loss: 401786.5000\n",
      "Epoch 31/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 404719.2500 - val_loss: 395189.4688\n",
      "Epoch 32/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 398011.5000 - val_loss: 388652.3438\n",
      "Epoch 33/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 391371.3438 - val_loss: 382194.3438\n",
      "Epoch 34/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 384793.6562 - val_loss: 375801.2812\n",
      "Epoch 35/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 378247.4062 - val_loss: 369467.5000\n",
      "Epoch 36/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 371764.9062 - val_loss: 363224.4375\n",
      "Epoch 37/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 365372.7500 - val_loss: 357042.5000\n",
      "Epoch 38/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 359071.4062 - val_loss: 350957.5625\n",
      "Epoch 39/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 352840.9688 - val_loss: 344972.2500\n",
      "Epoch 40/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 346733.4062 - val_loss: 339157.7812\n",
      "Epoch 41/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 340700.5625 - val_loss: 333275.9375\n",
      "Epoch 42/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 334612.2500 - val_loss: 327430.5625\n",
      "Epoch 43/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 328603.0312 - val_loss: 321738.8438\n",
      "Epoch 44/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 322693.0938 - val_loss: 316117.4062\n",
      "Epoch 45/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 316911.7812 - val_loss: 310594.8750\n",
      "Epoch 46/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 311282.0938 - val_loss: 305236.0625\n",
      "Epoch 47/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 305836.6562 - val_loss: 300079.0312\n",
      "Epoch 48/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 300604.3750 - val_loss: 295100.7812\n",
      "Epoch 49/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 295573.0312 - val_loss: 290302.0625\n",
      "Epoch 50/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 290757.2500 - val_loss: 285707.3125\n",
      "Epoch 51/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 286176.2500 - val_loss: 281279.4375\n",
      "Epoch 52/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 281794.7500 - val_loss: 277074.5625\n",
      "Epoch 53/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 277584.5938 - val_loss: 273038.6562\n",
      "Epoch 54/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 273549.8750 - val_loss: 269153.3125\n",
      "Epoch 55/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 269676.2812 - val_loss: 265342.0625\n",
      "Epoch 56/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 265976.9375 - val_loss: 261693.7656\n",
      "Epoch 57/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 262411.0625 - val_loss: 258154.9219\n",
      "Epoch 58/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 258959.9375 - val_loss: 254725.9844\n",
      "Epoch 59/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 255614.9688 - val_loss: 251310.5938\n",
      "Epoch 60/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 252359.6875 - val_loss: 248032.5781\n",
      "Epoch 61/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 249203.8438 - val_loss: 244923.8594\n",
      "Epoch 62/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 246181.6250 - val_loss: 241941.0312\n",
      "Epoch 63/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 243306.1094 - val_loss: 239160.1875\n",
      "Epoch 64/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 240571.0156 - val_loss: 236549.2188\n",
      "Epoch 65/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 237973.2188 - val_loss: 234075.9688\n",
      "Epoch 66/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 235512.7500 - val_loss: 231727.4375\n",
      "Epoch 67/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 233188.5000 - val_loss: 229480.5469\n",
      "Epoch 68/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 230977.8594 - val_loss: 227336.7656\n",
      "Epoch 69/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 228863.2969 - val_loss: 225284.4688\n",
      "Epoch 70/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 226819.4219 - val_loss: 223316.0000\n",
      "Epoch 71/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 224835.8906 - val_loss: 221426.5781\n",
      "Epoch 72/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 222905.2031 - val_loss: 219585.1406\n",
      "Epoch 73/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 221022.1562 - val_loss: 217785.1875\n",
      "Epoch 74/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 219172.9844 - val_loss: 216043.2188\n",
      "Epoch 75/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 217355.0000 - val_loss: 214328.3750\n",
      "Epoch 76/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 215554.5000 - val_loss: 212634.9219\n",
      "Epoch 77/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 213769.0625 - val_loss: 210965.8125\n",
      "Epoch 78/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 212023.4688 - val_loss: 209340.2344\n",
      "Epoch 79/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 210298.5938 - val_loss: 207715.4531\n",
      "Epoch 80/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 208612.1406 - val_loss: 206129.9219\n",
      "Epoch 81/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 206972.5000 - val_loss: 204554.3281\n",
      "Epoch 82/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 205369.3281 - val_loss: 202984.6094\n",
      "Epoch 83/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 203801.0312 - val_loss: 201432.2344\n",
      "Epoch 84/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 202266.5156 - val_loss: 199912.0781\n",
      "Epoch 85/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 200746.6094 - val_loss: 198427.2812\n",
      "Epoch 86/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 199261.4531 - val_loss: 196937.2812\n",
      "Epoch 87/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 197803.2031 - val_loss: 195478.7812\n",
      "Epoch 88/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 196378.2188 - val_loss: 194032.4219\n",
      "Epoch 89/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 194969.0781 - val_loss: 192616.5469\n",
      "Epoch 90/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 193573.5938 - val_loss: 191230.4062\n",
      "Epoch 91/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 192200.3438 - val_loss: 189855.4844\n",
      "Epoch 92/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 190837.0000 - val_loss: 188505.1875\n",
      "Epoch 93/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 189493.1719 - val_loss: 187184.1250\n",
      "Epoch 94/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 188196.6406 - val_loss: 185877.8906\n",
      "Epoch 95/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 186920.8281 - val_loss: 184584.8906\n",
      "Epoch 96/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 185663.6094 - val_loss: 183325.4531\n",
      "Epoch 97/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 184441.9219 - val_loss: 182119.8438\n",
      "Epoch 98/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 183242.6094 - val_loss: 180945.9688\n",
      "Epoch 99/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 182073.3750 - val_loss: 179780.5469\n",
      "Epoch 100/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 180943.2969 - val_loss: 178634.8125\n",
      "Epoch 101/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 179836.7344 - val_loss: 177507.9688\n",
      "Epoch 102/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 178750.6875 - val_loss: 176381.2812\n",
      "Epoch 103/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 177676.3750 - val_loss: 175272.4688\n",
      "Epoch 104/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 176607.9375 - val_loss: 174180.1406\n",
      "Epoch 105/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 175552.1406 - val_loss: 173091.1719\n",
      "Epoch 106/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 174518.5000 - val_loss: 172021.5156\n",
      "Epoch 107/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 173491.5938 - val_loss: 170967.1406\n",
      "Epoch 108/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 172483.4375 - val_loss: 169899.0781\n",
      "Epoch 109/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 171484.2812 - val_loss: 168850.3594\n",
      "Epoch 110/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 170488.2812 - val_loss: 167811.6875\n",
      "Epoch 111/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 169493.4062 - val_loss: 166778.7344\n",
      "Epoch 112/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 168506.6094 - val_loss: 165779.2812\n",
      "Epoch 113/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 167530.1094 - val_loss: 164783.9531\n",
      "Epoch 114/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 166568.1094 - val_loss: 163783.0625\n",
      "Epoch 115/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 165604.6094 - val_loss: 162804.5625\n",
      "Epoch 116/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 164648.9531 - val_loss: 161811.1875\n",
      "Epoch 117/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 163690.6250 - val_loss: 160827.4688\n",
      "Epoch 118/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 162739.8906 - val_loss: 159857.0312\n",
      "Epoch 119/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 161796.5156 - val_loss: 158890.8438\n",
      "Epoch 120/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 160855.7500 - val_loss: 157941.6250\n",
      "Epoch 121/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 159911.9062 - val_loss: 156990.7031\n",
      "Epoch 122/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 158972.5000 - val_loss: 156061.9688\n",
      "Epoch 123/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 158045.0781 - val_loss: 155150.4844\n",
      "Epoch 124/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 157122.6719 - val_loss: 154243.2344\n",
      "Epoch 125/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 156208.2969 - val_loss: 153330.3125\n",
      "Epoch 126/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 155307.7344 - val_loss: 152433.8594\n",
      "Epoch 127/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 154418.1562 - val_loss: 151550.0781\n",
      "Epoch 128/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 153540.2656 - val_loss: 150666.6250\n",
      "Epoch 129/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 152671.8438 - val_loss: 149801.9844\n",
      "Epoch 130/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 151810.7812 - val_loss: 148950.2812\n",
      "Epoch 131/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 150958.3125 - val_loss: 148110.0781\n",
      "Epoch 132/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 150121.2344 - val_loss: 147291.2188\n",
      "Epoch 133/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 149302.2188 - val_loss: 146473.3438\n",
      "Epoch 134/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 148497.2188 - val_loss: 145680.1250\n",
      "Epoch 135/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 147717.1875 - val_loss: 144898.9531\n",
      "Epoch 136/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 146952.2031 - val_loss: 144134.3594\n",
      "Epoch 137/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 146205.6875 - val_loss: 143379.7969\n",
      "Epoch 138/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 145474.2188 - val_loss: 142642.1094\n",
      "Epoch 139/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 144760.2188 - val_loss: 141920.6875\n",
      "Epoch 140/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 144059.2031 - val_loss: 141212.2500\n",
      "Epoch 141/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 143371.8125 - val_loss: 140537.0938\n",
      "Epoch 142/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 142708.3281 - val_loss: 139860.1875\n",
      "Epoch 143/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 142055.2188 - val_loss: 139227.6250\n",
      "Epoch 144/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 141425.1875 - val_loss: 138595.1406\n",
      "Epoch 145/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 140811.5469 - val_loss: 137981.9688\n",
      "Epoch 146/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 140225.3906 - val_loss: 137384.9844\n",
      "Epoch 147/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 139656.2188 - val_loss: 136806.6562\n",
      "Epoch 148/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 139106.5938 - val_loss: 136239.7969\n",
      "Epoch 149/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 138572.8750 - val_loss: 135688.0781\n",
      "Epoch 150/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 138047.7969 - val_loss: 135150.3125\n",
      "Epoch 151/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 137542.6406 - val_loss: 134631.0312\n",
      "Epoch 152/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 137051.8125 - val_loss: 134130.1094\n",
      "Epoch 153/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 136577.7969 - val_loss: 133618.7969\n",
      "Epoch 154/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 136109.9062 - val_loss: 133132.5625\n",
      "Epoch 155/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 135658.4531 - val_loss: 132653.6406\n",
      "Epoch 156/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 135213.3438 - val_loss: 132186.6250\n",
      "Epoch 157/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 134775.9844 - val_loss: 131733.1875\n",
      "Epoch 158/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 134349.9688 - val_loss: 131285.0469\n",
      "Epoch 159/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 133929.3438 - val_loss: 130846.2422\n",
      "Epoch 160/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 133512.8125 - val_loss: 130412.6172\n",
      "Epoch 161/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 133112.7812 - val_loss: 130000.3750\n",
      "Epoch 162/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 132721.2969 - val_loss: 129589.4219\n",
      "Epoch 163/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 132338.9375 - val_loss: 129195.7969\n",
      "Epoch 164/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 131969.2656 - val_loss: 128809.1797\n",
      "Epoch 165/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 131595.0000 - val_loss: 128430.3594\n",
      "Epoch 166/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 131236.3125 - val_loss: 128057.1406\n",
      "Epoch 167/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 130878.9375 - val_loss: 127695.9922\n",
      "Epoch 168/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 130532.5938 - val_loss: 127339.3828\n",
      "Epoch 169/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 130189.8516 - val_loss: 126986.7266\n",
      "Epoch 170/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 129848.5547 - val_loss: 126644.2031\n",
      "Epoch 171/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 129522.1797 - val_loss: 126314.9062\n",
      "Epoch 172/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 129194.4844 - val_loss: 125989.8594\n",
      "Epoch 173/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 128875.6172 - val_loss: 125670.2188\n",
      "Epoch 174/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 128557.2969 - val_loss: 125360.3984\n",
      "Epoch 175/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 128245.8047 - val_loss: 125052.4766\n",
      "Epoch 176/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 127936.3828 - val_loss: 124764.8281\n",
      "Epoch 177/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 127630.5312 - val_loss: 124464.6719\n",
      "Epoch 178/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 127325.3906 - val_loss: 124169.5781\n",
      "Epoch 179/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 127030.3594 - val_loss: 123877.1406\n",
      "Epoch 180/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 126732.5391 - val_loss: 123591.4844\n",
      "Epoch 181/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 126444.6328 - val_loss: 123322.4609\n",
      "Epoch 182/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 126163.1719 - val_loss: 123051.5156\n",
      "Epoch 183/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 125884.4922 - val_loss: 122786.1641\n",
      "Epoch 184/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 125605.4219 - val_loss: 122525.6484\n",
      "Epoch 185/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 125329.6797 - val_loss: 122260.3594\n",
      "Epoch 186/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 125062.6172 - val_loss: 122014.1875\n",
      "Epoch 187/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 124797.4844 - val_loss: 121760.6016\n",
      "Epoch 188/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 124531.9453 - val_loss: 121511.0938\n",
      "Epoch 189/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 124270.1719 - val_loss: 121270.3281\n",
      "Epoch 190/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 124015.2422 - val_loss: 121012.6719\n",
      "Epoch 191/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 123751.7891 - val_loss: 120777.1016\n",
      "Epoch 192/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 123499.7109 - val_loss: 120532.6953\n",
      "Epoch 193/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 123252.3594 - val_loss: 120292.0938\n",
      "Epoch 194/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 123003.2656 - val_loss: 120063.9375\n",
      "Epoch 195/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 122760.4219 - val_loss: 119821.8125\n",
      "Epoch 196/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 122513.7188 - val_loss: 119601.9219\n",
      "Epoch 197/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 122275.7969 - val_loss: 119367.9297\n",
      "Epoch 198/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 122041.8672 - val_loss: 119142.6328\n",
      "Epoch 199/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 121808.0469 - val_loss: 118924.8125\n",
      "Epoch 200/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 121577.2188 - val_loss: 118716.0625\n",
      "Epoch 201/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 121349.6250 - val_loss: 118509.4062\n",
      "Epoch 202/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 121128.8438 - val_loss: 118311.4141\n",
      "Epoch 203/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 120910.3281 - val_loss: 118108.8828\n",
      "Epoch 204/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 120697.2656 - val_loss: 117914.9688\n",
      "Epoch 205/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 120488.6562 - val_loss: 117722.7891\n",
      "Epoch 206/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 120281.5312 - val_loss: 117537.2031\n",
      "Epoch 207/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 120079.3516 - val_loss: 117355.1719\n",
      "Epoch 208/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 119876.9297 - val_loss: 117175.2266\n",
      "Epoch 209/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 119678.6016 - val_loss: 116991.4453\n",
      "Epoch 210/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 119482.9688 - val_loss: 116822.7188\n",
      "Epoch 211/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 119289.5547 - val_loss: 116657.7500\n",
      "Epoch 212/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 119097.8750 - val_loss: 116486.4141\n",
      "Epoch 213/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 118912.7578 - val_loss: 116312.2734\n",
      "Epoch 214/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 118732.3125 - val_loss: 116150.9219\n",
      "Epoch 215/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 118549.5312 - val_loss: 115980.5547\n",
      "Epoch 216/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 118374.5781 - val_loss: 115837.7969\n",
      "Epoch 217/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 118204.3203 - val_loss: 115684.8594\n",
      "Epoch 218/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 118037.2109 - val_loss: 115527.5234\n",
      "Epoch 219/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 117866.2656 - val_loss: 115376.5859\n",
      "Epoch 220/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 117703.3750 - val_loss: 115230.6875\n",
      "Epoch 221/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 117546.8828 - val_loss: 115085.0469\n",
      "Epoch 222/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 117382.7031 - val_loss: 114942.0781\n",
      "Epoch 223/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 117226.0234 - val_loss: 114805.4062\n",
      "Epoch 224/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 117070.2500 - val_loss: 114666.3359\n",
      "Epoch 225/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 116920.2266 - val_loss: 114539.5781\n",
      "Epoch 226/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 116774.2578 - val_loss: 114405.7266\n",
      "Epoch 227/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 116625.0703 - val_loss: 114278.2891\n",
      "Epoch 228/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 116475.5156 - val_loss: 114155.2812\n",
      "Epoch 229/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 116327.0234 - val_loss: 114027.6172\n",
      "Epoch 230/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 116183.7812 - val_loss: 113900.4531\n",
      "Epoch 231/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 116039.0938 - val_loss: 113782.0312\n",
      "Epoch 232/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 115897.9609 - val_loss: 113669.9219\n",
      "Epoch 233/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 115756.4609 - val_loss: 113560.4453\n",
      "Epoch 234/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 115624.8438 - val_loss: 113438.8438\n",
      "Epoch 235/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 115485.6250 - val_loss: 113331.3438\n",
      "Epoch 236/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 115354.5469 - val_loss: 113220.4844\n",
      "Epoch 237/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 115221.9766 - val_loss: 113114.1094\n",
      "Epoch 238/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 115096.3047 - val_loss: 113008.4453\n",
      "Epoch 239/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 114969.1328 - val_loss: 112912.3125\n",
      "Epoch 240/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 114846.2969 - val_loss: 112805.5938\n",
      "Epoch 241/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 114728.1562 - val_loss: 112710.1875\n",
      "Epoch 242/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 114615.1797 - val_loss: 112619.4375\n",
      "Epoch 243/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 114502.4297 - val_loss: 112528.0859\n",
      "Epoch 244/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 114396.8281 - val_loss: 112445.8750\n",
      "Epoch 245/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 114292.9609 - val_loss: 112353.8750\n",
      "Epoch 246/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 114187.2812 - val_loss: 112270.6016\n",
      "Epoch 247/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 114079.9531 - val_loss: 112189.8672\n",
      "Epoch 248/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 113984.7969 - val_loss: 112107.2812\n",
      "Epoch 249/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 113882.8047 - val_loss: 112022.8672\n",
      "Epoch 250/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 113782.4219 - val_loss: 111947.6172\n",
      "Epoch 251/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 113689.2891 - val_loss: 111872.1953\n",
      "Epoch 252/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 113594.4375 - val_loss: 111795.7969\n",
      "Epoch 253/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 113505.7188 - val_loss: 111723.4688\n",
      "Epoch 254/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 113419.6562 - val_loss: 111651.4922\n",
      "Epoch 255/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 113331.1719 - val_loss: 111577.2969\n",
      "Epoch 256/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 113247.0234 - val_loss: 111512.6172\n",
      "Epoch 257/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 113165.7344 - val_loss: 111442.1250\n",
      "Epoch 258/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 113085.5703 - val_loss: 111374.0938\n",
      "Epoch 259/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 113003.2031 - val_loss: 111310.3047\n",
      "Epoch 260/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 112930.8750 - val_loss: 111246.6797\n",
      "Epoch 261/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 112853.9219 - val_loss: 111191.5781\n",
      "Epoch 262/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 112784.2500 - val_loss: 111121.0078\n",
      "Epoch 263/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 112717.4609 - val_loss: 111064.7266\n",
      "Epoch 264/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 112642.5000 - val_loss: 111006.9844\n",
      "Epoch 265/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 112576.0469 - val_loss: 110949.7031\n",
      "Epoch 266/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 112509.3359 - val_loss: 110889.5938\n",
      "Epoch 267/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 112444.6953 - val_loss: 110834.9922\n",
      "Epoch 268/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 112381.8906 - val_loss: 110783.3750\n",
      "Epoch 269/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 112319.0859 - val_loss: 110732.4297\n",
      "Epoch 270/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 112260.7812 - val_loss: 110680.2891\n",
      "Epoch 271/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 112196.6250 - val_loss: 110616.6875\n",
      "Epoch 272/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 112138.7188 - val_loss: 110568.4062\n",
      "Epoch 273/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 112077.9766 - val_loss: 110517.1719\n",
      "Epoch 274/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 112019.6016 - val_loss: 110471.6562\n",
      "Epoch 275/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 111965.9922 - val_loss: 110418.7109\n",
      "Epoch 276/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 111909.4297 - val_loss: 110374.2812\n",
      "Epoch 277/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 111854.6875 - val_loss: 110323.1094\n",
      "Epoch 278/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 111797.3516 - val_loss: 110273.6016\n",
      "Epoch 279/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 111741.3672 - val_loss: 110228.0000\n",
      "Epoch 280/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 111688.3672 - val_loss: 110179.4609\n",
      "Epoch 281/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 111632.9609 - val_loss: 110127.8906\n",
      "Epoch 282/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 111577.8359 - val_loss: 110086.4922\n",
      "Epoch 283/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 111528.3828 - val_loss: 110041.9922\n",
      "Epoch 284/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 111476.0781 - val_loss: 109997.5781\n",
      "Epoch 285/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 111424.9453 - val_loss: 109954.3281\n",
      "Epoch 286/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 111372.2109 - val_loss: 109906.8906\n",
      "Epoch 287/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 111320.2422 - val_loss: 109864.2344\n",
      "Epoch 288/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 111271.3359 - val_loss: 109824.0469\n",
      "Epoch 289/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 111226.3438 - val_loss: 109778.5625\n",
      "Epoch 290/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 111174.4219 - val_loss: 109740.8516\n",
      "Epoch 291/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 111123.9609 - val_loss: 109707.3672\n",
      "Epoch 292/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 111076.2812 - val_loss: 109663.7109\n",
      "Epoch 293/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 111031.7500 - val_loss: 109627.8359\n",
      "Epoch 294/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110987.0234 - val_loss: 109590.7344\n",
      "Epoch 295/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110937.1172 - val_loss: 109546.8750\n",
      "Epoch 296/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110893.3594 - val_loss: 109509.1797\n",
      "Epoch 297/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110848.8750 - val_loss: 109474.2344\n",
      "Epoch 298/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110804.2188 - val_loss: 109443.6406\n",
      "Epoch 299/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110757.6172 - val_loss: 109397.5312\n",
      "Epoch 300/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110716.7812 - val_loss: 109365.4297\n",
      "Epoch 301/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110668.2188 - val_loss: 109333.0859\n",
      "Epoch 302/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110625.3359 - val_loss: 109295.0547\n",
      "Epoch 303/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110583.6406 - val_loss: 109258.7578\n",
      "Epoch 304/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110542.9375 - val_loss: 109223.4297\n",
      "Epoch 305/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110498.1562 - val_loss: 109184.3359\n",
      "Epoch 306/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110460.3750 - val_loss: 109152.1641\n",
      "Epoch 307/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110419.7266 - val_loss: 109119.2422\n",
      "Epoch 308/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110378.0391 - val_loss: 109086.7969\n",
      "Epoch 309/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110342.8594 - val_loss: 109053.7422\n",
      "Epoch 310/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110303.1250 - val_loss: 109027.4375\n",
      "Epoch 311/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110261.8047 - val_loss: 108989.7188\n",
      "Epoch 312/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 110229.0469 - val_loss: 108956.1016\n",
      "Epoch 313/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110189.7344 - val_loss: 108926.7891\n",
      "Epoch 314/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110151.4766 - val_loss: 108893.0078\n",
      "Epoch 315/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110115.8984 - val_loss: 108860.8125\n",
      "Epoch 316/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110080.1484 - val_loss: 108828.8047\n",
      "Epoch 317/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110040.2812 - val_loss: 108795.8672\n",
      "Epoch 318/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 110006.0234 - val_loss: 108766.0078\n",
      "Epoch 319/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109969.6562 - val_loss: 108739.4766\n",
      "Epoch 320/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109936.1875 - val_loss: 108710.7109\n",
      "Epoch 321/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109897.4453 - val_loss: 108685.2969\n",
      "Epoch 322/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109863.1406 - val_loss: 108651.7422\n",
      "Epoch 323/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109826.1094 - val_loss: 108622.8359\n",
      "Epoch 324/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109791.5703 - val_loss: 108591.1719\n",
      "Epoch 325/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109754.8984 - val_loss: 108571.5859\n",
      "Epoch 326/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109722.2734 - val_loss: 108532.8984\n",
      "Epoch 327/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109687.1953 - val_loss: 108502.5703\n",
      "Epoch 328/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109651.0156 - val_loss: 108476.4141\n",
      "Epoch 329/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109617.7812 - val_loss: 108452.4219\n",
      "Epoch 330/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109582.8438 - val_loss: 108421.4844\n",
      "Epoch 331/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109551.0547 - val_loss: 108400.6875\n",
      "Epoch 332/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109517.1953 - val_loss: 108371.5234\n",
      "Epoch 333/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109485.8203 - val_loss: 108345.2656\n",
      "Epoch 334/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109450.1641 - val_loss: 108319.9297\n",
      "Epoch 335/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109414.5781 - val_loss: 108290.1875\n",
      "Epoch 336/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109384.5859 - val_loss: 108256.7812\n",
      "Epoch 337/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109351.3672 - val_loss: 108231.5078\n",
      "Epoch 338/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109315.4922 - val_loss: 108204.9688\n",
      "Epoch 339/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109280.0469 - val_loss: 108179.0078\n",
      "Epoch 340/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109250.5781 - val_loss: 108149.8438\n",
      "Epoch 341/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109216.6016 - val_loss: 108126.4609\n",
      "Epoch 342/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109185.8672 - val_loss: 108097.1953\n",
      "Epoch 343/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109150.7969 - val_loss: 108069.7812\n",
      "Epoch 344/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109118.1953 - val_loss: 108047.3750\n",
      "Epoch 345/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109086.7578 - val_loss: 108017.0391\n",
      "Epoch 346/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109054.4375 - val_loss: 107989.3672\n",
      "Epoch 347/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 109022.7500 - val_loss: 107969.2969\n",
      "Epoch 348/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108991.5703 - val_loss: 107940.6328\n",
      "Epoch 349/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108965.7344 - val_loss: 107918.1172\n",
      "Epoch 350/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108934.1953 - val_loss: 107885.8672\n",
      "Epoch 351/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108902.8594 - val_loss: 107859.7969\n",
      "Epoch 352/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108872.7734 - val_loss: 107835.9609\n",
      "Epoch 353/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108848.9688 - val_loss: 107805.4922\n",
      "Epoch 354/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108813.1797 - val_loss: 107779.9844\n",
      "Epoch 355/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108782.5859 - val_loss: 107756.1875\n",
      "Epoch 356/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108752.3594 - val_loss: 107728.2969\n",
      "Epoch 357/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108724.1094 - val_loss: 107698.8359\n",
      "Epoch 358/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108694.5078 - val_loss: 107670.7969\n",
      "Epoch 359/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108664.1406 - val_loss: 107645.9531\n",
      "Epoch 360/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108643.1953 - val_loss: 107617.5859\n",
      "Epoch 361/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108608.3516 - val_loss: 107587.1953\n",
      "Epoch 362/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108582.6484 - val_loss: 107568.4531\n",
      "Epoch 363/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108554.9375 - val_loss: 107534.0625\n",
      "Epoch 364/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108523.4375 - val_loss: 107519.8906\n",
      "Epoch 365/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108499.3516 - val_loss: 107490.3125\n",
      "Epoch 366/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108469.2578 - val_loss: 107470.0078\n",
      "Epoch 367/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108440.4375 - val_loss: 107439.4375\n",
      "Epoch 368/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108415.5156 - val_loss: 107412.2812\n",
      "Epoch 369/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 108388.2500 - val_loss: 107390.0469\n",
      "Epoch 370/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108363.5859 - val_loss: 107358.4766\n",
      "Epoch 371/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 108335.8594 - val_loss: 107335.6094\n",
      "Epoch 372/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 108307.7734 - val_loss: 107307.7188\n",
      "Epoch 373/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 108284.8125 - val_loss: 107284.8750\n",
      "Epoch 374/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108259.6719 - val_loss: 107262.8906\n",
      "Epoch 375/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 108232.1016 - val_loss: 107238.5625\n",
      "Epoch 376/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 108204.3750 - val_loss: 107211.7734\n",
      "Epoch 377/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 108180.1953 - val_loss: 107186.7500\n",
      "Epoch 378/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 108157.6641 - val_loss: 107160.6172\n",
      "Epoch 379/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108130.1484 - val_loss: 107137.3828\n",
      "Epoch 380/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108106.0938 - val_loss: 107115.3281\n",
      "Epoch 381/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108080.1016 - val_loss: 107088.6406\n",
      "Epoch 382/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108054.5859 - val_loss: 107066.4141\n",
      "Epoch 383/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108030.3828 - val_loss: 107041.6797\n",
      "Epoch 384/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 108004.7734 - val_loss: 107011.3672\n",
      "Epoch 385/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107980.1172 - val_loss: 106987.0938\n",
      "Epoch 386/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107956.9766 - val_loss: 106973.4531\n",
      "Epoch 387/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107931.3203 - val_loss: 106941.1953\n",
      "Epoch 388/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107909.3125 - val_loss: 106912.5625\n",
      "Epoch 389/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107884.2656 - val_loss: 106891.9844\n",
      "Epoch 390/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107861.4453 - val_loss: 106865.9219\n",
      "Epoch 391/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107839.3906 - val_loss: 106845.1172\n",
      "Epoch 392/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 107811.8125 - val_loss: 106818.3672\n",
      "Epoch 393/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 107791.6328 - val_loss: 106793.1406\n",
      "Epoch 394/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107768.9062 - val_loss: 106775.0312\n",
      "Epoch 395/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107741.3750 - val_loss: 106751.6719\n",
      "Epoch 396/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107717.8516 - val_loss: 106721.9922\n",
      "Epoch 397/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107700.0234 - val_loss: 106707.1719\n",
      "Epoch 398/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107675.2969 - val_loss: 106676.7109\n",
      "Epoch 399/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107650.5078 - val_loss: 106653.6797\n",
      "Epoch 400/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107626.8750 - val_loss: 106626.9531\n",
      "Epoch 401/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107606.4141 - val_loss: 106602.7422\n",
      "Epoch 402/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107584.5078 - val_loss: 106572.5781\n",
      "Epoch 403/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107565.9375 - val_loss: 106550.1016\n",
      "Epoch 404/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107542.6953 - val_loss: 106523.2031\n",
      "Epoch 405/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107521.9531 - val_loss: 106504.3203\n",
      "Epoch 406/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107497.2109 - val_loss: 106472.0547\n",
      "Epoch 407/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107477.9766 - val_loss: 106446.7578\n",
      "Epoch 408/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107457.8281 - val_loss: 106429.5781\n",
      "Epoch 409/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 107430.5156 - val_loss: 106405.6328\n",
      "Epoch 410/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107410.9609 - val_loss: 106382.5469\n",
      "Epoch 411/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107389.8672 - val_loss: 106360.5312\n",
      "Epoch 412/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107372.7891 - val_loss: 106338.5781\n",
      "Epoch 413/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107352.4062 - val_loss: 106319.5000\n",
      "Epoch 414/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107332.8203 - val_loss: 106293.4219\n",
      "Epoch 415/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107313.8125 - val_loss: 106276.5078\n",
      "Epoch 416/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107293.5938 - val_loss: 106252.4844\n",
      "Epoch 417/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107275.0156 - val_loss: 106230.7500\n",
      "Epoch 418/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107256.2500 - val_loss: 106214.5234\n",
      "Epoch 419/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107237.2422 - val_loss: 106191.7656\n",
      "Epoch 420/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107218.9609 - val_loss: 106170.0391\n",
      "Epoch 421/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107199.8672 - val_loss: 106153.0312\n",
      "Epoch 422/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107181.2812 - val_loss: 106128.7188\n",
      "Epoch 423/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107164.1719 - val_loss: 106114.8359\n",
      "Epoch 424/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107145.2031 - val_loss: 106093.5391\n",
      "Epoch 425/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107127.5625 - val_loss: 106069.1250\n",
      "Epoch 426/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107110.3906 - val_loss: 106056.6484\n",
      "Epoch 427/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107092.6250 - val_loss: 106039.1797\n",
      "Epoch 428/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107077.7188 - val_loss: 106018.4062\n",
      "Epoch 429/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107058.9141 - val_loss: 106001.1562\n",
      "Epoch 430/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107037.6406 - val_loss: 105978.0781\n",
      "Epoch 431/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107020.1641 - val_loss: 105959.7188\n",
      "Epoch 432/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 107003.7969 - val_loss: 105941.9453\n",
      "Epoch 433/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106986.4297 - val_loss: 105924.8750\n",
      "Epoch 434/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106971.0391 - val_loss: 105907.1484\n",
      "Epoch 435/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106950.1328 - val_loss: 105885.4844\n",
      "Epoch 436/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106933.4453 - val_loss: 105863.2188\n",
      "Epoch 437/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106914.8125 - val_loss: 105847.3906\n",
      "Epoch 438/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106898.0703 - val_loss: 105827.9219\n",
      "Epoch 439/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106881.5703 - val_loss: 105809.0078\n",
      "Epoch 440/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106863.5156 - val_loss: 105788.5859\n",
      "Epoch 441/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 106846.6641 - val_loss: 105773.4844\n",
      "Epoch 442/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106831.4453 - val_loss: 105762.1875\n",
      "Epoch 443/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106813.3359 - val_loss: 105737.8516\n",
      "Epoch 444/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 106793.0547 - val_loss: 105719.1953\n",
      "Epoch 445/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 106774.0000 - val_loss: 105701.2969\n",
      "Epoch 446/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106760.2500 - val_loss: 105681.0000\n",
      "Epoch 447/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106741.9219 - val_loss: 105660.2188\n",
      "Epoch 448/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106724.7969 - val_loss: 105641.1172\n",
      "Epoch 449/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106706.2109 - val_loss: 105617.3750\n",
      "Epoch 450/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106690.1172 - val_loss: 105600.9609\n",
      "Epoch 451/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106671.9609 - val_loss: 105588.4922\n",
      "Epoch 452/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106654.9297 - val_loss: 105564.4844\n",
      "Epoch 453/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106635.9844 - val_loss: 105549.9062\n",
      "Epoch 454/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106620.3984 - val_loss: 105530.8359\n",
      "Epoch 455/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106610.2188 - val_loss: 105515.1562\n",
      "Epoch 456/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106588.5000 - val_loss: 105495.3906\n",
      "Epoch 457/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106575.7891 - val_loss: 105480.8906\n",
      "Epoch 458/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106551.4375 - val_loss: 105462.1328\n",
      "Epoch 459/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106540.3984 - val_loss: 105434.0781\n",
      "Epoch 460/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106523.8750 - val_loss: 105421.6875\n",
      "Epoch 461/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106508.4297 - val_loss: 105401.4297\n",
      "Epoch 462/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106490.9141 - val_loss: 105390.0391\n",
      "Epoch 463/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106470.2188 - val_loss: 105365.9297\n",
      "Epoch 464/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106457.6719 - val_loss: 105347.4141\n",
      "Epoch 465/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106441.7266 - val_loss: 105330.0469\n",
      "Epoch 466/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106425.3594 - val_loss: 105313.3984\n",
      "Epoch 467/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 106407.3828 - val_loss: 105305.5938\n",
      "Epoch 468/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106395.6953 - val_loss: 105287.2812\n",
      "Epoch 469/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106381.6250 - val_loss: 105266.7656\n",
      "Epoch 470/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 106363.0078 - val_loss: 105250.2812\n",
      "Epoch 471/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 106348.1875 - val_loss: 105242.3281\n",
      "Epoch 472/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106336.2344 - val_loss: 105221.7656\n",
      "Epoch 473/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 106320.2734 - val_loss: 105209.7812\n",
      "Epoch 474/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 106308.6719 - val_loss: 105196.8984\n",
      "Epoch 475/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106289.7188 - val_loss: 105178.9141\n",
      "Epoch 476/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106275.4609 - val_loss: 105154.0547\n",
      "Epoch 477/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106260.2656 - val_loss: 105140.3594\n",
      "Epoch 478/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106244.2344 - val_loss: 105122.3672\n",
      "Epoch 479/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106227.2734 - val_loss: 105111.6094\n",
      "Epoch 480/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106213.6797 - val_loss: 105094.5234\n",
      "Epoch 481/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106202.7891 - val_loss: 105078.6094\n",
      "Epoch 482/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106186.7656 - val_loss: 105062.5547\n",
      "Epoch 483/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106169.9531 - val_loss: 105044.8750\n",
      "Epoch 484/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106155.7109 - val_loss: 105029.8828\n",
      "Epoch 485/1000\n",
      "130/130 [==============================] - 1s 4ms/step - loss: 106139.7266 - val_loss: 105011.4609\n",
      "Epoch 486/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 106123.3047 - val_loss: 104998.5156\n",
      "Epoch 487/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 106112.3125 - val_loss: 104975.9375\n",
      "Epoch 488/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106094.2422 - val_loss: 104969.5469\n",
      "Epoch 489/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106080.9766 - val_loss: 104954.0781\n",
      "Epoch 490/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106064.0391 - val_loss: 104933.0078\n",
      "Epoch 491/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106050.6875 - val_loss: 104912.6719\n",
      "Epoch 492/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106031.9062 - val_loss: 104898.4219\n",
      "Epoch 493/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106022.1953 - val_loss: 104883.9219\n",
      "Epoch 494/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 106007.5156 - val_loss: 104864.3672\n",
      "Epoch 495/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105988.7578 - val_loss: 104852.8047\n",
      "Epoch 496/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105973.4375 - val_loss: 104841.8906\n",
      "Epoch 497/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105959.4766 - val_loss: 104822.4375\n",
      "Epoch 498/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105944.7656 - val_loss: 104807.3359\n",
      "Epoch 499/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105930.5938 - val_loss: 104789.2812\n",
      "Epoch 500/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105916.4844 - val_loss: 104772.1641\n",
      "Epoch 501/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105899.1094 - val_loss: 104753.9219\n",
      "Epoch 502/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105885.2500 - val_loss: 104742.4141\n",
      "Epoch 503/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105870.5312 - val_loss: 104721.9453\n",
      "Epoch 504/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105859.4609 - val_loss: 104709.9219\n",
      "Epoch 505/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105842.1484 - val_loss: 104690.3672\n",
      "Epoch 506/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105830.5781 - val_loss: 104674.8594\n",
      "Epoch 507/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105815.3984 - val_loss: 104664.7969\n",
      "Epoch 508/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105805.2344 - val_loss: 104646.3047\n",
      "Epoch 509/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105785.7344 - val_loss: 104631.2344\n",
      "Epoch 510/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105771.0859 - val_loss: 104612.8203\n",
      "Epoch 511/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105760.3047 - val_loss: 104600.4062\n",
      "Epoch 512/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105745.7578 - val_loss: 104588.3516\n",
      "Epoch 513/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105728.7422 - val_loss: 104570.2188\n",
      "Epoch 514/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105714.5938 - val_loss: 104558.2266\n",
      "Epoch 515/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105702.1406 - val_loss: 104538.4609\n",
      "Epoch 516/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105688.1328 - val_loss: 104524.8047\n",
      "Epoch 517/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105671.9922 - val_loss: 104508.8594\n",
      "Epoch 518/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105663.8438 - val_loss: 104499.3750\n",
      "Epoch 519/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105646.7500 - val_loss: 104488.6328\n",
      "Epoch 520/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105633.0859 - val_loss: 104473.1016\n",
      "Epoch 521/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105622.6328 - val_loss: 104456.3281\n",
      "Epoch 522/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105612.6641 - val_loss: 104443.2031\n",
      "Epoch 523/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105593.8828 - val_loss: 104430.3047\n",
      "Epoch 524/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105578.9062 - val_loss: 104415.2109\n",
      "Epoch 525/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105568.1719 - val_loss: 104401.2109\n",
      "Epoch 526/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105553.1094 - val_loss: 104385.5312\n",
      "Epoch 527/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105540.7734 - val_loss: 104367.9766\n",
      "Epoch 528/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105529.6172 - val_loss: 104352.6875\n",
      "Epoch 529/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105514.9297 - val_loss: 104349.1953\n",
      "Epoch 530/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105501.9922 - val_loss: 104326.5703\n",
      "Epoch 531/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105491.2422 - val_loss: 104322.5078\n",
      "Epoch 532/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105469.9844 - val_loss: 104308.2969\n",
      "Epoch 533/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105459.6797 - val_loss: 104288.8125\n",
      "Epoch 534/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105445.8047 - val_loss: 104275.3594\n",
      "Epoch 535/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105433.2812 - val_loss: 104267.4766\n",
      "Epoch 536/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105422.6641 - val_loss: 104249.2031\n",
      "Epoch 537/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105409.3281 - val_loss: 104239.2734\n",
      "Epoch 538/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105394.2344 - val_loss: 104225.1484\n",
      "Epoch 539/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105382.6484 - val_loss: 104210.3516\n",
      "Epoch 540/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105368.4297 - val_loss: 104193.3047\n",
      "Epoch 541/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105353.9375 - val_loss: 104184.2734\n",
      "Epoch 542/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105344.8672 - val_loss: 104167.7969\n",
      "Epoch 543/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105328.9375 - val_loss: 104154.3438\n",
      "Epoch 544/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105315.6875 - val_loss: 104138.9375\n",
      "Epoch 545/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105301.1797 - val_loss: 104127.0391\n",
      "Epoch 546/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105290.1641 - val_loss: 104107.2891\n",
      "Epoch 547/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105274.2812 - val_loss: 104092.2891\n",
      "Epoch 548/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105264.7969 - val_loss: 104085.3906\n",
      "Epoch 549/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105254.6953 - val_loss: 104067.0078\n",
      "Epoch 550/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105239.0234 - val_loss: 104052.1484\n",
      "Epoch 551/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105228.4453 - val_loss: 104040.2109\n",
      "Epoch 552/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105212.7109 - val_loss: 104029.3828\n",
      "Epoch 553/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105201.4609 - val_loss: 104019.1797\n",
      "Epoch 554/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105187.5234 - val_loss: 104003.8672\n",
      "Epoch 555/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105177.8516 - val_loss: 103990.8906\n",
      "Epoch 556/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105163.7500 - val_loss: 103978.6719\n",
      "Epoch 557/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105153.7422 - val_loss: 103955.9375\n",
      "Epoch 558/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105139.2031 - val_loss: 103951.4375\n",
      "Epoch 559/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105129.2812 - val_loss: 103939.9375\n",
      "Epoch 560/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105116.3828 - val_loss: 103920.6719\n",
      "Epoch 561/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105104.6562 - val_loss: 103914.4609\n",
      "Epoch 562/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105093.7578 - val_loss: 103903.6250\n",
      "Epoch 563/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105077.2188 - val_loss: 103888.8359\n",
      "Epoch 564/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105063.4531 - val_loss: 103877.8750\n",
      "Epoch 565/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105055.1875 - val_loss: 103864.4922\n",
      "Epoch 566/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105040.5469 - val_loss: 103850.5625\n",
      "Epoch 567/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105030.2656 - val_loss: 103838.9609\n",
      "Epoch 568/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105017.5625 - val_loss: 103827.0781\n",
      "Epoch 569/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 105007.4609 - val_loss: 103809.9219\n",
      "Epoch 570/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104993.1875 - val_loss: 103802.0938\n",
      "Epoch 571/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104985.7344 - val_loss: 103781.6875\n",
      "Epoch 572/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104967.9844 - val_loss: 103780.2422\n",
      "Epoch 573/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104954.8750 - val_loss: 103767.7969\n",
      "Epoch 574/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104943.9062 - val_loss: 103755.2812\n",
      "Epoch 575/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104932.2188 - val_loss: 103733.5312\n",
      "Epoch 576/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104921.1172 - val_loss: 103721.9219\n",
      "Epoch 577/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104909.5000 - val_loss: 103709.5391\n",
      "Epoch 578/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104897.6328 - val_loss: 103700.2812\n",
      "Epoch 579/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104886.0234 - val_loss: 103686.7500\n",
      "Epoch 580/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104877.3516 - val_loss: 103671.3203\n",
      "Epoch 581/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 104863.0625 - val_loss: 103663.4844\n",
      "Epoch 582/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 104848.4531 - val_loss: 103653.3438\n",
      "Epoch 583/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 104838.1328 - val_loss: 103643.9531\n",
      "Epoch 584/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 104827.3594 - val_loss: 103634.5391\n",
      "Epoch 585/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 104813.4062 - val_loss: 103618.6641\n",
      "Epoch 586/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104806.5234 - val_loss: 103610.1406\n",
      "Epoch 587/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 104789.3984 - val_loss: 103596.7734\n",
      "Epoch 588/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 104780.1328 - val_loss: 103582.5391\n",
      "Epoch 589/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 104771.9531 - val_loss: 103565.6094\n",
      "Epoch 590/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 104760.2266 - val_loss: 103561.6641\n",
      "Epoch 591/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 104744.5625 - val_loss: 103549.4219\n",
      "Epoch 592/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 104735.6406 - val_loss: 103540.7109\n",
      "Epoch 593/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 104720.9766 - val_loss: 103529.0000\n",
      "Epoch 594/1000\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 104712.0625 - val_loss: 103515.4688\n",
      "Epoch 595/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104697.2812 - val_loss: 103504.3672\n",
      "Epoch 596/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104689.1328 - val_loss: 103495.3516\n",
      "Epoch 597/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104674.9688 - val_loss: 103475.8594\n",
      "Epoch 598/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104669.2266 - val_loss: 103471.1406\n",
      "Epoch 599/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104657.0625 - val_loss: 103458.1641\n",
      "Epoch 600/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104644.6250 - val_loss: 103443.9297\n",
      "Epoch 601/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104632.3438 - val_loss: 103433.5781\n",
      "Epoch 602/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104621.9297 - val_loss: 103422.7891\n",
      "Epoch 603/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 104611.1484 - val_loss: 103414.1953\n",
      "Epoch 604/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104599.8125 - val_loss: 103401.1797\n",
      "Epoch 605/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104586.8125 - val_loss: 103387.8516\n",
      "Epoch 606/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104575.5469 - val_loss: 103379.0859\n",
      "Epoch 607/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104562.9297 - val_loss: 103367.9766\n",
      "Epoch 608/1000\n",
      "130/130 [==============================] - 0s 3ms/step - loss: 104551.5469 - val_loss: 103357.3750\n",
      "Epoch 609/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104544.5078 - val_loss: 103344.1562\n",
      "Epoch 610/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104534.9531 - val_loss: 103332.7578\n",
      "Epoch 611/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104521.7344 - val_loss: 103318.1562\n",
      "Epoch 612/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104509.1406 - val_loss: 103312.9688\n",
      "Epoch 613/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104496.3125 - val_loss: 103300.9062\n",
      "Epoch 614/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104486.7734 - val_loss: 103293.9531\n",
      "Epoch 615/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104473.9219 - val_loss: 103277.0312\n",
      "Epoch 616/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104463.7578 - val_loss: 103268.1484\n",
      "Epoch 617/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104452.3906 - val_loss: 103256.9297\n",
      "Epoch 618/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104439.7109 - val_loss: 103245.3828\n",
      "Epoch 619/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104430.3594 - val_loss: 103240.0156\n",
      "Epoch 620/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104420.9219 - val_loss: 103223.0703\n",
      "Epoch 621/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104409.4219 - val_loss: 103217.2109\n",
      "Epoch 622/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104402.0000 - val_loss: 103204.8594\n",
      "Epoch 623/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104390.6719 - val_loss: 103188.6875\n",
      "Epoch 624/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104375.7109 - val_loss: 103183.1484\n",
      "Epoch 625/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104365.1328 - val_loss: 103175.7422\n",
      "Epoch 626/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104357.8281 - val_loss: 103159.0547\n",
      "Epoch 627/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104345.5781 - val_loss: 103148.1797\n",
      "Epoch 628/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104332.1328 - val_loss: 103138.7109\n",
      "Epoch 629/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104326.0391 - val_loss: 103136.5156\n",
      "Epoch 630/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104311.3750 - val_loss: 103120.7031\n",
      "Epoch 631/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104301.8828 - val_loss: 103107.8594\n",
      "Epoch 632/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104289.1797 - val_loss: 103096.0859\n",
      "Epoch 633/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104278.4531 - val_loss: 103089.6562\n",
      "Epoch 634/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104269.1719 - val_loss: 103078.5547\n",
      "Epoch 635/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104260.0312 - val_loss: 103077.0234\n",
      "Epoch 636/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104248.8516 - val_loss: 103056.5625\n",
      "Epoch 637/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104237.0781 - val_loss: 103047.0000\n",
      "Epoch 638/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104227.8750 - val_loss: 103034.7969\n",
      "Epoch 639/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104215.2266 - val_loss: 103026.4766\n",
      "Epoch 640/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104204.0625 - val_loss: 103020.1406\n",
      "Epoch 641/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104197.5469 - val_loss: 103012.6797\n",
      "Epoch 642/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104181.6484 - val_loss: 102996.1484\n",
      "Epoch 643/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104172.0234 - val_loss: 102988.8984\n",
      "Epoch 644/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104161.2109 - val_loss: 102982.5469\n",
      "Epoch 645/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104153.5781 - val_loss: 102968.8750\n",
      "Epoch 646/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104142.5547 - val_loss: 102963.8984\n",
      "Epoch 647/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104128.8125 - val_loss: 102937.2656\n",
      "Epoch 648/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104119.4375 - val_loss: 102933.6562\n",
      "Epoch 649/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104105.7578 - val_loss: 102928.6094\n",
      "Epoch 650/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104095.7969 - val_loss: 102915.3359\n",
      "Epoch 651/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104081.5938 - val_loss: 102907.7188\n",
      "Epoch 652/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104076.3750 - val_loss: 102896.3984\n",
      "Epoch 653/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104066.0000 - val_loss: 102881.9062\n",
      "Epoch 654/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104051.0078 - val_loss: 102874.2344\n",
      "Epoch 655/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104041.8672 - val_loss: 102864.5312\n",
      "Epoch 656/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104032.6562 - val_loss: 102856.4609\n",
      "Epoch 657/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104022.9531 - val_loss: 102847.4766\n",
      "Epoch 658/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 104008.4766 - val_loss: 102834.0703\n",
      "Epoch 659/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103997.6172 - val_loss: 102829.2891\n",
      "Epoch 660/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103987.0703 - val_loss: 102817.8125\n",
      "Epoch 661/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103977.2188 - val_loss: 102810.1484\n",
      "Epoch 662/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103969.4375 - val_loss: 102796.1719\n",
      "Epoch 663/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103955.8438 - val_loss: 102791.1641\n",
      "Epoch 664/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103947.2656 - val_loss: 102783.4609\n",
      "Epoch 665/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103939.6719 - val_loss: 102768.5000\n",
      "Epoch 666/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103927.3750 - val_loss: 102759.7422\n",
      "Epoch 667/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103916.1797 - val_loss: 102751.2891\n",
      "Epoch 668/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103908.3984 - val_loss: 102745.4766\n",
      "Epoch 669/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103897.5156 - val_loss: 102736.7891\n",
      "Epoch 670/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103889.8594 - val_loss: 102728.6875\n",
      "Epoch 671/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103877.4609 - val_loss: 102718.8438\n",
      "Epoch 672/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103866.8438 - val_loss: 102706.8906\n",
      "Epoch 673/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103859.4688 - val_loss: 102698.5312\n",
      "Epoch 674/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103850.2578 - val_loss: 102688.0078\n",
      "Epoch 675/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103841.6484 - val_loss: 102682.9297\n",
      "Epoch 676/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103831.7578 - val_loss: 102669.2266\n",
      "Epoch 677/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103820.3594 - val_loss: 102663.7812\n",
      "Epoch 678/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103812.5312 - val_loss: 102656.9375\n",
      "Epoch 679/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103804.1562 - val_loss: 102648.4453\n",
      "Epoch 680/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103795.0078 - val_loss: 102637.2656\n",
      "Epoch 681/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103783.2109 - val_loss: 102627.1562\n",
      "Epoch 682/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103775.6719 - val_loss: 102615.6797\n",
      "Epoch 683/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103766.5781 - val_loss: 102610.1641\n",
      "Epoch 684/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103758.2422 - val_loss: 102601.3906\n",
      "Epoch 685/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103750.0078 - val_loss: 102595.6797\n",
      "Epoch 686/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103739.2500 - val_loss: 102583.0078\n",
      "Epoch 687/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103731.0156 - val_loss: 102569.6953\n",
      "Epoch 688/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103721.5547 - val_loss: 102562.8750\n",
      "Epoch 689/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103713.6797 - val_loss: 102553.7656\n",
      "Epoch 690/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103704.0078 - val_loss: 102550.2656\n",
      "Epoch 691/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103699.5234 - val_loss: 102539.3984\n",
      "Epoch 692/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103689.5625 - val_loss: 102532.4141\n",
      "Epoch 693/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103679.1328 - val_loss: 102524.2500\n",
      "Epoch 694/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103669.0234 - val_loss: 102514.1250\n",
      "Epoch 695/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103658.6172 - val_loss: 102504.9922\n",
      "Epoch 696/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103650.2266 - val_loss: 102494.3047\n",
      "Epoch 697/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103645.0625 - val_loss: 102491.2500\n",
      "Epoch 698/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103636.8125 - val_loss: 102488.5703\n",
      "Epoch 699/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103625.2578 - val_loss: 102479.3516\n",
      "Epoch 700/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103620.1875 - val_loss: 102470.4219\n",
      "Epoch 701/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103609.0938 - val_loss: 102463.4453\n",
      "Epoch 702/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103599.0469 - val_loss: 102455.8438\n",
      "Epoch 703/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103596.6250 - val_loss: 102439.7891\n",
      "Epoch 704/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103585.7812 - val_loss: 102439.0312\n",
      "Epoch 705/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103577.8516 - val_loss: 102433.1875\n",
      "Epoch 706/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103569.1484 - val_loss: 102429.8984\n",
      "Epoch 707/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103559.9453 - val_loss: 102418.8594\n",
      "Epoch 708/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103553.9531 - val_loss: 102408.6641\n",
      "Epoch 709/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103541.4062 - val_loss: 102399.5078\n",
      "Epoch 710/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103535.6328 - val_loss: 102388.5703\n",
      "Epoch 711/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103528.9375 - val_loss: 102387.4609\n",
      "Epoch 712/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103521.8047 - val_loss: 102377.0703\n",
      "Epoch 713/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103509.5703 - val_loss: 102368.8125\n",
      "Epoch 714/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103501.1016 - val_loss: 102363.0391\n",
      "Epoch 715/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103495.9297 - val_loss: 102355.1094\n",
      "Epoch 716/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103488.1016 - val_loss: 102346.1484\n",
      "Epoch 717/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103473.8672 - val_loss: 102349.3672\n",
      "Epoch 718/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103465.0312 - val_loss: 102332.8984\n",
      "Epoch 719/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103459.5547 - val_loss: 102324.9453\n",
      "Epoch 720/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103449.7266 - val_loss: 102314.0312\n",
      "Epoch 721/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103444.1016 - val_loss: 102310.6406\n",
      "Epoch 722/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103432.6406 - val_loss: 102304.7969\n",
      "Epoch 723/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103421.5703 - val_loss: 102304.4609\n",
      "Epoch 724/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103415.9922 - val_loss: 102293.4688\n",
      "Epoch 725/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103408.4688 - val_loss: 102281.0156\n",
      "Epoch 726/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103398.1562 - val_loss: 102271.9297\n",
      "Epoch 727/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103390.1016 - val_loss: 102267.7031\n",
      "Epoch 728/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103378.6406 - val_loss: 102259.5078\n",
      "Epoch 729/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103369.5625 - val_loss: 102251.6875\n",
      "Epoch 730/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103362.1484 - val_loss: 102245.4688\n",
      "Epoch 731/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103353.3750 - val_loss: 102233.0391\n",
      "Epoch 732/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103345.7891 - val_loss: 102235.4922\n",
      "Epoch 733/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103338.8906 - val_loss: 102222.4297\n",
      "Epoch 734/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103330.3047 - val_loss: 102210.5625\n",
      "Epoch 735/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103321.5000 - val_loss: 102204.5156\n",
      "Epoch 736/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103310.7500 - val_loss: 102195.7422\n",
      "Epoch 737/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103301.2500 - val_loss: 102188.8516\n",
      "Epoch 738/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103295.7422 - val_loss: 102182.0781\n",
      "Epoch 739/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103285.7422 - val_loss: 102177.2500\n",
      "Epoch 740/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103276.3594 - val_loss: 102169.2891\n",
      "Epoch 741/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103266.7812 - val_loss: 102156.4141\n",
      "Epoch 742/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103260.4141 - val_loss: 102155.4531\n",
      "Epoch 743/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103252.9844 - val_loss: 102148.4219\n",
      "Epoch 744/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103243.8828 - val_loss: 102140.4766\n",
      "Epoch 745/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103234.2344 - val_loss: 102132.4922\n",
      "Epoch 746/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103227.9609 - val_loss: 102129.5391\n",
      "Epoch 747/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103220.0938 - val_loss: 102122.3359\n",
      "Epoch 748/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103210.2266 - val_loss: 102108.3906\n",
      "Epoch 749/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103204.4141 - val_loss: 102105.6953\n",
      "Epoch 750/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103196.0703 - val_loss: 102098.6719\n",
      "Epoch 751/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103187.2969 - val_loss: 102093.8750\n",
      "Epoch 752/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103181.4219 - val_loss: 102091.6094\n",
      "Epoch 753/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103172.2188 - val_loss: 102080.8906\n",
      "Epoch 754/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103164.2031 - val_loss: 102064.2266\n",
      "Epoch 755/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103151.3594 - val_loss: 102065.0000\n",
      "Epoch 756/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103146.6719 - val_loss: 102063.4844\n",
      "Epoch 757/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103135.8906 - val_loss: 102054.2500\n",
      "Epoch 758/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103130.9375 - val_loss: 102043.9141\n",
      "Epoch 759/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103122.0781 - val_loss: 102037.2344\n",
      "Epoch 760/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103114.7422 - val_loss: 102032.1250\n",
      "Epoch 761/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103112.1875 - val_loss: 102028.2656\n",
      "Epoch 762/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103098.2812 - val_loss: 102018.6484\n",
      "Epoch 763/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103093.2969 - val_loss: 102013.8203\n",
      "Epoch 764/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103084.7188 - val_loss: 102003.2266\n",
      "Epoch 765/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103077.9609 - val_loss: 101998.7656\n",
      "Epoch 766/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103070.0703 - val_loss: 101994.5625\n",
      "Epoch 767/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103065.1641 - val_loss: 101982.2500\n",
      "Epoch 768/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103055.1016 - val_loss: 101969.4219\n",
      "Epoch 769/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103049.4062 - val_loss: 101970.2188\n",
      "Epoch 770/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103040.6094 - val_loss: 101960.1562\n",
      "Epoch 771/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103032.8906 - val_loss: 101956.2109\n",
      "Epoch 772/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103027.4375 - val_loss: 101952.7422\n",
      "Epoch 773/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103019.2422 - val_loss: 101941.4766\n",
      "Epoch 774/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103011.3828 - val_loss: 101941.2422\n",
      "Epoch 775/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 103001.6406 - val_loss: 101930.1875\n",
      "Epoch 776/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102999.5703 - val_loss: 101922.6875\n",
      "Epoch 777/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102986.0703 - val_loss: 101913.8438\n",
      "Epoch 778/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102983.4219 - val_loss: 101900.9766\n",
      "Epoch 779/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102972.5859 - val_loss: 101892.4141\n",
      "Epoch 780/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102965.9297 - val_loss: 101890.7812\n",
      "Epoch 781/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102957.1484 - val_loss: 101881.3594\n",
      "Epoch 782/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102952.0312 - val_loss: 101877.0703\n",
      "Epoch 783/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102941.3984 - val_loss: 101868.7500\n",
      "Epoch 784/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102935.8281 - val_loss: 101857.2344\n",
      "Epoch 785/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102927.5312 - val_loss: 101853.8750\n",
      "Epoch 786/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102920.3594 - val_loss: 101845.0859\n",
      "Epoch 787/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102909.9141 - val_loss: 101841.0547\n",
      "Epoch 788/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102902.2656 - val_loss: 101825.8438\n",
      "Epoch 789/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102898.1875 - val_loss: 101819.6953\n",
      "Epoch 790/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102893.6094 - val_loss: 101812.6953\n",
      "Epoch 791/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102883.2578 - val_loss: 101809.6953\n",
      "Epoch 792/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102872.1797 - val_loss: 101802.3281\n",
      "Epoch 793/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102865.5938 - val_loss: 101798.4688\n",
      "Epoch 794/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102860.4922 - val_loss: 101794.4375\n",
      "Epoch 795/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102856.1094 - val_loss: 101776.0781\n",
      "Epoch 796/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102846.8672 - val_loss: 101770.6328\n",
      "Epoch 797/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102837.8594 - val_loss: 101769.3281\n",
      "Epoch 798/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102831.6953 - val_loss: 101758.5078\n",
      "Epoch 799/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102824.9375 - val_loss: 101747.5312\n",
      "Epoch 800/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102817.3516 - val_loss: 101744.1797\n",
      "Epoch 801/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102806.2656 - val_loss: 101736.5938\n",
      "Epoch 802/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102800.1094 - val_loss: 101727.2422\n",
      "Epoch 803/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102794.1562 - val_loss: 101720.1172\n",
      "Epoch 804/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102789.6953 - val_loss: 101718.5547\n",
      "Epoch 805/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102781.1875 - val_loss: 101710.5781\n",
      "Epoch 806/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102773.0234 - val_loss: 101703.3438\n",
      "Epoch 807/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102766.5859 - val_loss: 101691.3516\n",
      "Epoch 808/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102759.9609 - val_loss: 101689.4297\n",
      "Epoch 809/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102750.4766 - val_loss: 101675.0938\n",
      "Epoch 810/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102744.3594 - val_loss: 101673.4531\n",
      "Epoch 811/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102737.0703 - val_loss: 101669.9297\n",
      "Epoch 812/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102731.0781 - val_loss: 101663.1484\n",
      "Epoch 813/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102723.9219 - val_loss: 101660.4375\n",
      "Epoch 814/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102714.8828 - val_loss: 101647.3516\n",
      "Epoch 815/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102710.6641 - val_loss: 101645.4141\n",
      "Epoch 816/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102701.8828 - val_loss: 101628.8672\n",
      "Epoch 817/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102695.6016 - val_loss: 101632.0078\n",
      "Epoch 818/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102691.0703 - val_loss: 101622.4375\n",
      "Epoch 819/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102684.1562 - val_loss: 101620.5469\n",
      "Epoch 820/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102673.9062 - val_loss: 101612.6172\n",
      "Epoch 821/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102665.9141 - val_loss: 101612.3672\n",
      "Epoch 822/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102667.0859 - val_loss: 101606.7969\n",
      "Epoch 823/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102661.8047 - val_loss: 101602.0156\n",
      "Epoch 824/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102649.6406 - val_loss: 101592.1172\n",
      "Epoch 825/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102641.0469 - val_loss: 101577.4688\n",
      "Epoch 826/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102634.6016 - val_loss: 101564.0234\n",
      "Epoch 827/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102630.8125 - val_loss: 101561.7500\n",
      "Epoch 828/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102624.1172 - val_loss: 101560.6953\n",
      "Epoch 829/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102616.5156 - val_loss: 101548.6172\n",
      "Epoch 830/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102610.4844 - val_loss: 101551.7812\n",
      "Epoch 831/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102606.1719 - val_loss: 101539.4531\n",
      "Epoch 832/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102600.4844 - val_loss: 101528.4375\n",
      "Epoch 833/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102591.6875 - val_loss: 101521.6094\n",
      "Epoch 834/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102584.8750 - val_loss: 101520.1250\n",
      "Epoch 835/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102579.4062 - val_loss: 101512.9297\n",
      "Epoch 836/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102570.9766 - val_loss: 101502.2344\n",
      "Epoch 837/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102568.5781 - val_loss: 101498.7578\n",
      "Epoch 838/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102560.0234 - val_loss: 101488.0078\n",
      "Epoch 839/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102550.8594 - val_loss: 101482.5000\n",
      "Epoch 840/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102544.3672 - val_loss: 101474.5625\n",
      "Epoch 841/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102539.0391 - val_loss: 101470.0547\n",
      "Epoch 842/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102531.4609 - val_loss: 101458.2578\n",
      "Epoch 843/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102526.5781 - val_loss: 101453.8359\n",
      "Epoch 844/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102518.0703 - val_loss: 101444.0547\n",
      "Epoch 845/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102513.3203 - val_loss: 101440.2969\n",
      "Epoch 846/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102506.6016 - val_loss: 101431.5703\n",
      "Epoch 847/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102503.1172 - val_loss: 101427.0000\n",
      "Epoch 848/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102494.3281 - val_loss: 101416.6875\n",
      "Epoch 849/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102489.8203 - val_loss: 101419.0234\n",
      "Epoch 850/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102484.6328 - val_loss: 101402.4609\n",
      "Epoch 851/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102476.9453 - val_loss: 101396.0391\n",
      "Epoch 852/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102470.1953 - val_loss: 101394.0234\n",
      "Epoch 853/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102466.1172 - val_loss: 101391.3594\n",
      "Epoch 854/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102459.1250 - val_loss: 101379.0625\n",
      "Epoch 855/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102452.8281 - val_loss: 101366.7969\n",
      "Epoch 856/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102444.5625 - val_loss: 101361.3750\n",
      "Epoch 857/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102438.8203 - val_loss: 101360.2656\n",
      "Epoch 858/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102430.5391 - val_loss: 101349.5156\n",
      "Epoch 859/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102426.8672 - val_loss: 101341.2344\n",
      "Epoch 860/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102418.5000 - val_loss: 101337.5547\n",
      "Epoch 861/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102411.1484 - val_loss: 101332.8750\n",
      "Epoch 862/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102406.4297 - val_loss: 101324.1719\n",
      "Epoch 863/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102401.5625 - val_loss: 101314.6719\n",
      "Epoch 864/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102395.4297 - val_loss: 101316.9375\n",
      "Epoch 865/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102389.1875 - val_loss: 101298.6875\n",
      "Epoch 866/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102383.6719 - val_loss: 101296.4531\n",
      "Epoch 867/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102377.2734 - val_loss: 101282.3125\n",
      "Epoch 868/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102368.1953 - val_loss: 101283.1797\n",
      "Epoch 869/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102364.6641 - val_loss: 101276.5859\n",
      "Epoch 870/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102361.1797 - val_loss: 101270.2969\n",
      "Epoch 871/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102350.5547 - val_loss: 101263.0156\n",
      "Epoch 872/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102346.5703 - val_loss: 101260.7344\n",
      "Epoch 873/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102339.1406 - val_loss: 101248.3672\n",
      "Epoch 874/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102334.2344 - val_loss: 101244.7344\n",
      "Epoch 875/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102329.0625 - val_loss: 101238.1562\n",
      "Epoch 876/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102321.3594 - val_loss: 101227.9453\n",
      "Epoch 877/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102317.4141 - val_loss: 101225.8672\n",
      "Epoch 878/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102310.6094 - val_loss: 101213.9531\n",
      "Epoch 879/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102307.6172 - val_loss: 101206.0234\n",
      "Epoch 880/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102298.5938 - val_loss: 101198.0234\n",
      "Epoch 881/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102292.2266 - val_loss: 101194.2578\n",
      "Epoch 882/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102283.6875 - val_loss: 101190.3438\n",
      "Epoch 883/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102278.5078 - val_loss: 101179.9453\n",
      "Epoch 884/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102273.6562 - val_loss: 101174.1484\n",
      "Epoch 885/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102269.4609 - val_loss: 101157.6562\n",
      "Epoch 886/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102261.7969 - val_loss: 101157.6641\n",
      "Epoch 887/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102255.0781 - val_loss: 101152.5703\n",
      "Epoch 888/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102249.7500 - val_loss: 101147.4688\n",
      "Epoch 889/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102244.0625 - val_loss: 101144.7734\n",
      "Epoch 890/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102240.5469 - val_loss: 101138.2188\n",
      "Epoch 891/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102231.3594 - val_loss: 101126.7500\n",
      "Epoch 892/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102225.3594 - val_loss: 101121.5703\n",
      "Epoch 893/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102223.3125 - val_loss: 101114.0703\n",
      "Epoch 894/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102215.0000 - val_loss: 101109.0938\n",
      "Epoch 895/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102211.6875 - val_loss: 101110.7969\n",
      "Epoch 896/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102203.9844 - val_loss: 101100.2188\n",
      "Epoch 897/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102199.9531 - val_loss: 101088.0078\n",
      "Epoch 898/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102192.7266 - val_loss: 101082.8281\n",
      "Epoch 899/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102191.7969 - val_loss: 101075.1484\n",
      "Epoch 900/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102187.1406 - val_loss: 101076.4141\n",
      "Epoch 901/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102178.8672 - val_loss: 101067.3594\n",
      "Epoch 902/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102169.7656 - val_loss: 101066.8906\n",
      "Epoch 903/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102166.7031 - val_loss: 101054.9062\n",
      "Epoch 904/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102162.7812 - val_loss: 101047.6953\n",
      "Epoch 905/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102156.5859 - val_loss: 101037.7578\n",
      "Epoch 906/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102149.3047 - val_loss: 101036.5469\n",
      "Epoch 907/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102144.2500 - val_loss: 101033.6172\n",
      "Epoch 908/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102140.7891 - val_loss: 101027.1484\n",
      "Epoch 909/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102132.4922 - val_loss: 101017.2422\n",
      "Epoch 910/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102131.5391 - val_loss: 101013.4453\n",
      "Epoch 911/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102122.7891 - val_loss: 101005.2031\n",
      "Epoch 912/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102117.7031 - val_loss: 101000.1250\n",
      "Epoch 913/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102110.9141 - val_loss: 100993.4375\n",
      "Epoch 914/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102106.0234 - val_loss: 100983.5547\n",
      "Epoch 915/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102103.9141 - val_loss: 100983.8047\n",
      "Epoch 916/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102096.3594 - val_loss: 100972.0391\n",
      "Epoch 917/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102090.1719 - val_loss: 100971.5703\n",
      "Epoch 918/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102083.3672 - val_loss: 100967.3828\n",
      "Epoch 919/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102076.7969 - val_loss: 100957.6328\n",
      "Epoch 920/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102071.3906 - val_loss: 100951.4219\n",
      "Epoch 921/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102066.3594 - val_loss: 100956.6250\n",
      "Epoch 922/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102061.0938 - val_loss: 100943.6875\n",
      "Epoch 923/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102057.1797 - val_loss: 100934.4844\n",
      "Epoch 924/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102048.8594 - val_loss: 100930.5469\n",
      "Epoch 925/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102046.7891 - val_loss: 100928.5312\n",
      "Epoch 926/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102038.8438 - val_loss: 100920.4219\n",
      "Epoch 927/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102035.6172 - val_loss: 100915.3828\n",
      "Epoch 928/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102032.1797 - val_loss: 100905.3672\n",
      "Epoch 929/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102024.0469 - val_loss: 100909.7188\n",
      "Epoch 930/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102019.4062 - val_loss: 100902.8203\n",
      "Epoch 931/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102014.1719 - val_loss: 100893.7500\n",
      "Epoch 932/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102010.7891 - val_loss: 100891.7891\n",
      "Epoch 933/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102005.8516 - val_loss: 100891.3828\n",
      "Epoch 934/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 102002.4453 - val_loss: 100874.4844\n",
      "Epoch 935/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101995.0625 - val_loss: 100876.2578\n",
      "Epoch 936/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101986.7422 - val_loss: 100873.7734\n",
      "Epoch 937/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101982.9922 - val_loss: 100864.0078\n",
      "Epoch 938/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101978.1250 - val_loss: 100862.1328\n",
      "Epoch 939/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101972.2188 - val_loss: 100866.0000\n",
      "Epoch 940/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101966.3984 - val_loss: 100857.6797\n",
      "Epoch 941/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101964.3359 - val_loss: 100855.2656\n",
      "Epoch 942/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101959.6562 - val_loss: 100837.4609\n",
      "Epoch 943/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101954.8750 - val_loss: 100833.8828\n",
      "Epoch 944/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101947.6094 - val_loss: 100833.4844\n",
      "Epoch 945/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101944.0859 - val_loss: 100827.2266\n",
      "Epoch 946/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101937.6328 - val_loss: 100820.4922\n",
      "Epoch 947/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101933.1641 - val_loss: 100821.0625\n",
      "Epoch 948/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101929.5000 - val_loss: 100819.0859\n",
      "Epoch 949/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101925.6562 - val_loss: 100812.8828\n",
      "Epoch 950/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101919.4297 - val_loss: 100805.8359\n",
      "Epoch 951/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101917.5156 - val_loss: 100800.7266\n",
      "Epoch 952/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101911.8594 - val_loss: 100794.4219\n",
      "Epoch 953/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101903.0625 - val_loss: 100791.1328\n",
      "Epoch 954/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101899.9609 - val_loss: 100785.2109\n",
      "Epoch 955/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101897.4297 - val_loss: 100784.1250\n",
      "Epoch 956/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101893.0078 - val_loss: 100783.8828\n",
      "Epoch 957/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101884.3359 - val_loss: 100776.0469\n",
      "Epoch 958/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101882.2969 - val_loss: 100767.0156\n",
      "Epoch 959/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101878.5469 - val_loss: 100766.3281\n",
      "Epoch 960/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101875.7969 - val_loss: 100764.9531\n",
      "Epoch 961/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101867.0156 - val_loss: 100758.2109\n",
      "Epoch 962/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101863.1641 - val_loss: 100758.3828\n",
      "Epoch 963/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101861.2969 - val_loss: 100752.8984\n",
      "Epoch 964/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101859.0938 - val_loss: 100749.9375\n",
      "Epoch 965/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101854.7969 - val_loss: 100740.6406\n",
      "Epoch 966/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101848.8906 - val_loss: 100737.2969\n",
      "Epoch 967/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101845.4688 - val_loss: 100736.5156\n",
      "Epoch 968/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101837.5156 - val_loss: 100729.8516\n",
      "Epoch 969/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101834.3203 - val_loss: 100728.3047\n",
      "Epoch 970/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101832.4766 - val_loss: 100718.1406\n",
      "Epoch 971/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101829.5391 - val_loss: 100719.9297\n",
      "Epoch 972/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101822.4062 - val_loss: 100710.8281\n",
      "Epoch 973/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101820.7578 - val_loss: 100711.8359\n",
      "Epoch 974/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101815.4688 - val_loss: 100705.2812\n",
      "Epoch 975/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101808.3984 - val_loss: 100704.0859\n",
      "Epoch 976/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101803.4141 - val_loss: 100698.8047\n",
      "Epoch 977/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101800.7266 - val_loss: 100697.2656\n",
      "Epoch 978/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101796.3984 - val_loss: 100693.2188\n",
      "Epoch 979/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101792.8828 - val_loss: 100685.4453\n",
      "Epoch 980/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101788.1641 - val_loss: 100683.5469\n",
      "Epoch 981/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101786.8125 - val_loss: 100678.9219\n",
      "Epoch 982/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101779.3984 - val_loss: 100674.3125\n",
      "Epoch 983/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101777.2812 - val_loss: 100670.5078\n",
      "Epoch 984/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101770.7188 - val_loss: 100659.6016\n",
      "Epoch 985/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101768.0156 - val_loss: 100653.4219\n",
      "Epoch 986/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101760.2266 - val_loss: 100651.1406\n",
      "Epoch 987/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101757.6172 - val_loss: 100649.7109\n",
      "Epoch 988/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101753.1016 - val_loss: 100645.5078\n",
      "Epoch 989/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101751.7500 - val_loss: 100641.1875\n",
      "Epoch 990/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101744.2188 - val_loss: 100637.3828\n",
      "Epoch 991/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101741.4688 - val_loss: 100632.8516\n",
      "Epoch 992/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101739.1484 - val_loss: 100625.5625\n",
      "Epoch 993/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101731.7188 - val_loss: 100629.5703\n",
      "Epoch 994/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101730.9766 - val_loss: 100619.4688\n",
      "Epoch 995/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101726.6953 - val_loss: 100620.1016\n",
      "Epoch 996/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101722.3203 - val_loss: 100615.9141\n",
      "Epoch 997/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101714.4219 - val_loss: 100610.9219\n",
      "Epoch 998/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101711.9688 - val_loss: 100605.8984\n",
      "Epoch 999/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101710.1797 - val_loss: 100598.5156\n",
      "Epoch 1000/1000\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 101701.9609 - val_loss: 100595.2266\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 101676.3984\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_log = model.fit(X_train, y_train, epochs=1000, batch_size=100, validation_split=.2, verbose=1)\n",
    "model.evaluate(X_test, y_test)\n",
    "plot_loss(train_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Make a simple neural network for predicting the price of homes in California. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  \n",
       "0    -122.23  \n",
       "1    -122.22  \n",
       "2    -122.24  \n",
       "3    -122.25  \n",
       "4    -122.25  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "cal = fetch_california_housing(as_frame=True)\n",
    "Xcal = pd.DataFrame(cal.data)\n",
    "ycal = pd.DataFrame(cal.target)\n",
    "Xcal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20640 entries, 0 to 20639\n",
      "Data columns (total 8 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   MedInc      20640 non-null  float64\n",
      " 1   HouseAge    20640 non-null  float64\n",
      " 2   AveRooms    20640 non-null  float64\n",
      " 3   AveBedrms   20640 non-null  float64\n",
      " 4   Population  20640 non-null  float64\n",
      " 5   AveOccup    20640 non-null  float64\n",
      " 6   Latitude    20640 non-null  float64\n",
      " 7   Longitude   20640 non-null  float64\n",
      "dtypes: float64(8)\n",
      "memory usage: 1.3 MB\n"
     ]
    }
   ],
   "source": [
    "Xcal.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 8) (20640, 1)\n"
     ]
    }
   ],
   "source": [
    "y = np.array(ycal).reshape(-1,1)\n",
    "X = np.array(Xcal)\n",
    "print(X.shape, y.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization_5 (Normalizat  (None, 8)                17        \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 18)                162       \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 1)                 19        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 198\n",
      "Trainable params: 181\n",
      "Non-trainable params: 17\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "normalizer.adapt(np.array(X_train))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(normalizer)\n",
    "model.add(Dense(18, input_shape=(18,), activation='relu'))\n",
    "#model.add(Dense(18, input_dim=18, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_absolute_error', optimizer=tf.optimizers.Adam(learning_rate=.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "124/124 [==============================] - 1s 2ms/step - loss: 0.6738 - val_loss: 0.4829\n",
      "Epoch 2/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.4586 - val_loss: 0.4589\n",
      "Epoch 3/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.4341 - val_loss: 0.4421\n",
      "Epoch 4/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.4247 - val_loss: 0.4361\n",
      "Epoch 5/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.4149 - val_loss: 0.4364\n",
      "Epoch 6/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.4151 - val_loss: 0.4262\n",
      "Epoch 7/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.4111 - val_loss: 0.4351\n",
      "Epoch 8/1000\n",
      "124/124 [==============================] - 0s 2ms/step - loss: 0.4063 - val_loss: 0.4180\n",
      "Epoch 9/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.4012 - val_loss: 0.4284\n",
      "Epoch 10/1000\n",
      "124/124 [==============================] - 0s 2ms/step - loss: 0.3996 - val_loss: 0.4114\n",
      "Epoch 11/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3906 - val_loss: 0.4129\n",
      "Epoch 12/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3919 - val_loss: 0.4171\n",
      "Epoch 13/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3921 - val_loss: 0.4135\n",
      "Epoch 14/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3876 - val_loss: 0.4015\n",
      "Epoch 15/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3905 - val_loss: 0.4118\n",
      "Epoch 16/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3905 - val_loss: 0.4144\n",
      "Epoch 17/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3876 - val_loss: 0.4053\n",
      "Epoch 18/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3827 - val_loss: 0.4059\n",
      "Epoch 19/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3855 - val_loss: 0.4078\n",
      "Epoch 20/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3849 - val_loss: 0.4013\n",
      "Epoch 21/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3828 - val_loss: 0.4000\n",
      "Epoch 22/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3808 - val_loss: 0.4075\n",
      "Epoch 23/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3827 - val_loss: 0.4032\n",
      "Epoch 24/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3804 - val_loss: 0.4074\n",
      "Epoch 25/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3790 - val_loss: 0.4063\n",
      "Epoch 26/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3813 - val_loss: 0.4017\n",
      "Epoch 27/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3797 - val_loss: 0.3960\n",
      "Epoch 28/1000\n",
      "124/124 [==============================] - 0s 2ms/step - loss: 0.3823 - val_loss: 0.4011\n",
      "Epoch 29/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3789 - val_loss: 0.4046\n",
      "Epoch 30/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3772 - val_loss: 0.3989\n",
      "Epoch 31/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3788 - val_loss: 0.4014\n",
      "Epoch 32/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3753 - val_loss: 0.4022\n",
      "Epoch 33/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3764 - val_loss: 0.3941\n",
      "Epoch 34/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3776 - val_loss: 0.3963\n",
      "Epoch 35/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3753 - val_loss: 0.3971\n",
      "Epoch 36/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3751 - val_loss: 0.3883\n",
      "Epoch 37/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3718 - val_loss: 0.3912\n",
      "Epoch 38/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3752 - val_loss: 0.4065\n",
      "Epoch 39/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3764 - val_loss: 0.3911\n",
      "Epoch 40/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3750 - val_loss: 0.3895\n",
      "Epoch 41/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3732 - val_loss: 0.4045\n",
      "Epoch 42/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3758 - val_loss: 0.3896\n",
      "Epoch 43/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3726 - val_loss: 0.3945\n",
      "Epoch 44/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3745 - val_loss: 0.3899\n",
      "Epoch 45/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3737 - val_loss: 0.3957\n",
      "Epoch 46/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3717 - val_loss: 0.3994\n",
      "Epoch 47/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3698 - val_loss: 0.3877\n",
      "Epoch 48/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3732 - val_loss: 0.3946\n",
      "Epoch 49/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3725 - val_loss: 0.3872\n",
      "Epoch 50/1000\n",
      "124/124 [==============================] - 0s 2ms/step - loss: 0.3704 - val_loss: 0.3980\n",
      "Epoch 51/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3720 - val_loss: 0.3996\n",
      "Epoch 52/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3712 - val_loss: 0.3914\n",
      "Epoch 53/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3696 - val_loss: 0.3848\n",
      "Epoch 54/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3702 - val_loss: 0.3933\n",
      "Epoch 55/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3708 - val_loss: 0.3895\n",
      "Epoch 56/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3679 - val_loss: 0.3858\n",
      "Epoch 57/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3673 - val_loss: 0.3821\n",
      "Epoch 58/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3683 - val_loss: 0.3850\n",
      "Epoch 59/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3689 - val_loss: 0.3904\n",
      "Epoch 60/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3698 - val_loss: 0.3907\n",
      "Epoch 61/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3738 - val_loss: 0.3867\n",
      "Epoch 62/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3667 - val_loss: 0.3860\n",
      "Epoch 63/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3669 - val_loss: 0.3837\n",
      "Epoch 64/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3674 - val_loss: 0.3813\n",
      "Epoch 65/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3678 - val_loss: 0.3800\n",
      "Epoch 66/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3656 - val_loss: 0.3877\n",
      "Epoch 67/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3684 - val_loss: 0.3824\n",
      "Epoch 68/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3645 - val_loss: 0.3921\n",
      "Epoch 69/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3634 - val_loss: 0.3839\n",
      "Epoch 70/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3684 - val_loss: 0.3844\n",
      "Epoch 71/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3637 - val_loss: 0.3840\n",
      "Epoch 72/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3624 - val_loss: 0.3864\n",
      "Epoch 73/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3631 - val_loss: 0.3794\n",
      "Epoch 74/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3639 - val_loss: 0.3995\n",
      "Epoch 75/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3636 - val_loss: 0.3833\n",
      "Epoch 76/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3635 - val_loss: 0.3788\n",
      "Epoch 77/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3624 - val_loss: 0.3805\n",
      "Epoch 78/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3616 - val_loss: 0.3911\n",
      "Epoch 79/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3605 - val_loss: 0.3812\n",
      "Epoch 80/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3651 - val_loss: 0.3816\n",
      "Epoch 81/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3609 - val_loss: 0.3848\n",
      "Epoch 82/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3617 - val_loss: 0.3772\n",
      "Epoch 83/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3624 - val_loss: 0.3885\n",
      "Epoch 84/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3602 - val_loss: 0.3768\n",
      "Epoch 85/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3604 - val_loss: 0.3905\n",
      "Epoch 86/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3584 - val_loss: 0.3798\n",
      "Epoch 87/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3623 - val_loss: 0.3806\n",
      "Epoch 88/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3599 - val_loss: 0.3801\n",
      "Epoch 89/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3593 - val_loss: 0.3827\n",
      "Epoch 90/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3575 - val_loss: 0.3836\n",
      "Epoch 91/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3648 - val_loss: 0.3811\n",
      "Epoch 92/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3571 - val_loss: 0.3827\n",
      "Epoch 93/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3593 - val_loss: 0.3792\n",
      "Epoch 94/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3576 - val_loss: 0.3734\n",
      "Epoch 95/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3592 - val_loss: 0.3727\n",
      "Epoch 96/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3589 - val_loss: 0.3783\n",
      "Epoch 97/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3603 - val_loss: 0.3857\n",
      "Epoch 98/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3581 - val_loss: 0.3718\n",
      "Epoch 99/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3562 - val_loss: 0.3723\n",
      "Epoch 100/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3578 - val_loss: 0.3840\n",
      "Epoch 101/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3586 - val_loss: 0.3805\n",
      "Epoch 102/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3587 - val_loss: 0.3729\n",
      "Epoch 103/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3619 - val_loss: 0.3725\n",
      "Epoch 104/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3595 - val_loss: 0.3729\n",
      "Epoch 105/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3572 - val_loss: 0.3729\n",
      "Epoch 106/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3556 - val_loss: 0.3665\n",
      "Epoch 107/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3586 - val_loss: 0.3760\n",
      "Epoch 108/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3560 - val_loss: 0.3791\n",
      "Epoch 109/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3569 - val_loss: 0.3845\n",
      "Epoch 110/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3570 - val_loss: 0.3726\n",
      "Epoch 111/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3570 - val_loss: 0.3693\n",
      "Epoch 112/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3586 - val_loss: 0.3797\n",
      "Epoch 113/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3581 - val_loss: 0.3703\n",
      "Epoch 114/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3560 - val_loss: 0.3711\n",
      "Epoch 115/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3540 - val_loss: 0.3711\n",
      "Epoch 116/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3582 - val_loss: 0.3792\n",
      "Epoch 117/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3560 - val_loss: 0.3680\n",
      "Epoch 118/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3584 - val_loss: 0.3675\n",
      "Epoch 119/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3571 - val_loss: 0.3710\n",
      "Epoch 120/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3570 - val_loss: 0.3781\n",
      "Epoch 121/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3555 - val_loss: 0.3815\n",
      "Epoch 122/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3563 - val_loss: 0.3709\n",
      "Epoch 123/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3574 - val_loss: 0.3653\n",
      "Epoch 124/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3548 - val_loss: 0.3789\n",
      "Epoch 125/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3552 - val_loss: 0.3786\n",
      "Epoch 126/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3555 - val_loss: 0.3704\n",
      "Epoch 127/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3553 - val_loss: 0.3689\n",
      "Epoch 128/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3571 - val_loss: 0.3739\n",
      "Epoch 129/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3528 - val_loss: 0.3678\n",
      "Epoch 130/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3543 - val_loss: 0.3822\n",
      "Epoch 131/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3553 - val_loss: 0.3752\n",
      "Epoch 132/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3547 - val_loss: 0.3687\n",
      "Epoch 133/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3546 - val_loss: 0.3861\n",
      "Epoch 134/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3565 - val_loss: 0.3655\n",
      "Epoch 135/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3575 - val_loss: 0.3761\n",
      "Epoch 136/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3581 - val_loss: 0.3711\n",
      "Epoch 137/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3533 - val_loss: 0.3715\n",
      "Epoch 138/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3534 - val_loss: 0.3713\n",
      "Epoch 139/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3551 - val_loss: 0.3686\n",
      "Epoch 140/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3559 - val_loss: 0.3776\n",
      "Epoch 141/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3559 - val_loss: 0.3701\n",
      "Epoch 142/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3547 - val_loss: 0.3697\n",
      "Epoch 143/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3553 - val_loss: 0.3797\n",
      "Epoch 144/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3575 - val_loss: 0.3664\n",
      "Epoch 145/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3539 - val_loss: 0.3652\n",
      "Epoch 146/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3531 - val_loss: 0.3698\n",
      "Epoch 147/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3532 - val_loss: 0.3748\n",
      "Epoch 148/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3540 - val_loss: 0.3667\n",
      "Epoch 149/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3529 - val_loss: 0.3653\n",
      "Epoch 150/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3559 - val_loss: 0.3744\n",
      "Epoch 151/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3529 - val_loss: 0.3689\n",
      "Epoch 152/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3524 - val_loss: 0.3703\n",
      "Epoch 153/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3567 - val_loss: 0.3679\n",
      "Epoch 154/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3536 - val_loss: 0.3820\n",
      "Epoch 155/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3540 - val_loss: 0.3725\n",
      "Epoch 156/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3547 - val_loss: 0.3669\n",
      "Epoch 157/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3541 - val_loss: 0.3719\n",
      "Epoch 158/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3537 - val_loss: 0.3760\n",
      "Epoch 159/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3539 - val_loss: 0.3675\n",
      "Epoch 160/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3537 - val_loss: 0.3737\n",
      "Epoch 161/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3577 - val_loss: 0.3603\n",
      "Epoch 162/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3514 - val_loss: 0.3700\n",
      "Epoch 163/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3578 - val_loss: 0.3639\n",
      "Epoch 164/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3561 - val_loss: 0.3837\n",
      "Epoch 165/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3558 - val_loss: 0.3704\n",
      "Epoch 166/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3588 - val_loss: 0.3698\n",
      "Epoch 167/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3569 - val_loss: 0.3629\n",
      "Epoch 168/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3528 - val_loss: 0.3671\n",
      "Epoch 169/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3536 - val_loss: 0.3664\n",
      "Epoch 170/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3511 - val_loss: 0.3668\n",
      "Epoch 171/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3547 - val_loss: 0.3732\n",
      "Epoch 172/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3531 - val_loss: 0.3796\n",
      "Epoch 173/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3545 - val_loss: 0.3668\n",
      "Epoch 174/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3524 - val_loss: 0.3692\n",
      "Epoch 175/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3570 - val_loss: 0.3677\n",
      "Epoch 176/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3535 - val_loss: 0.3831\n",
      "Epoch 177/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3551 - val_loss: 0.3639\n",
      "Epoch 178/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3527 - val_loss: 0.3730\n",
      "Epoch 179/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3550 - val_loss: 0.3763\n",
      "Epoch 180/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3530 - val_loss: 0.3761\n",
      "Epoch 181/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3574 - val_loss: 0.3802\n",
      "Epoch 182/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3533 - val_loss: 0.3676\n",
      "Epoch 183/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3515 - val_loss: 0.3663\n",
      "Epoch 184/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3529 - val_loss: 0.3723\n",
      "Epoch 185/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3523 - val_loss: 0.3674\n",
      "Epoch 186/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3512 - val_loss: 0.3797\n",
      "Epoch 187/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3550 - val_loss: 0.3695\n",
      "Epoch 188/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3531 - val_loss: 0.3679\n",
      "Epoch 189/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3538 - val_loss: 0.3602\n",
      "Epoch 190/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3513 - val_loss: 0.3669\n",
      "Epoch 191/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3522 - val_loss: 0.3682\n",
      "Epoch 192/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3507 - val_loss: 0.3776\n",
      "Epoch 193/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3549 - val_loss: 0.3676\n",
      "Epoch 194/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3514 - val_loss: 0.3715\n",
      "Epoch 195/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3553 - val_loss: 0.3610\n",
      "Epoch 196/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3525 - val_loss: 0.3643\n",
      "Epoch 197/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3515 - val_loss: 0.3675\n",
      "Epoch 198/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3505 - val_loss: 0.3710\n",
      "Epoch 199/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3527 - val_loss: 0.3659\n",
      "Epoch 200/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3485 - val_loss: 0.3693\n",
      "Epoch 201/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3562 - val_loss: 0.3658\n",
      "Epoch 202/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3530 - val_loss: 0.3619\n",
      "Epoch 203/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3516 - val_loss: 0.3711\n",
      "Epoch 204/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3536 - val_loss: 0.3628\n",
      "Epoch 205/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3501 - val_loss: 0.3616\n",
      "Epoch 206/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3512 - val_loss: 0.3618\n",
      "Epoch 207/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3524 - val_loss: 0.3838\n",
      "Epoch 208/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3516 - val_loss: 0.3576\n",
      "Epoch 209/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3507 - val_loss: 0.3692\n",
      "Epoch 210/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3503 - val_loss: 0.3621\n",
      "Epoch 211/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3521 - val_loss: 0.3620\n",
      "Epoch 212/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3530 - val_loss: 0.3777\n",
      "Epoch 213/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3514 - val_loss: 0.3728\n",
      "Epoch 214/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3511 - val_loss: 0.3644\n",
      "Epoch 215/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3513 - val_loss: 0.3714\n",
      "Epoch 216/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3548 - val_loss: 0.3689\n",
      "Epoch 217/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3503 - val_loss: 0.3658\n",
      "Epoch 218/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3497 - val_loss: 0.3624\n",
      "Epoch 219/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3496 - val_loss: 0.3702\n",
      "Epoch 220/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3505 - val_loss: 0.3640\n",
      "Epoch 221/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3516 - val_loss: 0.3750\n",
      "Epoch 222/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3504 - val_loss: 0.3576\n",
      "Epoch 223/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3511 - val_loss: 0.3603\n",
      "Epoch 224/1000\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.3498 - val_loss: 0.3650\n",
      "Epoch 225/1000\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3501"
     ]
    }
   ],
   "source": [
    "train_log = model.fit(X_train, y_train, epochs=1000, batch_size=100, validation_split=.2, verbose=1)\n",
    "model.evaluate(X_test, y_test)\n",
    "plot_loss(train_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics of Overfitting and Underfitting in Neural Networks\n",
    "\n",
    "Just like any other type of model, our primary task in trying to attain an accurate set of predictions is to balance the overfitting and underfitting. In a neural network, the ideas are the same as with standard models, however the tools and their usage can differ slightly. \n",
    "\n",
    "### Add Data\n",
    "\n",
    "Adding data to the training set is the number one way to improve accuracy. As noted above, neural networks are commonly able to acheive very high accuracy levels if provided with very large training sets. For smaller datasets, the probability of a neural network being the best model is much lower than with big data. \n",
    "\n",
    "### Model Capacity\n",
    "\n",
    "The model capacity is the \"size\" of the model - refering to the combination of the number of neurons on each layer and the number of layers. \n",
    "\n",
    "In general the larger a feature set is, the larger a capacity we will need to be able to avoid underfitting and make accurate predictions. However, similar to a decision tree, if the model becomes too large for the data, we are likely to overfit. \n",
    "\n",
    "In big data scenarios (e.g. Google or Tesla training image recognition models) the feature sets can be massive (e.g. a 5 megapixel image is at least 15 million features) so the networks used have a very high capacity. Because there is a lot of training data, the model is able to have a huge capacity, but not overfit. These models can take FOREVER to process (e.g. weeks with the work paralellized on dedicated and fast machines) but they are able to make very accurate predictions since they get all the \"benefits\" of overfitting - predictions highly tailored to the training data; along with all the \"benefits\" of underfitting - since there is so much training data, they are still generalized enough to predict new data. \n",
    "\n",
    "The combination of large datasets, deep networks, and fast processing allows for most of the modern AI that we see or interact with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization_1 (Normalizat  (None, 18)               37        \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               2432      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 52,134\n",
      "Trainable params: 52,097\n",
      "Non-trainable params: 37\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Test Different Model Capacities\n",
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "normalizer.adapt(np.array(X_train))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(normalizer)\n",
    "model.add(Dense(128, input_dim=18, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 1ms/step - loss: 71781.4688\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.compile(loss='mean_absolute_error', optimizer=tf.optimizers.Adam(learning_rate=.01))\n",
    "train_log = model.fit(X_train, y_train, epochs=1000, batch_size=1000, validation_split=.2, verbose=0)\n",
    "model.evaluate(X_test, y_test)\n",
    "plot_loss(train_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "\n",
    "Early stopping is very common with neural networks, due to the common pattern mentioned above of the optimal balance of over/under fitting occuring at some point within many, potentially thousands, of epochs. Early stopping kills the process after it detects that validation loss is going back up. \n",
    "\n",
    "We can put early stopping in place by using a Keras function called a callback, which has odd syntax, but is quite simple to use. The patience pararmeter controls how many epcohs of worsening scores are tolerated before implementing the stop. The restore_best_weights tells the model to roll back all of its weights to the optimal point - so we automatically get the best model post-training. \n",
    "\n",
    "In most cases we probabyl want to use early stopping along with a high epoch number. We can let the model train, and just tell us when it is finished. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "normalizer.adapt(np.array(X_train))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(normalizer)\n",
    "model.add(Dense(18, input_dim=18, activation='relu'))\n",
    "model.add(Dense(18, activation='relu'))\n",
    "model.add(Dense(18, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 812us/step - loss: 72343.2266\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=50, restore_best_weights=True) \n",
    "\n",
    "model.compile(loss='mean_absolute_error', optimizer=tf.optimizers.Adam(learning_rate=.01))\n",
    "train_log = model.fit(X_train, y_train, epochs=1000, batch_size=100, validation_split=.2, verbose=0, callbacks=[callback])\n",
    "model.evaluate(X_test, y_test)\n",
    "plot_loss(train_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "Like other linear models, we can implement regularization to help tame overfitting. \n",
    "\n",
    "We can use both L2 (Ridge) regularization that will limit growth of coefficients, and L1 (Lasso) regularization that is able to eliminate features by shrinking their coefficients to 0. The functionality is the same as we are used to, a regularization term is added to the loss, and the optimization, such as gradient descent, is then performed as normal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization_3 (Normalizat  (None, 18)               37        \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 128)               2432      \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 52,134\n",
      "Trainable params: 52,097\n",
      "Non-trainable params: 37\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Regularization\n",
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "normalizer.adapt(np.array(X_train))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(normalizer)\n",
    "model.add(Dense(128, input_dim=18, activation='relu'))\n",
    "model.add(Dense(128, activation='relu', kernel_regularizer=\"l1\"))\n",
    "model.add(Dense(128, activation='relu', kernel_regularizer=\"l1\"))\n",
    "model.add(Dense(128, activation='relu', kernel_regularizer=\"l1\"))\n",
    "model.add(Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 1ms/step - loss: 68506.4375\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.compile(loss='mean_absolute_error', optimizer=tf.optimizers.Adam(learning_rate=.01))\n",
    "train_log = model.fit(X_train, y_train, epochs=100, batch_size=100, validation_split=.2, verbose=0)\n",
    "model.evaluate(X_test, y_test)\n",
    "plot_loss(train_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "Neural networks also commonly employ a technique call dropouts to prevent overfitting. This works just like the name says, every time the data is moved from one layer to another some portion of the features are randomly held out from being used. \n",
    "\n",
    "The intuitive explanation for dropout is that because individual nodes in the network cannot rely on the output of the others, each node must output features that are useful on their own. This sounds somewhat weird, but is actually effective. The number of features held out is called the dropout rate, typically between .2 and .5. \n",
    "\n",
    "An analogy can be drawn to the bootstrapping we looked at with trees - some random subset of features is selected each time, resulting in each batch getting \"a slightly different look at the data\", thus preventing overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization_4 (Normalizat  (None, 18)               37        \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 512)               9728      \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 798,246\n",
      "Trainable params: 798,209\n",
      "Non-trainable params: 37\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Dropout\n",
    "#Test Different Model Capacities\n",
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "normalizer.adapt(np.array(X_train))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(normalizer)\n",
    "model.add(Dense(512, input_dim=18, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-6dc1f24d3332>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mean_absolute_error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_log\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplot_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_log\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1418\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1419\u001b[0m                 steps_per_execution=self._steps_per_execution)\n\u001b[1;32m-> 1420\u001b[1;33m           val_logs = self.evaluate(\n\u001b[0m\u001b[0;32m   1421\u001b[0m               \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1422\u001b[0m               \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1714\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1715\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1716\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1717\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1718\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    952\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 954\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    955\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(loss='mean_absolute_error', optimizer=tf.optimizers.Adam(learning_rate=.01))\n",
    "train_log = model.fit(X_train, y_train, epochs=1000, batch_size=100, validation_split=.2, verbose=0)\n",
    "model.evaluate(X_test, y_test)\n",
    "plot_loss(train_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "\n",
    "Once the model is trained, using it is mostly familiar to us from the sklearn stuff. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test)\n",
    "mean_absolute_error(y_test, preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Use the California data from previously and try to add some regularization things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customized Loss\n",
    "\n",
    "Most scenarios are totally fine with a standard loss function, but what if we have something odd? What if we are playing on The Price is Right? We want to get as close as we can, without going over. We can write a loss function to mirror that!\n",
    "\n",
    "More practically, some real life scenarios have a disperse impact of different types of error. For example, if you are working for a call centre and predicting the number of agents to staff. Having slightly too many may be an error that costs a little bit of money, but not that big of a deal. Predicting too few might incur serious penalties if callers wait and you violate an SLA. Being off in one direction is bad, being off in the other direction can cause you to \"fall off of a cliff\" so to speak. In cases where the impact of the error is not uniform, custom loss functions may make sense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def priceIsRight(y_true, y_pred):\n",
    "    if y_pred <= y_true:\n",
    "        return (y_true - y_pred) ** 2\n",
    "    else:\n",
    "        return (y_true - y_pred) ** 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Exercise\n",
    "\n",
    "Try to use the California data with a customized loss function. \n",
    "\n",
    "Note: this is a 20 way classification, so you'll probably want that many neurons on the output layer, an appropriate activation (softmax), and the y values will need to be run through np_utils.to_categorical. As well, think about the loss function, try categorical crossentropy.\n",
    "\n",
    "We'll look at activation and loss functions more next week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Exercise - Newsgroup Classification\n",
    "\n",
    "Try to classify the newsgroup data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "remove = (\"headers\", \"footers\", \"quotes\")\n",
    "\n",
    "data_train = fetch_20newsgroups(\n",
    "    subset=\"train\", shuffle=True, remove=remove)\n",
    "\n",
    "data_test = fetch_20newsgroups(\n",
    "    subset=\"test\", shuffle=True, remove=remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d722d3adfa415172c1f5238b519fb86b488acdae450fd691ab06c09f4ca9173"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml3950')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
