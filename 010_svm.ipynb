{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import ml_utils\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "Support vector machines are really common and versitile - they can do classification and regression.\n",
    "\n",
    "SVM are easiest to understand through looking at some example visualizations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris Dataset\n",
    "df = ml_utils.sklearn_to_df(datasets.load_iris())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also simplify the data a little, so we can picture the support vectors. We want only 2 features to plot on a scatter plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = df.drop(columns={\"sepal length (cm)\", \"sepal width (cm)\"})\n",
    "d = d[d[\"target\"] <= 1]\n",
    "x_lab = \"petal length (cm)\"\n",
    "y_lab = \"petal width (cm)\"\n",
    "\n",
    "sns.scatterplot(data=d, y=y_lab, x=x_lab, hue=\"target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate These\n",
    "\n",
    "After we've plotted the data we can intuitively divide the groups - we could sketch a line right through the middle that divides the two groups. These groups are linearly separable, they can be separated by a line. In fact they could be separated by several different lines. \n",
    "\n",
    "![SVM Separation](images/svm_bad.png \"SVM\" )\n",
    "\n",
    "We can show a separation that the SVM determines by fitting an SVM model to the data, and doing a bunch of plotting work to make it pretty. SVM looks for the separation line (called a hyperplane) that maximizes that margin between the two classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM Stuff\n",
    "#Note: this stuff is just to draw the lines, you don't always need all this. \n",
    "#Note 2: Plotting this stuff is a pain! There's a library that we'll look at in a bit that makes it easier. \n",
    "def plot_svc_decision_boundary(svm_clf, xmin, xmax, color=\"black\"):\n",
    "    w = svm_clf.coef_[0]\n",
    "    b = svm_clf.intercept_[0]\n",
    "\n",
    "    # At the decision boundary, w0*x0 + w1*x1 + b = 0\n",
    "    # => x1 = -w0/w1 * x0 - b/w1\n",
    "    x0 = np.linspace(xmin, xmax, 200)\n",
    "    decision_boundary = -w[0]/w[1] * x0 - b/w[1]\n",
    "\n",
    "    margin = 1/w[1]\n",
    "    gutter_up = decision_boundary + margin\n",
    "    gutter_down = decision_boundary - margin\n",
    "\n",
    "    svs = svm_clf.support_vectors_\n",
    "    plt.scatter(svs[:, 0], svs[:, 1], s=180, facecolors='#FFAAAA')\n",
    "    plt.plot(x0, decision_boundary, \"-\", linewidth=2, color=color)\n",
    "    plt.plot(x0, gutter_up, \"--\", linewidth=2, color=color)\n",
    "    plt.plot(x0, gutter_down, \"--\", linewidth=2, color=color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate fitted Support Vector Classifier and plot the separation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = d[\"target\"]\n",
    "X = d.drop(columns={\"target\"})\n",
    "svm_clf = SVC(kernel=\"linear\", C=float(\"inf\"))\n",
    "svm_clf.fit(X, y)\n",
    "\n",
    "sns.scatterplot(data=d, y=y_lab, x=x_lab, hue=\"target\")\n",
    "plot_svc_decision_boundary(svm_clf, 0, 5.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Separation\n",
    "\n",
    "The above result shows how SVM works in one picture - it draws a line to separate the two groups, and chooses the place for that line that maximizes the width of the dotted lines - the decision boundaries. \n",
    "\n",
    "In this example the two classes are very separate, so the SVM can create a nice wide boundary splitting A and B. Note that if we had a bunch more data that was outside of the boundary, it wouldn't change the results - only things that impact the decision boundary \"matter\" in making the model. \n",
    "\n",
    "What we have is a hard margin classification - there is a strict division between classes. \n",
    "\n",
    "### The Hyperplane\n",
    "\n",
    "The hyperplane is the term for that line generated between the two datasets. What is it? It is a linear function in dimensions n-1 that splits the groups. For 2D examples, it is a line. For 3 dimensions, it is a plane. For greater dimensions, it gets hard to picture.\n",
    "\n",
    "![SVM Hyperplane](images/svm_plane.png \"SVM Hyperplane\" )\n",
    "\n",
    "The SVM algorithm tries to maximize this boundary size - this should intuitively make sense. The wider the gulf between the two classes, the more confident we can be in separating an item into one group or another. The items that are close to the boundaries are the \"borderline\" items that are close and could go either way - we want to minimize that. \n",
    "\n",
    "#### Support Vectors\n",
    "\n",
    "The points that define the boundaries, that are intersected by the dashed lines, are called the support vectors. \n",
    "\n",
    "#### SVM Soft Margins\n",
    "\n",
    "Many to most things we want to classify don't have such a severe and simple separation of classes. With hard margins, even one outlier that was \"on the other side\" could mess things up. To deal with this we can make our model softer, or allow it to deal with some amount of overlap in our classes. This is (obviously) soft margin classification. \n",
    "\n",
    "This is done by setting a hyperparameter C in the model: \n",
    "<ul>\n",
    "<li> High C values minimize overlap, but result in smaller boundaries. \n",
    "<li> Low C values allow for more overlap, but have wider boundaries. \n",
    "</ul>\n",
    "\n",
    "In general higher C will make models more fitted, lower will make them more general. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = df.drop(columns={\"sepal length (cm)\", \"sepal width (cm)\"})\n",
    "d2 = d2[d2[\"target\"] != 2]\n",
    "y2 = d2[\"target\"]\n",
    "X2 = d2.drop(columns={\"target\"})\n",
    "\n",
    "svm_wide = SVC(kernel=\"linear\", C=float(5))\n",
    "svm_wide.fit(X2, y2)\n",
    "svm_nar = SVC(kernel=\"linear\", C=float(.1))\n",
    "svm_nar.fit(X2, y2)\n",
    "\n",
    "sns.scatterplot(data=d2, y=y_lab, x=x_lab, hue=\"target\").set(title=\"Low C - Wider Margins. Less Fitted\")\n",
    "plot_svc_decision_boundary(svm_nar, 0, 6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=d2, y=y_lab, x=x_lab, hue=\"target\").set(title=\"High C - Smaller Margins. More Fitted\")\n",
    "plot_svc_decision_boundary(svm_wide, 0, 6, color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non Linear Separation\n",
    "\n",
    "The SVM we just looked at looks pretty good, as long as things are linearly separable. Unfortunately not all data is like that. \n",
    "\n",
    "Similarly to using a basis function to \"curve\" a linear regression to non-linear data, we can use the kernal argument to apply an SVM to a non linear pattern. This is sometimes called the \"kernal trick\" and is the key to dealing with real data, which often doesn't have nice wide spearations like we did with the sample. \n",
    "\n",
    "For example, we can create some dummy data that is not very linearly separable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "X_moon, y_moon = make_moons(n_samples=100, noise=0.15, random_state=42)\n",
    "d_moon = pd.DataFrame(X_moon[:,0], columns={\"X1\"})\n",
    "d_moon[\"X2\"] = X_moon[:,1]\n",
    "d_moon[\"y\"] = y_moon\n",
    "sns.scatterplot(data=d_moon, x=\"X1\", y=\"X2\", hue=\"y\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separation\n",
    "\n",
    "These two datasets are nonn linearly separable. If we were to draw a curved line between them it would look kind of like a cubic polynomial. We can use a kernel in our model to incorporate that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernels\n",
    "\n",
    "Kernels function like basis functions in linear regression. We can specify this as a hyperparameter when creating the SVM object. The choices are:\n",
    "<ul>\n",
    "<li> linear\n",
    "<li> rbf - radial (circular)\n",
    "<li> poly - polynomial\n",
    "<li> sigmoid - like our sigmoid curve in linear regression. \n",
    "<li> custom - you can create a custom function to do the transformation you'd like\n",
    "</ul>\n",
    "\n",
    "Each of these will generate a different split, we want to choose one that matches our data. With these 2D examples, that isn't too hard; with real data we are probably in a guess and test with a grid search scenario. \n",
    "\n",
    "![SVM Kernels](images/svm_kern.png \"SVM Kernels\" )\n",
    "\n",
    "Note that the linear one is fast, especially on large datasets, the others are a fair bit slower. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_kernel_svm_clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1))\n",
    "    ])\n",
    "poly_kernel_svm_clf.fit(X_moon, y_moon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting with Hyperplanes\n",
    "\n",
    "As we've seen above, plotting these charts can be a pain, there is a library that makes it easier but it does need to be installed. The library is called mlxtend. This one isn't in the conda install stuff, you need to use pip to install it:\n",
    "\n",
    "> pip install mlxtend\n",
    "\n",
    "If you don't have pip, that might actually be helpful with installing things in general. It can be installed via:\n",
    "\n",
    "> conda install pip\n",
    "\n",
    "If you don't have this installed, it isn't really the end of the world. This visualization stuff is more for the demonstrations of how SVMs work, when using real data it is pretty hard to visualize it like this when there are >3 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "plot_decision_regions(X=X_moon, \n",
    "                      y=y_moon,\n",
    "                      clf=poly_kernel_svm_clf, \n",
    "                      legend=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Radial Kernel\n",
    "\n",
    "What if we make some data that is circular? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "X_cir, y_cir = make_circles(n_samples=200, noise=0.15, factor=.3)\n",
    "d_cir = pd.DataFrame(X_cir[:,0], columns={\"X1\"})\n",
    "d_cir[\"X2\"] = X_cir[:,1]\n",
    "d_cir[\"y\"] = y_cir\n",
    "sns.scatterplot(data=d_cir, x=\"X1\", y=\"X2\", hue=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is obviously pretty difficult to imposible to separate linearly, but if we give our SVM classifier a radial kernel, it can split these two classes, probably almost perfectly (we have randomness, so each dataset is slightly different)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radial_kernel_svm_clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm_clf\", SVC(kernel=\"rbf\"))\n",
    "    ])\n",
    "radial_kernel_svm_clf.fit(X_cir, y_cir)\n",
    "\n",
    "plot_decision_regions(X=X_cir, \n",
    "                      y=y_cir,\n",
    "                      clf=radial_kernel_svm_clf, \n",
    "                      legend=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Weird Shape\n",
    "\n",
    "The code below will give us a random classification to classify. Try using a grid search with different kernels to see which one fits. Every run will give a different shape of data, so the right answer is random. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make some random data\n",
    "from sklearn.datasets import make_classification\n",
    "X_cla, y_cla = make_classification(n_samples=10000, n_features=2, n_redundant=0, class_sep=.9)\n",
    "d_cla = pd.DataFrame(X_cla[:,0], columns={\"X1\"})\n",
    "d_cla[\"X2\"] = X_cla[:,1]\n",
    "d_cla[\"y\"] = y_cla\n",
    "sns.scatterplot(data=d_cla, x=\"X1\", y=\"X2\", hue=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use grid search to find svm model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cla, y_cla)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curves\n",
    "\n",
    "In addition to our friend the confusion matrix, ROC curves are another useful way to visualize classifier performance. ROC stands for Receiver Operating Characteristic and it plots the true positive rate (recall) against the false positive rate (1-specificity). The way we read the curve is pretty simple, the faster the line for our model pushes towards the top left corner, the better it is. What that is measuring is effectively that the model is able to \"accumulate\" true positives \"before\" false positives - or the ratio of TP to FP is high, which is what we want. \n",
    "\n",
    "The dotted diagonal line represents random chance, and the line for our model represents the improvement over random chance. The random chance line shows that if we were just guessing, TP and FP would be equally likely, so we'd climb to getting 100% of the TP getting an equal amount of FP. The farther up our line is, the more TP we get \"for each\" FP that the model predicts. \n",
    "\n",
    "There isn't a all-in-one chart in seaborn that can print a visualization of a ROC curve in one step, which is annoying. This is the type of problem that is both common and relatively easy, so the internet offers us a million different implementations that we can borrow from if we don't want to make a linechart by hand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fper, tper):\n",
    "    plt.plot(fper, tper, color='red', label='ROC')\n",
    "    plt.plot([0, 1], [0, 1], color='green', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic Curve')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_train, train_preds)\n",
    "plot_roc_curve(fpr,tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Score\n",
    "\n",
    "We can also get the ROC score, which measures the area under the curve (AUC). The score goes between 0 and 1, the higher the better. If it was 1, that would indicate that we get all the true positives, with no false positives, which is pretty good!. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_train, train_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM and Multinomial Classifications\n",
    "\n",
    "Like linear regression, SVMs are a 2 class thing by default. In an SVM classifier we can choose between One vs One, or One vs Rest, with OVR being the default. \n",
    "\n",
    "We can try it for the flower example, with all the data. This has 3 output classes and 4 features. We can't really visualize it in 4D very easily but we can make the classification. If we take a peek at the scatterplots and think about the OVR decisions, I'd suspect going in that we'd be able to do this pretty accurately. If we visualize one set vs the other two, each scatter plot looks like we can come pretty close to separating those groups linearly - they are reasonably distinct from each other. Let's see the results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=df, hue=\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y2 = np.array(df[\"target\"]).reshape(-1,1)\n",
    "X2 = np.array(df.drop(columns={\"target\"}))\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2)\n",
    "\n",
    "flower_clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm_clf\", SVC())\n",
    "    ])\n",
    "\n",
    "params = {'svm_clf__kernel':[\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "        \"svm_clf__C\":[0.1, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2, 4, 6, 10]}\n",
    " \n",
    "clf2 = GridSearchCV(flower_clf, param_grid=params, cv=5, n_jobs=-1) \n",
    "clf2.fit(X_train2, y_train2.ravel())\n",
    "best2 = clf2.best_estimator_\n",
    "train_preds2 = best2.predict(X_train2)\n",
    "test_preds2 = best2.predict(X_test2)\n",
    "print(\"Train Score:\", best2.score(X_train2, y_train2))\n",
    "print(\"Test Score:\", best2.score(X_test2, y_test2))\n",
    "print(best2)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d722d3adfa415172c1f5238b519fb86b488acdae450fd691ab06c09f4ca9173"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ml3950': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
