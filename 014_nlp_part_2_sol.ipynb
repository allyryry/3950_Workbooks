{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More NLP\n",
    "\n",
    "## Truncated Singular Value Decomposition and Dimensionality Reduction\n",
    "\n",
    "When processing text we end up with feature sets that are large! There is up to one feature per different word in our text sample, far larger than a typical feature set that we're used to. One thing we can do when vectorizing is just to cap the number of features we end up with, but that doesn't seem to be the most sophisticated approach. \n",
    "\n",
    "TSVD is one thing that we can do to chop down the feature set - or reduce the dimensions - with a little more thought. \n",
    "\n",
    "## Dimensionality Reduction\n",
    "\n",
    "Dimensionality reduction is a common technique in machine learning, it does its name - reduces the dimensions in our feature data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset from Last Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: (5572,)\n",
      "vectorized: (5572, 89635)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>00 easter</th>\n",
       "      <th>00 easter prize</th>\n",
       "      <th>00 easter prize draw</th>\n",
       "      <th>00 sub</th>\n",
       "      <th>00 sub 16</th>\n",
       "      <th>00 sub 16 remove</th>\n",
       "      <th>00 sub 16 unsub</th>\n",
       "      <th>00 subs</th>\n",
       "      <th>00 subs 16</th>\n",
       "      <th>...</th>\n",
       "      <th>zoom cine actually</th>\n",
       "      <th>zoom cine actually tonight</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zouk nichols</th>\n",
       "      <th>zouk nichols paris</th>\n",
       "      <th>zouk nichols paris free</th>\n",
       "      <th>zyada</th>\n",
       "      <th>zyada kisi</th>\n",
       "      <th>zyada kisi ko</th>\n",
       "      <th>zyada kisi ko kuch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4364</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5075</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1572</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2717</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3628</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 89635 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       00  00 easter  00 easter prize  00 easter prize draw  00 sub  \\\n",
       "4364  0.0        0.0              0.0                   0.0     0.0   \n",
       "5075  0.0        0.0              0.0                   0.0     0.0   \n",
       "1572  0.0        0.0              0.0                   0.0     0.0   \n",
       "2717  0.0        0.0              0.0                   0.0     0.0   \n",
       "3628  0.0        0.0              0.0                   0.0     0.0   \n",
       "\n",
       "      00 sub 16  00 sub 16 remove  00 sub 16 unsub  00 subs  00 subs 16  ...  \\\n",
       "4364        0.0               0.0              0.0      0.0         0.0  ...   \n",
       "5075        0.0               0.0              0.0      0.0         0.0  ...   \n",
       "1572        0.0               0.0              0.0      0.0         0.0  ...   \n",
       "2717        0.0               0.0              0.0      0.0         0.0  ...   \n",
       "3628        0.0               0.0              0.0      0.0         0.0  ...   \n",
       "\n",
       "      zoom cine actually  zoom cine actually tonight  zouk  zouk nichols  \\\n",
       "4364                 0.0                         0.0   0.0           0.0   \n",
       "5075                 0.0                         0.0   0.0           0.0   \n",
       "1572                 0.0                         0.0   0.0           0.0   \n",
       "2717                 0.0                         0.0   0.0           0.0   \n",
       "3628                 0.0                         0.0   0.0           0.0   \n",
       "\n",
       "      zouk nichols paris  zouk nichols paris free  zyada  zyada kisi  \\\n",
       "4364                 0.0                      0.0    0.0         0.0   \n",
       "5075                 0.0                      0.0    0.0         0.0   \n",
       "1572                 0.0                      0.0    0.0         0.0   \n",
       "2717                 0.0                      0.0    0.0         0.0   \n",
       "3628                 0.0                      0.0    0.0         0.0   \n",
       "\n",
       "      zyada kisi ko  zyada kisi ko kuch  \n",
       "4364            0.0                 0.0  \n",
       "5075            0.0                 0.0  \n",
       "1572            0.0                 0.0  \n",
       "2717            0.0                 0.0  \n",
       "3628            0.0                 0.0  \n",
       "\n",
       "[5 rows x 89635 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load Data\n",
    "df = pd.read_csv(\"data/spam.csv\", encoding=\"ISO-8859-1\")\n",
    "df.drop(columns={\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"}, inplace=True)\n",
    "df.rename(columns={\"v1\":\"target\", \"v2\":\"text\"}, inplace=True)\n",
    "#TF-IDF\n",
    "vec_tf = TfidfVectorizer(sublinear_tf=True, ngram_range=(1,4), stop_words=\"english\", strip_accents=\"unicode\")\n",
    "tmp = vec_tf.fit_transform(df[\"text\"])\n",
    "tok_cols = vec_tf.get_feature_names()\n",
    "tok_df = pd.DataFrame(tmp.toarray(), columns=tok_cols)\n",
    "print(\"original:\", df[\"text\"].shape)\n",
    "print(\"vectorized:\", tmp.shape)\n",
    "y = df[\"target\"]\n",
    "X = df[\"text\"]\n",
    "tok_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA - Latent Semantic Analysis\n",
    "\n",
    "The TSVD performs somehting called latent semantic analysis. The process of LSA and the math behind it are not something we need to explore in detail. (LSA is often called LSI - Latent Semantic Indexing)\n",
    "\n",
    "The idea of LSA is that it can generate \"concepts\" in the text. These concepts are found by looking at which terms occur in which documents - documents that have the same terms repeated are likely related to the same concept; other documents that share other words with those documents are likely on the same concept as well.  \n",
    "\n",
    "An important part is the word \"Latent\" - i.e. the patterns detected are hidden, not explicit in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement SVD to Trim Dataset\n",
    "\n",
    "We are starting with LOTS of feature inputs. Below we can loop through several models of different number of remaining components to see the accuracy depending on the number of features we keep in the feature set. \n",
    "\n",
    "The truncated part of truncated SVD trims the featureset down to the most significant features. \n",
    "\n",
    "We started with a lot of features - we can make predictions that are close to as accurate with far fewer, hopefully!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "\tmodels = dict()\n",
    "\tfor i in range(1,10):\n",
    "\t\tn = i*50\n",
    "\t\tsteps = [('svd', TruncatedSVD(n_components=n)), ('m', LogisticRegression())]\n",
    "\t\tmodels[str(n)] = Pipeline(steps=steps)\n",
    "\treturn models\n",
    " \n",
    "# evaluate a give model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "\t#Splits cut for speed\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=2, n_repeats=1)\n",
    "\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "\treturn scores\n",
    " \n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "\tscores = evaluate_model(model, tok_df, y)\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "# plot model performance for comparison\n",
    "\n",
    "plt.boxplot(results, labels=names, showmeans=True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Truncated SVD\n",
    "\n",
    "Try to use the same text for predictions from the newsgroups last time. Try to use the TSVD with a limited number of components and see if the accuracy can stay similar to what we got last time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "remove = (\"headers\", \"footers\", \"quotes\")\n",
    "categories = [\"alt.atheism\", \"talk.religion.misc\"]\n",
    "\n",
    "data_train = fetch_20newsgroups(\n",
    "    subset=\"train\", categories=categories, shuffle=True, random_state=42, remove=remove)\n",
    "\n",
    "data_test = fetch_20newsgroups(\n",
    "    subset=\"test\", categories=categories, shuffle=True, random_state=42, remove=remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (x,y): (857, 226373)   Test (x,y): (570, 226373)\n"
     ]
    }
   ],
   "source": [
    "# Vectorize and prep datasets\n",
    "news_tf = TfidfVectorizer(sublinear_tf=True, ngram_range=(1,4), stop_words=\"english\", strip_accents=\"unicode\")\n",
    "X_train = news_tf.fit_transform(data_train.data)\n",
    "y_train = data_train.target\n",
    "X_test = news_tf.transform(data_test.data)\n",
    "y_test = data_test.target\n",
    "print(\"Train (x,y):\", X_train.shape, \"  Test (x,y):\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5964912280701754"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Models\n",
    "tsvd = TruncatedSVD(n_components=50)\n",
    "news_steps = [('svd', tsvd), ('m', LogisticRegression())]\n",
    "news_model = Pipeline(steps=news_steps)\n",
    "news_model.fit(X_train, y_train)\n",
    "news_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:  ['ico', 'ico tek', 'ico tek com', 'vice ico', 'vice ico tek']\n",
      "Topic 1:  ['just', 'god', 'people', 'don', 'think']\n",
      "Topic 2:  ['cobb', '3rd debate', '3rd debate cobb', '3rd debate cobb alexia', 'champaign urbana']\n",
      "Topic 3:  ['mom', 'men', 'isc', 'isc rit', 'isc rit edu']\n",
      "Topic 4:  ['freewill', 'angels freewill', 'angels freewill god', 'angels freewill god tells', 'freewill god']\n",
      "Topic 5:  ['deletion', 'argument', 'exist', 'atheism', 'definition']\n",
      "Topic 6:  ['deletion', 'ra', 'mcconkie', 'mormon', 'lds']\n",
      "Topic 7:  ['just', 'deletion', 'start', 'hand', 'account smile']\n",
      "Topic 8:  ['context', 'quote context', 'jim', 'quotes context', 'quote']\n",
      "Topic 9:  ['messenger', 'koresh', 'carried says', 'carried says character', 'carried says character messenger']\n",
      "Topic 10:  ['washed', 'washed blood', 'bull', 'blood', 'lamb']\n",
      "Topic 11:  ['rb', 'young', 'preying', 'preying young', 'altar boy']\n",
      "Topic 12:  ['said surrender', 'surrender', 'broadcast message', 'broadcast message didn', 'broadcast message didn said']\n",
      "Topic 13:  ['petition', 'public schools', 'schools', 'dm', 'public']\n",
      "Topic 14:  ['petition', 'public schools', 'dm', 'schools', 'hare']\n",
      "Topic 15:  ['just', 'god', 'jesus', 'israelites', 'chosen people']\n",
      "Topic 16:  ['finished writing', 'finished writing sequel', 'finished writing sequel bible', 'koresh finished', 'koresh finished writing']\n",
      "Topic 17:  ['just', 'just bet', 'oh just bet', 'oh just', 'oh']\n",
      "Topic 18:  ['dean', 'dean kaflowitz', 'kaflowitz', 'objective', 'thread']\n",
      "Topic 19:  ['broken', 'promises broken', 'promises', 'cheat hillary', 'hillary']\n",
      "Topic 20:  ['just', 'broken', 'promises broken', 'promises', 'objective']\n",
      "Topic 21:  ['jose', 'san jose', 'atheism', 'san', 'hausmann']\n",
      "Topic 22:  ['cheat hillary', 'hillary', 'cheat', 'just', 'believing god prepared']\n",
      "Topic 23:  ['order', 'cheat hillary', 'hillary', 'cheat', 'rosicrucian']\n",
      "Topic 24:  ['god', 'objective', 'jesus', 'atheism', 'did claim']\n",
      "Topic 25:  ['believing god prepared', 'believing god prepared eternal', 'blashephemers', 'blashephemers hell', 'blashephemers hell believing']\n",
      "Topic 26:  ['atheism', 'atheists', 'just', 'don', 'god']\n",
      "Topic 27:  ['think', 'animals', 'moral', 'cheat hillary', 'cheat']\n",
      "Topic 28:  ['cheat hillary', 'hillary', 'cheat', 'values', 'explain pertains']\n",
      "Topic 29:  ['argument', 'explain pertains', 'explain pertains position', 'explain pertains position statement', 'pertains position']\n",
      "Topic 30:  ['claim', 'islamic', 'did', 'did claim', 'atheism']\n",
      "Topic 31:  ['did claim', 'cruel', 'claim', 'did', 'claim objective']\n",
      "Topic 32:  ['mean', 'sex', 'depression', 'marital', 'marital sex']\n",
      "Topic 33:  ['think', 'don think', 'values', 'motto', 'science']\n",
      "Topic 34:  ['ignorance strength', 'strength', 'ignorance', 'koresh', 'animals']\n",
      "Topic 35:  ['list', 'long', 'morality', 'christian', 'know']\n",
      "Topic 36:  ['just', 'things', 'bit argument', 'just bit argument', 'quote just']\n",
      "Topic 37:  ['cruel', 'sure', 'does', 'allah', 'true']\n",
      "Topic 38:  ['right', 'enlightening', 'right thanks', 'right thanks enlightening', 'thanks enlightening']\n",
      "Topic 39:  ['alt', 'say', 'alt atheism', 'rosicrucian', 'want']\n",
      "Topic 40:  ['ignorance strength', 'strength', 'ignorance', 'right', 'rosicrucian']\n",
      "Topic 41:  ['ignorance strength', 'strength', 'ignorance', 'lunacy', 'cruel']\n",
      "Topic 42:  ['ignorance strength', 'ignorance', 'strength', 'did claim', 'did']\n",
      "Topic 43:  ['theory', 'imaginative', 'imaginative theory', 'religious', 'paradise']\n",
      "Topic 44:  ['evidence', 'values', 'know', 'bit argument', 'just bit argument']\n",
      "Topic 45:  ['ignorance strength', 'strength', 'ignorance', 'paradise', 'salvation']\n",
      "Topic 46:  ['ignorance strength', 'strength', 'ignorance', 'just', 'moral']\n",
      "Topic 47:  ['sure wrong', 'sure', 'maybe', 'notion', 'science']\n",
      "Topic 48:  ['list', 'truth', 'sabbath', 'say', 'just bet']\n",
      "Topic 49:  ['did claim', 'did', 'cruel', 'bible', 'religious']\n"
     ]
    }
   ],
   "source": [
    "terms = news_tf.get_feature_names()\n",
    "for index, component in enumerate(tsvd.components_):\n",
    "    zipped = zip(terms, component)\n",
    "    top_terms_key=sorted(zipped, key = lambda t: t[1], reverse=True)[:5]\n",
    "    top_terms_list=list(dict(top_terms_key).keys())\n",
    "    print(\"Topic \"+str(index)+\": \",top_terms_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec and Classification\n",
    "\n",
    "In addition to calculating things solely directly from our data, we can also use some external tools that can help create embeddings that are a little better (hopefully). This is also a neural network running behind the scenes to help us out. \n",
    "\n",
    "Word2Vec is an algorithm made by Google that can help process text and produce embeddings. Word2Vec looks for associations of words that occur with each other. \n",
    "\n",
    "### Word2Vec in Process\n",
    "\n",
    "Word2Vec generates its embeddings by looking at words in a sentence, and the surrounding words in that same sentence. \n",
    "\n",
    "### Gensim\n",
    "\n",
    "Gensim is a package that we can install that has an implementation of Word2Vec that we can use pretty easily. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-28538d0ff423>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpackage\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'stopwords'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'punkt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'wordnet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpackage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "for package in ['stopwords','punkt','wordnet']:\n",
    "    nltk.download(package) \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words    = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "Since we are not using the vecorizer from sklearn, we need to provide our own tokenization. We can use the nltk based one from last time. \n",
    "\n",
    "We can also do any other types of processing here that we may want - stemming, customized stop words, etc... For this one I chopped out any 1 character tokens and added a regex filter to get rid of punctuation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lemmaTokenizer(object):\n",
    "    def __init__(self, stop_words):\n",
    "        self.stop_words = stop_words\n",
    "        from nltk.stem import WordNetLemmatizer\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        tokens = word_tokenize(doc)\n",
    "        filtered_tok = []\n",
    "        for tok in tokens:\n",
    "            if tok not in stop_words:\n",
    "                tok = re.sub('\\W+','', tok) #Punctuation strip\n",
    "                tmp = self.lemmatizer.lemmatize(tok)\n",
    "                if len(tmp) >= 2:\n",
    "                    filtered_tok.append(tmp)\n",
    "        return filtered_tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Clan Text - Tokenize and Lemmatize\n",
    "\n",
    "Prep some data. The \"second half\" of the dataframe is what we can use with the Word2Vec prediction models - we have cleaned up lists of tokens as well as translating the targets to 1 and 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>target2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>[Go, jurong, point, crazy, Available, bugis, g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>[Ok, lar, Joking, wif, oni]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[Free, entry, wkly, comp, win, FA, Cup, final,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>[dun, say, early, hor, already, say]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[Nah, nt, think, go, usf, life, around, though]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                               text  \\\n",
       "0    ham  Go until jurong point, crazy.. Available only ...   \n",
       "1    ham                      Ok lar... Joking wif u oni...   \n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3    ham  U dun say so early hor... U c already then say...   \n",
       "4    ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                          clean_text  target2  \n",
       "0  [Go, jurong, point, crazy, Available, bugis, g...        0  \n",
       "1                        [Ok, lar, Joking, wif, oni]        0  \n",
       "2  [Free, entry, wkly, comp, win, FA, Cup, final,...        1  \n",
       "3               [dun, say, early, hor, already, say]        0  \n",
       "4    [Nah, nt, think, go, usf, life, around, though]        0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = lemmaTokenizer(stop_words)\n",
    "df[\"clean_text\"] = df[\"text\"].apply(lambda x: tok(x))\n",
    "df[\"target2\"] = pd.get_dummies(df[\"target\"], drop_first=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Word2Vec\n",
    "\n",
    "We can train our Word2Vec model with our cleaned up data. This will have Word2Vec do its magic behind the scenes and perform the training. W2V works in one of two ways, which are roughly opposites of each other, when doing this training:\n",
    "<ul>\n",
    "<li> Continuous Bag of Words: looks at a window around each target word to try to predict surrounding words.\n",
    "<li> Skip-Gram: looks at words surrounding target to try to predict it. \n",
    "</ul>\n",
    "\n",
    "We'll revisit the details of this stuff later on when we look at neural networks, since W2V is a neural network algorithm, it will make more sense in context. \n",
    "\n",
    "Note: this training is not making a model that we are using to make predictions. This is training inside the W2V algorithm to generate representations of our tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Word2vec model\n",
    "model = Word2Vec(df['clean_text'],min_count=1, vector_size=200, sg=1)\n",
    "#min_count=1 means word should be present at least across all documents,\n",
    "#if min_count=2 means if the word is present less than 2 times across all the documents then we shouldn't consider it\n",
    "\n",
    "#combination of word and its vector\n",
    "w2v = dict(zip(model.wv.index_to_key, model.wv.vectors))  \n",
    "\n",
    "#for converting sentence to vectors/numbers from word vectors result by Word2Vec\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Model\n",
    "\n",
    "Each word in the vocabulary now has a vector representing it - of size 200. We can make a dataframe and see each token in our text and its vector representation. This vector is the internal representation of each token that is generated by Word2Vec. This is how the algorithm calculates things like similarity..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>call</th>\n",
       "      <th>nt</th>\n",
       "      <th>get</th>\n",
       "      <th>gt</th>\n",
       "      <th>lt</th>\n",
       "      <th>ur</th>\n",
       "      <th>You</th>\n",
       "      <th>go</th>\n",
       "      <th>know</th>\n",
       "      <th>like</th>\n",
       "      <th>...</th>\n",
       "      <th>ARR</th>\n",
       "      <th>oscar</th>\n",
       "      <th>Open</th>\n",
       "      <th>rebtel</th>\n",
       "      <th>firefox</th>\n",
       "      <th>Married</th>\n",
       "      <th>MATCH</th>\n",
       "      <th>69969</th>\n",
       "      <th>BCMSFWC1N3XX</th>\n",
       "      <th>stereo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.150014</td>\n",
       "      <td>0.140973</td>\n",
       "      <td>0.155234</td>\n",
       "      <td>0.313671</td>\n",
       "      <td>0.184144</td>\n",
       "      <td>0.184378</td>\n",
       "      <td>0.147953</td>\n",
       "      <td>0.134820</td>\n",
       "      <td>0.151135</td>\n",
       "      <td>0.166885</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007919</td>\n",
       "      <td>0.006109</td>\n",
       "      <td>0.014609</td>\n",
       "      <td>0.017589</td>\n",
       "      <td>0.017651</td>\n",
       "      <td>0.007834</td>\n",
       "      <td>0.024493</td>\n",
       "      <td>0.016004</td>\n",
       "      <td>0.006854</td>\n",
       "      <td>0.010637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.142326</td>\n",
       "      <td>-0.060369</td>\n",
       "      <td>-0.072934</td>\n",
       "      <td>0.079580</td>\n",
       "      <td>0.139030</td>\n",
       "      <td>-0.161924</td>\n",
       "      <td>-0.116762</td>\n",
       "      <td>-0.063853</td>\n",
       "      <td>-0.067081</td>\n",
       "      <td>-0.051913</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003700</td>\n",
       "      <td>-0.009346</td>\n",
       "      <td>-0.009956</td>\n",
       "      <td>-0.009417</td>\n",
       "      <td>-0.011845</td>\n",
       "      <td>-0.007473</td>\n",
       "      <td>-0.018274</td>\n",
       "      <td>-0.021063</td>\n",
       "      <td>-0.007991</td>\n",
       "      <td>-0.013714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.088994</td>\n",
       "      <td>-0.047234</td>\n",
       "      <td>-0.009189</td>\n",
       "      <td>0.026657</td>\n",
       "      <td>0.010834</td>\n",
       "      <td>0.038615</td>\n",
       "      <td>0.017828</td>\n",
       "      <td>-0.031769</td>\n",
       "      <td>-0.011203</td>\n",
       "      <td>-0.011733</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003548</td>\n",
       "      <td>0.005631</td>\n",
       "      <td>-0.000425</td>\n",
       "      <td>0.005585</td>\n",
       "      <td>0.002352</td>\n",
       "      <td>0.003458</td>\n",
       "      <td>0.003737</td>\n",
       "      <td>0.003831</td>\n",
       "      <td>0.003624</td>\n",
       "      <td>0.005702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.111481</td>\n",
       "      <td>0.080862</td>\n",
       "      <td>0.102752</td>\n",
       "      <td>0.249139</td>\n",
       "      <td>0.328954</td>\n",
       "      <td>0.027910</td>\n",
       "      <td>0.049240</td>\n",
       "      <td>0.067798</td>\n",
       "      <td>0.083567</td>\n",
       "      <td>0.103523</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000777</td>\n",
       "      <td>0.007031</td>\n",
       "      <td>0.000901</td>\n",
       "      <td>0.008647</td>\n",
       "      <td>-0.000732</td>\n",
       "      <td>-0.000104</td>\n",
       "      <td>0.003348</td>\n",
       "      <td>0.006658</td>\n",
       "      <td>0.002567</td>\n",
       "      <td>-0.000280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.313374</td>\n",
       "      <td>0.239469</td>\n",
       "      <td>0.269988</td>\n",
       "      <td>0.401648</td>\n",
       "      <td>0.445792</td>\n",
       "      <td>0.244672</td>\n",
       "      <td>0.253980</td>\n",
       "      <td>0.233451</td>\n",
       "      <td>0.256763</td>\n",
       "      <td>0.268269</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007495</td>\n",
       "      <td>0.012834</td>\n",
       "      <td>0.019305</td>\n",
       "      <td>0.021011</td>\n",
       "      <td>0.023671</td>\n",
       "      <td>0.017004</td>\n",
       "      <td>0.028636</td>\n",
       "      <td>0.035334</td>\n",
       "      <td>0.014269</td>\n",
       "      <td>0.018845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 10583 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       call        nt       get        gt        lt        ur       You  \\\n",
       "0  0.150014  0.140973  0.155234  0.313671  0.184144  0.184378  0.147953   \n",
       "1 -0.142326 -0.060369 -0.072934  0.079580  0.139030 -0.161924 -0.116762   \n",
       "2  0.088994 -0.047234 -0.009189  0.026657  0.010834  0.038615  0.017828   \n",
       "3  0.111481  0.080862  0.102752  0.249139  0.328954  0.027910  0.049240   \n",
       "4  0.313374  0.239469  0.269988  0.401648  0.445792  0.244672  0.253980   \n",
       "\n",
       "         go      know      like  ...       ARR     oscar      Open    rebtel  \\\n",
       "0  0.134820  0.151135  0.166885  ...  0.007919  0.006109  0.014609  0.017589   \n",
       "1 -0.063853 -0.067081 -0.051913  ... -0.003700 -0.009346 -0.009956 -0.009417   \n",
       "2 -0.031769 -0.011203 -0.011733  ... -0.003548  0.005631 -0.000425  0.005585   \n",
       "3  0.067798  0.083567  0.103523  ...  0.000777  0.007031  0.000901  0.008647   \n",
       "4  0.233451  0.256763  0.268269  ...  0.007495  0.012834  0.019305  0.021011   \n",
       "\n",
       "    firefox   Married     MATCH     69969  BCMSFWC1N3XX    stereo  \n",
       "0  0.017651  0.007834  0.024493  0.016004      0.006854  0.010637  \n",
       "1 -0.011845 -0.007473 -0.018274 -0.021063     -0.007991 -0.013714  \n",
       "2  0.002352  0.003458  0.003737  0.003831      0.003624  0.005702  \n",
       "3 -0.000732 -0.000104  0.003348  0.006658      0.002567 -0.000280  \n",
       "4  0.023671  0.017004  0.028636  0.035334      0.014269  0.018845  \n",
       "\n",
       "[5 rows x 10583 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = pd.DataFrame(w2v)\n",
    "vectors = model.wv\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity\n",
    "\n",
    "One of the things that Word2Vec allows us to do is to look at the similarity of words. This similarity is calculated via the cosine distance of the vectors. Cosine similarity is a technique to calculate the distance between two vectors - smaller distance, more similar. \n",
    "\n",
    "![Cosine Similarity](images/cosine_sim.png \"Cosine Similarity\" )\n",
    "\n",
    "Once the vectors are derived by in the training process, these similarity calculations are pretty easy and quick. \n",
    "\n",
    "Note: the similarites here are calculated by the values derived from our trained model. So they are based on the relationships in our text. Word2Vec and other NLP packages also commonly have pretrained models that can be downloaded that are based on large amounts of text. Words may be represented very differently in those vs whatever we train here - the more data we have, the more consistent they'll be; the more \"unique\" our text is, the more different it will be. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('see', 0.9983924031257629),\n",
       " ('night', 0.9980188012123108),\n",
       " ('one', 0.9978786110877991)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find the most similar word to anything in our vocabulary \n",
    "vectors.most_similar(\"think\")[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97080845"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also see how similar different words are. \n",
    "vectors.similarity(\"think\", \"determine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions\n",
    "\n",
    "We can take our actual data now and transform it through the Word2Vec model that we've made. This will generate our smaller feature set that we can build our models from.\n",
    "\n",
    "One of the things that the MeanEmbeddingVectorizer does is to collapse the data down to those 200 dimensions in the vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4179, 200)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SPLITTING THE TRAINING DATASET INTO TRAINING AND VALIDATION\n",
    " \n",
    "# Split data - using the new dataframe parts that we cleaned up. \n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"clean_text\"],df[\"target2\"])\n",
    "\n",
    "#Word2vec\n",
    "# Fit and transform\n",
    "modelw = MeanEmbeddingVectorizer(w2v)\n",
    "X_train_vectors_w2v = modelw.transform(X_train)\n",
    "X_test_vectors_w2v = modelw.transform(X_test)\n",
    "X_train_vectors_w2v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Model\n",
    "\n",
    "We can now use the new data to make predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      1212\n",
      "           1       0.88      0.84      0.86       181\n",
      "\n",
      "    accuracy                           0.96      1393\n",
      "   macro avg       0.93      0.91      0.92      1393\n",
      "weighted avg       0.96      0.96      0.96      1393\n",
      "\n",
      "Confusion Matrix: [[1192   20]\n",
      " [  29  152]]\n",
      "AUC: 0.9792407417537334\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "lr_w2v = RandomForestClassifier()\n",
    "lr_w2v.fit(X_train_vectors_w2v, y_train)  #model\n",
    "\n",
    "#Predict y value for test dataset\n",
    "y_predict = lr_w2v.predict(X_test_vectors_w2v)\n",
    "y_prob = lr_w2v.predict_proba(X_test_vectors_w2v)[:,1]\n",
    " \n",
    "\n",
    "print(classification_report(y_test,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Word2Vec\n",
    "\n",
    "Use the newsgroup data and Word2Vec to make predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datsets and Tokenize\n",
    "tok = lemmaTokenizer(stop_words)\n",
    "X_w2v_news_train = [tok(x) for x in data_train.data]\n",
    "X_w2v_news_test = [tok(x) for x in data_test.data]\n",
    "\n",
    "y_train_news = data_train.target\n",
    "y_test_news = data_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Word2vec model\n",
    "model_news = Word2Vec(X_w2v_news_train, min_count=1, vector_size=200)\n",
    "w2v_news = dict(zip(model_news.wv.index_to_key, model_news.wv.vectors)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4179, 200)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Word2vec\n",
    "# Fit and transform\n",
    "model_news_w = MeanEmbeddingVectorizer(w2v_news)\n",
    "X_train_vectors_w2v_news = modelw.transform(X_w2v_news_train)\n",
    "X_val_vectors_w2v_news = modelw.transform(X_w2v_news_test)\n",
    "X_train_vectors_w2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.65      0.60       319\n",
      "           1       0.43      0.34      0.38       251\n",
      "\n",
      "    accuracy                           0.51       570\n",
      "   macro avg       0.49      0.50      0.49       570\n",
      "weighted avg       0.50      0.51      0.50       570\n",
      "\n",
      "Confusion Matrix: [[208 111]\n",
      " [166  85]]\n",
      "AUC: 0.5131324232849167\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "news_clf = RandomForestClassifier()\n",
    "news_clf.fit(X_train_vectors_w2v_news, y_train_news)  #model\n",
    "\n",
    "#Predict y value for test dataset\n",
    "y_predict_news = news_clf.predict(X_val_vectors_w2v_news)\n",
    "y_prob_news = news_clf.predict_proba(X_val_vectors_w2v_news)[:,1]\n",
    " \n",
    "\n",
    "print(classification_report(y_test_news,y_predict_news))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test_news, y_predict_news))\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_test_news, y_prob_news)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)  "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d722d3adfa415172c1f5238b519fb86b488acdae450fd691ab06c09f4ca9173"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ml3950': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
