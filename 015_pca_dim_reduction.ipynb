{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA - Principal Component Analysis\n",
    "\n",
    "When dealing with text we looked at the truncated SVD algorithm that could reduce the massive datasets generated from encoding text down to a subset of features. \n",
    "\n",
    "PCA is a similar concept, we can take high dimension feature sets and reduce them down to a subset of features used for prediction. PCA is a very common method for dimensionality reduction. \n",
    "\n",
    "## PCA Concepts \n",
    "\n",
    "PCA reduces dimensionality by breaking down the variance in the data into its \"principal components\", then keeping only those components that do the best job in explaining said variance. We can understand this well with an example, in 2D. We'll create something that looks like an example from simple linear regression type of data - we have a bunch of points, each point is located by its X and Y values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make some random numbers\n",
    "plt.rcParams['figure.figsize'] = 12,6\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "\n",
    "X = np.dot(np.random.rand(2, 2), np.random.randn(2, 200)).T\n",
    "sns.regplot(data=X, x=X[:,0], y=X[:,1], ci=0, ax=ax[0])\n",
    "ax[0].set_ylabel('Y')\n",
    "ax[0].set_xlabel('X')\n",
    "\n",
    "tmpPCA = PCA(2)\n",
    "tmpData = tmpPCA.fit_transform(X)\n",
    "sns.regplot(data=tmpData, x=tmpData[:,0], y=tmpData[:,1], ci=0, ax=ax[1])\n",
    "ax[1].set_ylabel('PC 2')\n",
    "ax[1].set_xlabel('PC 1')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Components\n",
    "\n",
    "In normal analysis, each of these points is defined by their X and Y values:\n",
    "<ul>\n",
    "<li> X - how far left and right the point is.  \n",
    "<li> Y - how far up and down the point is. \n",
    "</ul>\n",
    "\n",
    "Together these points explain all of the position data of the points. \n",
    "\n",
    "\n",
    "Once we look at PCA, we can also think of these points being defined by two components:\n",
    "<ul>\n",
    "<li> Along the regression line. The majority of the variance in Y is explained by the position along this line. \n",
    "<li> Perpindicular to the regression line. Some smaller part of the variance in Y is explained by how \"far off\" it is from the regession line.\n",
    "</ul>\n",
    "\n",
    "In essence, we can explain the position of our points mostly by examining where it is along the regression line (component 1), along with a little info on how far off it is from that line. These two components can explain our data - \"A\" amount \"up and down\" the line, along with \"B\" amount \"off the line\". This also explains the position of the points, but does so with different values than X and Y. \n",
    "\n",
    "If we look at the plot of the PCA components, PC1 (plotted as X) has a wide range, or lots of variance. PC2 (plotted as Y) has a small range, or a small amount of variance. \n",
    "\n",
    "#### Animated Example\n",
    "\n",
    "See: https://setosa.io/ev/principal-component-analysis/ \n",
    "\n",
    "### PCA and Eigenvectors\n",
    "\n",
    "The components generated by the PCA are called eigenvectors. We don't need to worry about much of the math, but this PCA can be calculated by hand with some linear math. We can skip that, computers are good at math. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA and Dimensionality Reduction\n",
    "\n",
    "Once we've established the components, reducing the dimensions of our feature set is simple - just reduce the components that matter least to 0. In our example, we'd ignore the \"off the line\" component that is responsible for only a little bit of the position of our points, and keep the \"up the line\" component that explains the majority of the position of our points. \n",
    "\n",
    "In the XY system, both X and Y are very important in specifying where a point is, X somewhat more important than Y. In our component system, the \"up the line\" component provides the majority of the information on our points, with the \"off the line\" component only adding a little bit of info. This is the key to the dimensionality reduction - if we feature selected away the Y value, we would lose substantial information on the location of the points. If we PCA-away the \"off the line\" component, we only lose a small amount of information! So we can describe this data \"pretty well\" with only 1/2 the number of features if we describe the data with the components over the original features. When dealing with large numbers of features, this can allow us to reduce them down to a much smaller number of components, without missing out on too much information describing the real data. \n",
    "\n",
    "The true benefit of PCA is if there are a lot of features. We can do something like the example here to grab the \"best\" components, drop the rest, and have a smaller feature set with a comparable level of accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Colinearity and Multi-colinearity\n",
    "\n",
    "One of the other benefits of PCA is that it reduces colinearity between features. The components that PCA generates are orthogonal of each other - the colinearity is reduced to effectively 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension Reduction in Multiple Dimensions\n",
    "\n",
    "This 2D example is simple to picture. The same concept applies when we have data with lots of dimensions. We can break the data down into components, remove the least impactful, and end up with a feature set that captures most of the variance in our target with fewer inputs. \n",
    "\n",
    "### Example with Real Data\n",
    "\n",
    "This dataset is one of the sklearn samples, containing measurements from people with and without breast cancer. The classification of cancer/no cancer is the target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_to_df(sklearn_dataset):\n",
    "    df = pd.DataFrame(sklearn_dataset.data, columns=sklearn_dataset.feature_names)\n",
    "    df['target'] = pd.Series(sklearn_dataset.target)\n",
    "    return df\n",
    "\n",
    "df = sklearn_to_df(load_breast_cancer())\n",
    "y1 = df[\"target\"]\n",
    "X1 = df.drop(columns=\"target\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre PCA Test\n",
    "\n",
    "We can run a test to approximate the accuracy without doing PCA. We don't want accuracy to drop too much after the PCA process. This is our baseline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_model = LogisticRegression()\n",
    "pre_scale = MinMaxScaler()\n",
    "pre_pipe = Pipeline([(\"scale\", pre_scale), (\"model\", pre_model)])\n",
    "print(\"Estimated Initial Accuracy:\", np.mean(cross_val_score(pre_pipe, X1, y1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Dimensionality and Correlation\n",
    "\n",
    "One classfication target, along with 30 features. We can look for correlation between those features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Original Correlation\n",
    "plt.rcParams['figure.figsize'] = 15,5\n",
    "sns.heatmap(X1.corr(), cmap=\"BuPu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate VIF for Multicolinearity\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(X1.values, i) for i in range(X1.shape[1])]\n",
    "vif[\"features\"] = X1.columns\n",
    "vif.sort_values(\"VIF Factor\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Colinearity Results\n",
    "\n",
    "Looks like there is a lot of correlation going on. The heatmap shows many values that are pretty correlated, and the VIF shows some really high values. Recall, values for a VIF over about 10 are really large. \n",
    "\n",
    "For the model, we'll be sure to use a logistic regression, that is very impacted by the colinearity. \n",
    "\n",
    "Feel free to play with the number of components and observe results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check accuracy \n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1)\n",
    "can_pca = PCA()\n",
    "can_model = LogisticRegression()\n",
    "can_steps = [\n",
    "    (\"scale\", MinMaxScaler()), \n",
    "    (\"pca\", can_pca),\n",
    "    (\"can_model\", can_model)\n",
    "    ]\n",
    "can_pipe = Pipeline(steps=can_steps)\n",
    "can_params = {\n",
    "    \"pca__n_components\":[15]\n",
    "}\n",
    "\n",
    "clf1 = GridSearchCV(estimator=can_pipe, param_grid=can_params, cv=5, n_jobs=-1) \n",
    "clf1.fit(X_train1, y_train1.ravel())\n",
    "print(clf1.score(X_test1, y_test1))\n",
    "best1 = clf1.best_estimator_\n",
    "print(best1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results - We Have Accuracy!\n",
    "\n",
    "Accuracy looks pretty good, even though we've reduced the number of features. How is the information on our target (the variance) distributed amongst our components? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get PCA Info\n",
    "comps1 = best1.named_steps['pca'].components_ \n",
    "ev1 = best1.named_steps['pca'].explained_variance_ratio_\n",
    "plt.rcParams['figure.figsize'] = 6,6\n",
    "plt.plot(np.cumsum(ev1))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is in the PCA Components?\n",
    "\n",
    "We can also reconstruct the importance of the contributions of the different features to the components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for i in range(len(comps1)):\n",
    "    label = \"PC-\"+str(i)\n",
    "    labels.append(label)\n",
    "\n",
    "PCA1_res_comps = pd.DataFrame(comps1,columns=X1.columns, index = labels)\n",
    "PCA1_res_comps.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results of PCA\n",
    "\n",
    "PCA allows us to reduce down the original 30 feature set to a much smaller number, while still making accurate predictions. In this case, it looks like we can get about 90% of the explained varaiance in the data by using around 6 or so components. Yay, that's cool!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA and Feature Selection\n",
    "\n",
    "PCA is not a feature selection technique. PCA does do a similar thing to feature selection in reducing the size of our feature set that goes into a model, but it is technically different. \n",
    "\n",
    "Feature selection removes features. PCA removes components, that are created from features, but that are not, themselves, features. In PCA, the features are being transformed for the components to be created, and each component includes portions of multiple features - for example, in the scatter plot above, both the \"up the line\" and \"off the line\" components contain parts of the X and Y features. If we drop the \"off the line\" feature when doing PCA we aren't really eliminating any features - we still need X and Y to calculate each of our components. In the breast cancer example, each of those features still contributes to the components, but the actual predictors are far reduced. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working Example\n",
    "\n",
    "Predict if people have diabetes (Outcome) using PCA to help. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/diabetes.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Component Importance\n",
    "\n",
    "We can plot the effectiveness with different numbers of components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the variance by component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA with Images - Big Dimensions!\n",
    "\n",
    "One common example of something with a large feature set is images - even our simple set of handwritten numbers had 784 features for each digit. Generating models from all 70,000 of those simple images could take forever, and those are about the most simple images we can imagine!\n",
    "\n",
    "Reducing the dimensions of very large images can be highly beneficial, especially if we can keep the useful bits that we need to do identification. \n",
    "\n",
    "### Faces, PCA, and You\n",
    "\n",
    "This dataset is a more complex set of images than the digits we used previously. It is a set of a bunch of faces of past world leaders, our goal being to make a model that will recognize each person from their picture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "faces = fetch_lfw_people(min_faces_per_person=60)\n",
    "print(faces.target_names)\n",
    "print(faces.images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting Dimensions and PCA Dimensions\n",
    "\n",
    "We start with ~1350 images, each 62 x 47 pixels, color depth of 1 - resulting in a feature set that is around 3000 columns wide. \n",
    "\n",
    "We can fit the data to a PCA transformation, and chop the feature set down to a much smaller number of components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate PCA and inversed face-sets\n",
    "pca150 = PCA(150).fit(faces.data)\n",
    "components150 = pca150.transform(faces.data)\n",
    "projected150 = pca150.inverse_transform(components150)\n",
    "\n",
    "pca15 = PCA(15).fit(faces.data)\n",
    "components15 = pca15.transform(faces.data)\n",
    "projected15 = pca15.inverse_transform(components15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Picture Some Pictures\n",
    "\n",
    "We can look at what the pictures look like in their original state, and after the PCA process has reduced their dimensions by various amounts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot faces and PCA faces\n",
    "fig, ax = plt.subplots(3, 12, figsize=(12, 6),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "for i in range(12):\n",
    "    ax[0,i].imshow(faces.data[i].reshape(62, 47), cmap='bone')\n",
    "    ax[1,i].imshow(projected150[i].reshape(62, 47), cmap='bone')\n",
    "    ax[2,i].imshow(projected15[i].reshape(62, 47), cmap='bone')\n",
    "\n",
    "ax[0, 0].set_ylabel('Original')\n",
    "ax[1, 0].set_ylabel('150-dim')\n",
    "ax[2, 0].set_ylabel('15-dim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amount of Variance Captured in Components\n",
    "\n",
    "We can look at our PCA'd data and see that while the images are much less clear and defined, they are pretty similar on the whole! We can probably still do a good job of IDing the people, even though we have roughly 1/20 (or 1/200) the number of features as we started with. Cool. Even with the 15 component set, the images are still somewhat able to be recognized. \n",
    "\n",
    "The PCA allows us to call up the details on how much of the variance was captured in each component. The first few contain lots of the useful info, once we reach 20 components we have about ~75% or so of the original varaince. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(pca150.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scree Plot and Number of Components\n",
    "\n",
    "One question we're left with is how many components should we keep? This answer varies, common suggestions are enough to capture somewhere around 80% to 95% of the explained variance. These metrics are somewhat arbitrary - testing different numbers of components will likely make sense in many cases. \n",
    "\n",
    "One method to choose the number of features is a scree plot. This is a plot that shows the contribution of each component. The scree plot shows the same information as the graph above, but formatted differently. The idea of a scree plot is to find the \"elbow\", or where the plot levels out. This flattening point is approximately where you should cut off the number of components - the idea being that you capture all the components that make a substantial difference, and let the ones that make a small difference go. \n",
    "\n",
    "Personally, I think the cumulative plot above is easier to view, but scree plots are pretty common. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scree Plot\n",
    "PC_values = np.arange(pca150.n_components_) + 1\n",
    "plt.plot(PC_values, pca150.explained_variance_ratio_, 'o-', linewidth=2, color='blue')\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Variance Explained')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions with PCA\n",
    "\n",
    "We can try to make some predictions and see what the results are with PCA'd data. We'll use a multinomial HP to tell our regression to directly predict multiple classes with our friend the softmax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get data\n",
    "y = faces.target\n",
    "X = faces.data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "#Model and grid search of components. \n",
    "scaler = MinMaxScaler()\n",
    "logistic = LogisticRegression(max_iter=10000, tol=0.1, multi_class=\"multinomial\")\n",
    "pca_dia = PCA()\n",
    "pipe = Pipeline(steps=[(\"scaler\", scaler), (\"pca\", pca_dia), (\"logistic\", logistic)])\n",
    "\n",
    "param_grid = {\n",
    "    \"pca__n_components\": [130]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, n_jobs=-1)\n",
    "grid.fit(X_train, y_train.ravel())\n",
    "print(\"Best parameter (CV score=%0.3f):\" % grid.best_score_)\n",
    "print(grid.best_params_)\n",
    "print(\"Test Score:\", grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel PCA\n",
    "\n",
    "Similarly to support vector machines, we can use a kernel transformation to make PCA better suit data with non-linear relationships. The concept is the same as with the SVMs - we can provide a kernel that does a transformation, then the linear algebra of PCA can be executed on the transformed data. \n",
    "\n",
    "The implementation is very simple - we replace PCA with KernelPCA, and provide the kernel we want to use. \n",
    "\n",
    "We can see if a different kernel is better than the original... Try with a grid search of the different kernels other than linear. Also, for the polynomial kernel, try with multiple values in the grid search. Documentation is: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Kernel PCA\n",
    "from sklearn.decomposition import KernelPCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse PCA\n",
    "\n",
    "Sparse PCA is another implementation of PCA that includes L1 regularization - resulting in some of the values being regularized down to 0. The end result of this is that you end up with a subset of the features being used to construct the components. The others are feature selected out just like Lasso rregression. \n",
    "\n",
    "We can redo the table of component details from the previous breast cancer example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import SparsePCA\n",
    "\n",
    "sPCA = SparsePCA(15)\n",
    "sparse = sPCA.fit_transform(X1)\n",
    "\n",
    "comps3 = sPCA.components_ \n",
    "labels = []\n",
    "for i in range(len(comps3)):\n",
    "    label = \"PC-\"+str(i)\n",
    "    labels.append(label)\n",
    "\n",
    "PCA3_res_comps = pd.DataFrame(comps3, columns=X1.columns, index = labels)\n",
    "PCA3_res_comps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA3_res_comps.describe().T.sort_values(\"mean\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d722d3adfa415172c1f5238b519fb86b488acdae450fd691ab06c09f4ca9173"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ml3950': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
